{"title":"Loss or Cost funtions","markdown":{"yaml":{"description":"Loss or Cost funtions","output-file":"loss_or_cost_funtions.html","title":"Loss or Cost funtions"},"headingText":"Loss Functions in Brief","containsRefs":false,"markdown":"\n\n\n\n<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->\n\n\nThe key takeaway is that the loss function is a measurable way to gauge the performance and accuracy of a machine learning model. In this case, the loss function acts as a guide for the learning process within a model or machine learning algorithm.\n\nThe role of the loss function is crucial in the training of machine learning models and includes the following:\n\n- Performance measurement: Loss functions offer a clear metric to evaluate a model's performance by quantifying the difference between predictions and actual results.\n- Direction for improvement: Loss functions guide model improvement by directing the algorithm to adjust parameters(weights) iteratively to reduce loss and improve predictions.\n- Balancing bias and variance: Effective loss functions help balance model bias (oversimplification) and variance (overfitting), essential for the model's generalization to new data.\n- Influencing model behavior: Certain loss functions can affect the model's behavior, such as being more robust against data outliers or prioritizing specific types of errors.\n\n\nLet's explore the roles of particular loss functions in later sections and build a detailed intuition and understanding of the loss function.\n\n## Applicability to Classification\n\n### Binary Cross-Entropy Loss / Log Loss\n\nTo understand Binary Cross-Entropy Loss, sometimes called Log Loss, it is helpful to discuss the components of the terms.\n- Loss: This is a mathematical quantification of the margin/difference between the prediction of a machine learning algorithm and the actual target value.\n- Entropy: A simple definition of entropy is that it is a calculation of the degree of randomness or disorder within a system\n- Cross Entropy: This is a term commonly utilised in information theory, and it measures the differences between two probability distributions that can be used to identify an observation.\n- Binary: This is an expression of numerical digits using either of two states, 0 or 1. This is extended to the definition of Binary Classification where we ditingus=ish two classes(A and B) using binary representation, where class A is assigned the numerical representation of 0 and class B is assigned 1.\n\n$L(y , f(x)) = -[y * log(f(x)) + (1 - y) * log(1 - f(x))]$\n\nWhere:\n\n    L represents the Binary Cross-Entropy Loss function\n    y is the true binary label (0 or 1)\n    f(x) is the predicted probability of the positive class (between 0 and 1)\n\n### Categorical Cross-Entropy Loss\n\n### Hinge Loss\n> Hinge Loss is a loss function utilized within machine learning to train classifiers that optimize to increase the margin between data points and the decision boundary. Hence, it is mainly used for maximum margin classifications. To ensure the maximum margin between the data points and boundaries, hinge loss penalizes predictions from the machine learning model that are wrongly classified, which are predictions that fall on the wrong side of the margin boundary and also predictions that are correctly classified but are within close proximity to the decision boundary.\n\n$L(y - f(x)) = \\max(0, 1 - y * f(x))$\nWhere:\n\n    L represents the Hinge Loss\n    y is the true label or target value (-1 or 1)\n    f(x) is the predicted value or decision function output\n\n### Log Loss\n \n \n \n\n## Applicability to Regression\n\n### Mean Square Error (MSE) / L2 Loss\n> MSE is a standard loss function utilized in most regression tasks since it directs the model to optimize to minimize the squared differences between the predicted and target values.\n\n$MSE  = \\dfrac{1}{n} * \\sum(y_i -\\bar{y})^2$\n\nWhere:\n\n    n is the number of samples in the dataset\n    yᵢ is the predicted value for the i-th sample\n    ȳ is the target value for the i-th sample\n\n### Mean Absolute Error (MAE) / L1 Loss\n> A scenario where MAE is an applicable loss function is one where we don't want to penalize outliers considerably or at all, for example, predicting delivery times for a food delivery company.\n\n\n$MAE  = \\dfrac{1}{n} * \\sum(y_i -\\bar{y})$\n\nWhere:\n\n    n is the number of samples in the dataset\n    yᵢ is the predicted value for the i-th sample\n    ȳ is the target value for the i-th sample\n\n### Huber Loss / Smooth Mean Absolute Error\n> The Huber Loss function effectively combines two components for handling errors differently, with the transition point between these components determined by the threshold δ:\\\n> Quadratic Component for Small Errors: For errors smaller than δ, it uses the quadratic component (1/2) * (f(x) - y)^2\\\n> Linear Component for Large Errors: For errors larger than δ, it applies the linear component δ * |f(x) - y| - (1/2) * δ^2\n\n$L(\\delta, y , f(x)) = \\dfrac{1}{2} * (f(x) - y)^2 \\quad if \\quad |f(x) - y| <= \\delta$\\\n$\\quad \\quad \\quad \\quad = \\delta * |f(x) - y| - \\dfrac{1}{2} * \\delta^2 \\quad if \\quad |f(x) - y| > \\delta$\n\nWhere:\n\n    L represents the Huber Loss function\n    δ is the delta parameter, which determines the threshold for switching between the quadratic and linear components of the loss function\n    y is the true value or target value\n    f(x) is the predicted value\n","srcMarkdownNoYaml":"\n\n\n\n<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->\n\n## Loss Functions in Brief\n\nThe key takeaway is that the loss function is a measurable way to gauge the performance and accuracy of a machine learning model. In this case, the loss function acts as a guide for the learning process within a model or machine learning algorithm.\n\nThe role of the loss function is crucial in the training of machine learning models and includes the following:\n\n- Performance measurement: Loss functions offer a clear metric to evaluate a model's performance by quantifying the difference between predictions and actual results.\n- Direction for improvement: Loss functions guide model improvement by directing the algorithm to adjust parameters(weights) iteratively to reduce loss and improve predictions.\n- Balancing bias and variance: Effective loss functions help balance model bias (oversimplification) and variance (overfitting), essential for the model's generalization to new data.\n- Influencing model behavior: Certain loss functions can affect the model's behavior, such as being more robust against data outliers or prioritizing specific types of errors.\n\n\nLet's explore the roles of particular loss functions in later sections and build a detailed intuition and understanding of the loss function.\n\n## Applicability to Classification\n\n### Binary Cross-Entropy Loss / Log Loss\n\nTo understand Binary Cross-Entropy Loss, sometimes called Log Loss, it is helpful to discuss the components of the terms.\n- Loss: This is a mathematical quantification of the margin/difference between the prediction of a machine learning algorithm and the actual target value.\n- Entropy: A simple definition of entropy is that it is a calculation of the degree of randomness or disorder within a system\n- Cross Entropy: This is a term commonly utilised in information theory, and it measures the differences between two probability distributions that can be used to identify an observation.\n- Binary: This is an expression of numerical digits using either of two states, 0 or 1. This is extended to the definition of Binary Classification where we ditingus=ish two classes(A and B) using binary representation, where class A is assigned the numerical representation of 0 and class B is assigned 1.\n\n$L(y , f(x)) = -[y * log(f(x)) + (1 - y) * log(1 - f(x))]$\n\nWhere:\n\n    L represents the Binary Cross-Entropy Loss function\n    y is the true binary label (0 or 1)\n    f(x) is the predicted probability of the positive class (between 0 and 1)\n\n### Categorical Cross-Entropy Loss\n\n### Hinge Loss\n> Hinge Loss is a loss function utilized within machine learning to train classifiers that optimize to increase the margin between data points and the decision boundary. Hence, it is mainly used for maximum margin classifications. To ensure the maximum margin between the data points and boundaries, hinge loss penalizes predictions from the machine learning model that are wrongly classified, which are predictions that fall on the wrong side of the margin boundary and also predictions that are correctly classified but are within close proximity to the decision boundary.\n\n$L(y - f(x)) = \\max(0, 1 - y * f(x))$\nWhere:\n\n    L represents the Hinge Loss\n    y is the true label or target value (-1 or 1)\n    f(x) is the predicted value or decision function output\n\n### Log Loss\n \n \n \n\n## Applicability to Regression\n\n### Mean Square Error (MSE) / L2 Loss\n> MSE is a standard loss function utilized in most regression tasks since it directs the model to optimize to minimize the squared differences between the predicted and target values.\n\n$MSE  = \\dfrac{1}{n} * \\sum(y_i -\\bar{y})^2$\n\nWhere:\n\n    n is the number of samples in the dataset\n    yᵢ is the predicted value for the i-th sample\n    ȳ is the target value for the i-th sample\n\n### Mean Absolute Error (MAE) / L1 Loss\n> A scenario where MAE is an applicable loss function is one where we don't want to penalize outliers considerably or at all, for example, predicting delivery times for a food delivery company.\n\n\n$MAE  = \\dfrac{1}{n} * \\sum(y_i -\\bar{y})$\n\nWhere:\n\n    n is the number of samples in the dataset\n    yᵢ is the predicted value for the i-th sample\n    ȳ is the target value for the i-th sample\n\n### Huber Loss / Smooth Mean Absolute Error\n> The Huber Loss function effectively combines two components for handling errors differently, with the transition point between these components determined by the threshold δ:\\\n> Quadratic Component for Small Errors: For errors smaller than δ, it uses the quadratic component (1/2) * (f(x) - y)^2\\\n> Linear Component for Large Errors: For errors larger than δ, it applies the linear component δ * |f(x) - y| - (1/2) * δ^2\n\n$L(\\delta, y , f(x)) = \\dfrac{1}{2} * (f(x) - y)^2 \\quad if \\quad |f(x) - y| <= \\delta$\\\n$\\quad \\quad \\quad \\quad = \\delta * |f(x) - y| - \\dfrac{1}{2} * \\delta^2 \\quad if \\quad |f(x) - y| > \\delta$\n\nWhere:\n\n    L represents the Huber Loss function\n    δ is the delta parameter, which determines the threshold for switching between the quadratic and linear components of the loss function\n    y is the true value or target value\n    f(x) is the predicted value\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"loss_or_cost_funtions.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.27","comments":{"utterances":{"repo":"quarto-dev/quarto-web"}},"author":"Benedict Thekkel","theme":{"light":"flatly","dark":"darkly"},"description":"Loss or Cost funtions","title":"Loss or Cost funtions"},"extensions":{"book":{"multiFile":true}}},"gfm":{"identifier":{"display-name":"Github (GFM)","target-format":"gfm","base-format":"gfm"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"png","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":false,"output-ext":"md","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"variant":"+autolink_bare_uris+emoji+footnotes+gfm_auto_identifiers+pipe_tables+strikeout+task_lists+tex_math_dollars"},"pandoc":{"standalone":true,"default-image-extension":"png","to":"commonmark","output-file":"loss_or_cost_funtions.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"comments":{"utterances":{"repo":"quarto-dev/quarto-web"}},"description":"Loss or Cost funtions","title":"Loss or Cost funtions"}}},"projectFormats":["html","gfm"]}