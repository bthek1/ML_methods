[
  {
    "objectID": "cnns.html",
    "href": "cnns.html",
    "title": "Convolution Neural Networks",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "bagging_heart_disease_prediction.html",
    "href": "bagging_heart_disease_prediction.html",
    "title": "Ensemble Tutorial",
    "section": "",
    "text": "dataset credits: https://www.kaggle.com/fedesoriano/heart-failure-prediction\nimport pandas as pd\n\ndf = pd.read_csv(\"Data/heart.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nAge\nSex\nChestPainType\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nHeartDisease\n\n\n\n\n0\n40\nM\nATA\n140\n289\n0\nNormal\n172\nN\n0.0\nUp\n0\n\n\n1\n49\nF\nNAP\n160\n180\n0\nNormal\n156\nN\n1.0\nFlat\n1\n\n\n2\n37\nM\nATA\n130\n283\n0\nST\n98\nN\n0.0\nUp\n0\n\n\n3\n48\nF\nASY\n138\n214\n0\nNormal\n108\nY\n1.5\nFlat\n1\n\n\n4\n54\nM\nNAP\n150\n195\n0\nNormal\n122\nN\n0.0\nUp\n0\ndf.shape\n\n(918, 12)\ndf.describe()\n\n\n\n\n\n\n\n\nAge\nRestingBP\nCholesterol\nFastingBS\nMaxHR\nOldpeak\nHeartDisease\n\n\n\n\ncount\n918.000000\n918.000000\n918.000000\n918.000000\n918.000000\n918.000000\n918.000000\n\n\nmean\n53.510893\n132.396514\n198.799564\n0.233115\n136.809368\n0.887364\n0.553377\n\n\nstd\n9.432617\n18.514154\n109.384145\n0.423046\n25.460334\n1.066570\n0.497414\n\n\nmin\n28.000000\n0.000000\n0.000000\n0.000000\n60.000000\n-2.600000\n0.000000\n\n\n25%\n47.000000\n120.000000\n173.250000\n0.000000\n120.000000\n0.000000\n0.000000\n\n\n50%\n54.000000\n130.000000\n223.000000\n0.000000\n138.000000\n0.600000\n1.000000\n\n\n75%\n60.000000\n140.000000\n267.000000\n0.000000\n156.000000\n1.500000\n1.000000\n\n\nmax\n77.000000\n200.000000\n603.000000\n1.000000\n202.000000\n6.200000\n1.000000\ndf[df.Cholesterol&gt;(df.Cholesterol.mean()+3*df.Cholesterol.std())]\n\n\n\n\n\n\n\n\nAge\nSex\nChestPainType\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nHeartDisease\n\n\n\n\n76\n32\nM\nASY\n118\n529\n0\nNormal\n130\nN\n0.0\nFlat\n1\n\n\n149\n54\nM\nASY\n130\n603\n1\nNormal\n125\nY\n1.0\nFlat\n1\n\n\n616\n67\nF\nNAP\n115\n564\n0\nLVH\n160\nN\n1.6\nFlat\n0\ndf.shape\n\n(918, 12)\ndf1 = df[df.Cholesterol&lt;=(df.Cholesterol.mean()+3*df.Cholesterol.std())]\ndf1.shape\n\n(915, 12)\ndf[df.MaxHR&gt;(df.MaxHR.mean()+3*df.MaxHR.std())]\n\n\n\n\n\n\n\n\nAge\nSex\nChestPainType\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nHeartDisease\ndf[df.FastingBS&gt;(df.FastingBS.mean()+3*df.FastingBS.std())]\n\n\n\n\n\n\n\n\nAge\nSex\nChestPainType\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nHeartDisease\ndf[df.Oldpeak&gt;(df.Oldpeak.mean()+3*df.Oldpeak.std())]\n\n\n\n\n\n\n\n\nAge\nSex\nChestPainType\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nHeartDisease\n\n\n\n\n166\n50\nM\nASY\n140\n231\n0\nST\n140\nY\n5.0\nFlat\n1\n\n\n702\n59\nM\nTA\n178\n270\n0\nLVH\n145\nN\n4.2\nDown\n0\n\n\n771\n55\nM\nASY\n140\n217\n0\nNormal\n111\nY\n5.6\nDown\n1\n\n\n791\n51\nM\nASY\n140\n298\n0\nNormal\n122\nY\n4.2\nFlat\n1\n\n\n850\n62\nF\nASY\n160\n164\n0\nLVH\n145\nN\n6.2\nDown\n1\n\n\n900\n58\nM\nASY\n114\n318\n0\nST\n140\nN\n4.4\nDown\n1\ndf2 = df1[df1.Oldpeak&lt;=(df1.Oldpeak.mean()+3*df1.Oldpeak.std())]\ndf2.shape\n\n(909, 12)\ndf[df.RestingBP&gt;(df.RestingBP.mean()+3*df.RestingBP.std())]\n\n\n\n\n\n\n\n\nAge\nSex\nChestPainType\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nHeartDisease\n\n\n\n\n109\n39\nM\nATA\n190\n241\n0\nNormal\n106\nN\n0.0\nUp\n0\n\n\n241\n54\nM\nASY\n200\n198\n0\nNormal\n142\nY\n2.0\nFlat\n1\n\n\n365\n64\nF\nASY\n200\n0\n0\nNormal\n140\nY\n1.0\nFlat\n1\n\n\n399\n61\nM\nNAP\n200\n0\n1\nST\n70\nN\n0.0\nFlat\n1\n\n\n592\n61\nM\nASY\n190\n287\n1\nLVH\n150\nY\n2.0\nDown\n1\n\n\n732\n56\nF\nASY\n200\n288\n1\nLVH\n133\nY\n4.0\nDown\n1\n\n\n759\n54\nM\nATA\n192\n283\n0\nLVH\n195\nN\n0.0\nUp\n1\ndf3 = df2[df2.RestingBP&lt;=(df2.RestingBP.mean()+3*df2.RestingBP.std())]\ndf3.shape\n\n(902, 12)\ndf.ChestPainType.unique()\n\narray(['ATA', 'NAP', 'ASY', 'TA'], dtype=object)\ndf.RestingECG.unique()\n\narray(['Normal', 'ST', 'LVH'], dtype=object)\ndf.ExerciseAngina.unique()\n\narray(['N', 'Y'], dtype=object)\ndf.ST_Slope.unique()\n\narray(['Up', 'Flat', 'Down'], dtype=object)\ndf4 = df3.copy()\ndf4.ExerciseAngina.replace(\n    {\n        'N': 0,\n        'Y': 1\n    },\n    inplace=True)\n\ndf4.ST_Slope.replace(\n    {\n        'Down': 1,\n        'Flat': 2,\n        'Up': 3\n    },\n    inplace=True\n)\n\ndf4.RestingECG.replace(\n    {\n        'Normal': 1,\n        'ST': 2,\n        'LVH': 3\n    },\n    inplace=True)\n\ndf4.head()\n\n\n\n\n\n\n\n\nAge\nSex\nChestPainType\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nHeartDisease\n\n\n\n\n0\n40\nM\nATA\n140\n289\n0\n1\n172\n0\n0.0\n3\n0\n\n\n1\n49\nF\nNAP\n160\n180\n0\n1\n156\n0\n1.0\n2\n1\n\n\n2\n37\nM\nATA\n130\n283\n0\n2\n98\n0\n0.0\n3\n0\n\n\n3\n48\nF\nASY\n138\n214\n0\n1\n108\n1\n1.5\n2\n1\n\n\n4\n54\nM\nNAP\n150\n195\n0\n1\n122\n0\n0.0\n3\n0\ndf5 = pd.get_dummies(df4, drop_first=True)\ndf5.head()\n\n\n\n\n\n\n\n\nAge\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nHeartDisease\nSex_M\nChestPainType_ATA\nChestPainType_NAP\nChestPainType_TA\n\n\n\n\n0\n40\n140\n289\n0\n1\n172\n0\n0.0\n3\n0\n1\n1\n0\n0\n\n\n1\n49\n160\n180\n0\n1\n156\n0\n1.0\n2\n1\n0\n0\n1\n0\n\n\n2\n37\n130\n283\n0\n2\n98\n0\n0.0\n3\n0\n1\n1\n0\n0\n\n\n3\n48\n138\n214\n0\n1\n108\n1\n1.5\n2\n1\n0\n0\n0\n0\n\n\n4\n54\n150\n195\n0\n1\n122\n0\n0.0\n3\n0\n1\n0\n1\n0\nX = df5.drop(\"HeartDisease\",axis='columns')\ny = df5.HeartDisease\n\nX.head()\n\n\n\n\n\n\n\n\nAge\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nSex_M\nChestPainType_ATA\nChestPainType_NAP\nChestPainType_TA\n\n\n\n\n0\n40\n140\n289\n0\n1\n172\n0\n0.0\n3\n1\n1\n0\n0\n\n\n1\n49\n160\n180\n0\n1\n156\n0\n1.0\n2\n0\n0\n1\n0\n\n\n2\n37\n130\n283\n0\n2\n98\n0\n0.0\n3\n1\n1\n0\n0\n\n\n3\n48\n138\n214\n0\n1\n108\n1\n1.5\n2\n0\n0\n0\n0\n\n\n4\n54\n150\n195\n0\n1\n122\n0\n0.0\n3\n1\n0\n1\n0\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nX_scaled\n\narray([[-1.42896269,  0.46089071,  0.85238015, ...,  2.06757196,\n        -0.53547478, -0.22914788],\n       [-0.47545956,  1.5925728 , -0.16132855, ..., -0.4836591 ,\n         1.86750159, -0.22914788],\n       [-1.74679706, -0.10495034,  0.79657967, ...,  2.06757196,\n        -0.53547478, -0.22914788],\n       ...,\n       [ 0.37209878, -0.10495034, -0.61703246, ..., -0.4836591 ,\n        -0.53547478, -0.22914788],\n       [ 0.37209878, -0.10495034,  0.35947592, ...,  2.06757196,\n        -0.53547478, -0.22914788],\n       [-1.64085227,  0.3477225 , -0.20782894, ..., -0.4836591 ,\n         1.86750159, -0.22914788]])\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=20)\nX_train.shape\n\n(721, 13)\nX_test.shape\n\n(181, 13)\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(SVC(), X, y, cv=5)\nscores.mean()\n\n0.6906445672191528\nUse bagging now with svm\nfrom sklearn.ensemble import BaggingClassifier\n\nbag_model = BaggingClassifier(estimator=SVC(), n_estimators=100, max_samples=0.8, random_state=0)\nscores = cross_val_score(bag_model, X, y, cv=5)\nscores.mean()\n\n0.6839656230816453\nAs you can see above, using bagging in case of SVM doesn’t make much difference in terms of model accuracy. Bagging is effective when we have high variance and instable model such as decision tree. Let’s explore how bagging changes the performance for a decision tree classifier.\nfrom sklearn.tree import DecisionTreeClassifier\n\nscores = cross_val_score(DecisionTreeClassifier(random_state=0), X, y, cv=5)\nscores.mean()\n\n0.7193984039287907\nUse bagging now with decision tree\nbag_model = BaggingClassifier(\n    estimator=DecisionTreeClassifier(random_state=0), \n    n_estimators=100, \n    max_samples=0.9, \n    oob_score=True,\n    random_state=0\n)\n\nscores = cross_val_score(bag_model, X, y, cv=5)\nscores.mean()\n\n0.8037016574585636\nYou can see that with bagging the score improved from 71.93% to 80.37%\nfrom sklearn.ensemble import RandomForestClassifier\n\nscores = cross_val_score(RandomForestClassifier(), X, y, cv=5)\nscores.mean()\n\n0.826998158379374"
  },
  {
    "objectID": "bagging_heart_disease_prediction.html#boosting",
    "href": "bagging_heart_disease_prediction.html#boosting",
    "title": "Ensemble Tutorial",
    "section": "Boosting",
    "text": "Boosting\n\nfrom sklearn.ensemble import AdaBoostClassifier\n\n\nclf = AdaBoostClassifier(\n    n_estimators=100,\n    random_state=0,\n    algorithm='SAMME')\n\n\nclf.fit(X_train, y_train)\n\nAdaBoostClassifier(algorithm='SAMME', n_estimators=100, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.AdaBoostClassifierAdaBoostClassifier(algorithm='SAMME', n_estimators=100, random_state=0)\n\n\n\nclf.score(X_test, y_test)\n\n0.8397790055248618\n\n\nRandom forest gave even a better performance with 81.7% as score. Underneath it used bagging where it sampled not only data rows but also the columns (or features)"
  },
  {
    "objectID": "linearregression.html",
    "href": "linearregression.html",
    "title": "Linear Regression",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\npath = Path('Data/homeprices.csv')\ndf = pd.read_csv(path)\ndf\n\n\n\n\n\n\n\n\narea\nbedrooms\nage\nprice\n\n\n\n\n0\n2600\n3.0\n20\n550000\n\n\n1\n3000\n4.0\n15\n565000\n\n\n2\n3200\nNaN\n18\n610000\n\n\n3\n3600\n3.0\n30\n595000\n\n\n4\n4000\n5.0\n8\n760000\n\n\n5\n4100\n6.0\n8\n810000\nplt.xlabel('area')\nplt.ylabel('price')\nplt.scatter(df.area,df.price,color='red',marker='+')\n\nplt.show()\nnew_df = df.drop('price',axis='columns')\nnew_df = new_df.drop('bedrooms',axis='columns')\nnew_df = new_df.drop('age',axis='columns')\nnew_df\n\n\n\n\n\n\n\n\narea\n\n\n\n\n0\n2600\n\n\n1\n3000\n\n\n2\n3200\n\n\n3\n3600\n\n\n4\n4000\n\n\n5\n4100\nprice = df.price\nprice\n\n0    550000\n1    565000\n2    610000\n3    595000\n4    760000\n5    810000\nName: price, dtype: int64\nfrom sklearn import linear_model\n# Create linear regression object\nreg = linear_model.LinearRegression()\nreg.fit(new_df,price)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\nreg.predict([[3300]])\n\n/home/ben/mambaforge/envs/cfast/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n  warnings.warn(\n\n\narray([628813.88621022])\nreg.coef_\n\narray([167.30954677])\nreg.intercept_\n\n76692.3818707813\nY = m * X + b (m is coefficient and b is intercept)\n5000*reg.coef_ + reg.intercept_\n\narray([913240.11571842])\nreg.predict([[5000]])\n\n/home/ben/mambaforge/envs/cfast/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n  warnings.warn(\n\n\narray([913240.11571842])"
  },
  {
    "objectID": "linearregression.html#generate-csv-file-with-list-of-home-price-predictions",
    "href": "linearregression.html#generate-csv-file-with-list-of-home-price-predictions",
    "title": "Linear Regression",
    "section": "Generate CSV file with list of home price predictions",
    "text": "Generate CSV file with list of home price predictions"
  },
  {
    "objectID": "backpropagation.html",
    "href": "backpropagation.html",
    "title": "Back Propagation",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "logisticregression.html",
    "href": "logisticregression.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "Binary Classification\nMulticlass Classification"
  },
  {
    "objectID": "logisticregression.html#classification-types",
    "href": "logisticregression.html#classification-types",
    "title": "Logistic Regression",
    "section": "",
    "text": "Binary Classification\nMulticlass Classification"
  },
  {
    "objectID": "logisticregression.html#binary-classification",
    "href": "logisticregression.html#binary-classification",
    "title": "Logistic Regression",
    "section": "Binary Classification",
    "text": "Binary Classification\n\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom pathlib import Path\nimport numpy as np\n\n\npath = Path('Data/insurance_data.csv')\ndf = pd.read_csv(path)\ndf.head()\n\n\n\n\n\n\n\n\nage\nbought_insurance\n\n\n\n\n0\n22\n0\n\n\n1\n25\n0\n\n\n2\n47\n1\n\n\n3\n52\n0\n\n\n4\n46\n1\n\n\n\n\n\n\n\n\\(\\text{sigmoid}(z) = \\dfrac{1}{1+e^-z}\\) where e = Euler’s number ~ 2.71828\n\nplt.scatter(df.age,df.bought_insurance,marker='+',color='red')\n\n&lt;matplotlib.collections.PathCollection&gt;\n\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\n\nX_train, X_test, y_train, y_test = train_test_split(df[['age']],df.bought_insurance,train_size=0.8)\n\n\nX_test\n\n\n\n\n\n\n\n\nage\n\n\n\n\n12\n27\n\n\n25\n54\n\n\n7\n60\n\n\n2\n47\n\n\n10\n18\n\n\n9\n61\n\n\n\n\n\n\n\n\nmodel = LogisticRegression()\n\n\nmodel.fit(X_train, y_train)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\nX_test\n\n\n\n\n\n\n\n\nage\n\n\n\n\n12\n27\n\n\n25\n54\n\n\n7\n60\n\n\n2\n47\n\n\n10\n18\n\n\n9\n61\n\n\n\n\n\n\n\n\ny_predicted = model.predict(X_test)\ny_predicted\n\narray([0, 1, 1, 1, 0, 1])\n\n\n\ny_probability = model.predict_proba(X_test)\n\n\nmodel.score(X_test,y_test)\n\n1.0\n\n\n\narray1 = np.array(X_test)\n\n# Stack the arrays horizontally\ncombined_array = np.hstack((array1, y_probability[:,1].reshape(-1, 1)))\n\nprint(\"Combined Array:\")\nprint(combined_array)\n\nCombined Array:\n[[27.          0.18888707]\n [54.          0.84143616]\n [60.          0.91401439]\n [47.          0.70233749]\n [18.          0.07590529]\n [61.          0.92268871]]\n\n\n\nsorted_array = np.sort(combined_array, 0)\nsorted_array[:,0]\n\narray([18., 27., 47., 54., 60., 61.])\n\n\n\nplt.scatter(df.age,df.bought_insurance,marker='+',color='blue')\nplt.scatter(X_test,y_test,marker='+',color='red')\nplt.plot(sorted_array[:,0],sorted_array[:,1],marker='+',color='green')\nplt.scatter(X_test,y_predicted,marker='*',color='green')\n\n&lt;matplotlib.collections.PathCollection&gt;"
  },
  {
    "objectID": "logisticregression.html#multiclass-regression",
    "href": "logisticregression.html#multiclass-regression",
    "title": "Logistic Regression",
    "section": "Multiclass Regression",
    "text": "Multiclass Regression\n\nfrom sklearn.datasets import load_digits\nimport matplotlib.pyplot as plt\n\n\ndigits = load_digits()\n\n\nplt.gray() \nfor i in range(2):\n    plt.matshow(digits.images[i])\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\ndir(digits)\n\n['DESCR', 'data', 'feature_names', 'frame', 'images', 'target', 'target_names']\n\n\n\ndigits.target[:]\n\narray([0, 1, 2, ..., 8, 9, 8])\n\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\n\nmodel = LogisticRegression()\n\n\nX_train, X_test, y_train, y_test = train_test_split(digits.data,digits.target, test_size=0.2)\n\n\nmodel.fit(X_train, y_train)\n\n/home/ben/mambaforge/envs/cfast/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\nmodel.score(X_test, y_test)\n\n0.9722222222222222\n\n\n\nmodel.predict(digits.data[0:5])\n\narray([0, 1, 2, 3, 4])\n\n\n\ny_predicted = model.predict(X_test)\n\narray([5, 4, 0, 2, 9, 5, 3, 2, 0, 4, 1, 3, 5, 1, 5, 3, 6, 3, 5, 2, 3, 2,\n       0, 8, 1, 9, 6, 7, 0, 8, 9, 4, 5, 7, 2, 4, 4, 4, 8, 3, 7, 8, 3, 6,\n       4, 9, 2, 4, 6, 3, 5, 1, 6, 0, 7, 9, 4, 8, 8, 3, 8, 9, 5, 6, 4, 9,\n       8, 5, 2, 0, 7, 7, 6, 2, 5, 8, 9, 5, 7, 5, 5, 4, 4, 8, 9, 8, 9, 2,\n       1, 0, 7, 4, 8, 6, 3, 3, 3, 8, 1, 1, 5, 6, 7, 6, 1, 7, 2, 8, 1, 5,\n       3, 4, 4, 9, 5, 0, 7, 0, 6, 3, 2, 2, 4, 3, 4, 8, 6, 0, 8, 0, 3, 1,\n       4, 9, 0, 3, 2, 9, 9, 6, 7, 8, 4, 6, 8, 6, 9, 0, 4, 9, 7, 6, 8, 3,\n       9, 6, 0, 7, 1, 7, 2, 5, 2, 3, 3, 8, 0, 0, 9, 4, 4, 5, 9, 0, 8, 8,\n       7, 9, 9, 8, 3, 3, 8, 7, 0, 4, 6, 6, 1, 1, 9, 0, 3, 1, 3, 9, 2, 8,\n       3, 7, 4, 5, 5, 7, 2, 1, 9, 5, 5, 7, 9, 1, 9, 1, 7, 6, 5, 1, 6, 7,\n       5, 6, 7, 2, 9, 4, 9, 0, 8, 3, 3, 6, 0, 1, 3, 3, 9, 6, 1, 5, 1, 6,\n       6, 3, 1, 0, 1, 0, 2, 2, 1, 9, 7, 9, 1, 0, 9, 1, 3, 8, 1, 5, 0, 0,\n       8, 6, 1, 2, 6, 6, 9, 5, 3, 6, 3, 8, 9, 8, 6, 9, 7, 2, 8, 5, 9, 6,\n       9, 7, 3, 7, 4, 3, 2, 1, 5, 8, 0, 8, 6, 6, 7, 5, 6, 6, 4, 6, 3, 7,\n       2, 3, 6, 8, 5, 3, 1, 6, 8, 8, 9, 0, 8, 5, 6, 8, 0, 1, 2, 0, 0, 1,\n       9, 6, 7, 6, 3, 2, 0, 5, 5, 2, 7, 1, 6, 4, 6, 0, 2, 5, 2, 0, 7, 6,\n       9, 6, 1, 9, 1, 1, 0, 0])\n\n\n\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sn\n\n\ncm = confusion_matrix(y_test, y_predicted)\nplt.figure(figsize = (10,7))\nsn.heatmap(cm, annot=True)\nplt.xlabel('Predicted')\nplt.ylabel('Truth')\n\nText(95.72222222222221, 0.5, 'Truth')\n\n\n\n\n\n\nfrom sklearn.metrics import classification_report\n\n\nreport = classification_report(y_test, y_predicted)\nprint(report)\n\n              precision    recall  f1-score   support\n\n           0       0.97      1.00      0.99        35\n           1       0.97      0.97      0.97        36\n           2       0.93      0.96      0.95        28\n           3       0.97      0.95      0.96        40\n           4       0.96      1.00      0.98        27\n           5       0.94      0.94      0.94        35\n           6       1.00      1.00      1.00        46\n           7       1.00      0.97      0.98        33\n           8       0.97      0.97      0.97        38\n           9       0.98      0.95      0.96        42\n\n    accuracy                           0.97       360\n   macro avg       0.97      0.97      0.97       360\nweighted avg       0.97      0.97      0.97       360"
  },
  {
    "objectID": "polynomialregression.html",
    "href": "polynomialregression.html",
    "title": "Polynomial Regression",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nX = np.random.rand(100,1)\ny = 4 + 5 * X + 1 * np.random.randn(100, 1)\nplt.scatter(X, y)\nplt.show()\nreg = LinearRegression()\nreg.fit(X, y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\nX_vals = np.linspace(0, 1, 100).reshape(-1,1)\ny_vals = reg.predict(X_vals)\nplt.scatter(X, y)\nplt.plot(X_vals, y_vals, color ='g')\nplt.show()"
  },
  {
    "objectID": "polynomialregression.html#second-order",
    "href": "polynomialregression.html#second-order",
    "title": "Polynomial Regression",
    "section": "Second Order",
    "text": "Second Order\n\nX = 4 * np.random.rand(50,1) -2\ny = 4 + 2 * X + 5 * X ** 2 + 2 * np.random.randn(50, 1)\n\n\npoly_features = PolynomialFeatures(degree = 2, include_bias = False)\nX_poly = poly_features.fit_transform(X)\n\n\nreg1 = LinearRegression()\nreg1.fit(X_poly, y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nX_vals = np.linspace(-2, 2, 50).reshape(-1,1)\nX_vals_poly = poly_features.transform(X_vals)\ny_vals = reg1.predict(X_vals_poly)\nplt.scatter(X, y)\nplt.plot(X_vals, y_vals, color ='g')\nplt.show()\n\n\n\n\n\nX_vals_poly[:,1]\n\narray([4.00000000e+00, 3.68013328e+00, 3.37359434e+00, 3.08038317e+00,\n       2.80049979e+00, 2.53394419e+00, 2.28071637e+00, 2.04081633e+00,\n       1.81424406e+00, 1.60099958e+00, 1.40108288e+00, 1.21449396e+00,\n       1.04123282e+00, 8.81299459e-01, 7.34693878e-01, 6.01416077e-01,\n       4.81466056e-01, 3.74843815e-01, 2.81549354e-01, 2.01582674e-01,\n       1.34943773e-01, 8.16326531e-02, 4.16493128e-02, 1.49937526e-02,\n       1.66597251e-03, 1.66597251e-03, 1.49937526e-02, 4.16493128e-02,\n       8.16326531e-02, 1.34943773e-01, 2.01582674e-01, 2.81549354e-01,\n       3.74843815e-01, 4.81466056e-01, 6.01416077e-01, 7.34693878e-01,\n       8.81299459e-01, 1.04123282e+00, 1.21449396e+00, 1.40108288e+00,\n       1.60099958e+00, 1.81424406e+00, 2.04081633e+00, 2.28071637e+00,\n       2.53394419e+00, 2.80049979e+00, 3.08038317e+00, 3.37359434e+00,\n       3.68013328e+00, 4.00000000e+00])\n\n\n\nplt.scatter(X_vals, X_vals_poly[:,1])\n\n&lt;matplotlib.collections.PathCollection&gt;"
  },
  {
    "objectID": "polynomialregression.html#higher-order",
    "href": "polynomialregression.html#higher-order",
    "title": "Polynomial Regression",
    "section": "Higher Order",
    "text": "Higher Order\n\nX = 4 * np.random.rand(50,1) -2\ny = 4 + 2 * X + 5 * X ** 2 + 12 * X ** 3 + 2 * X ** 4 + + 2 * np.random.randn(50, 1)\n\npoly_features = PolynomialFeatures(degree = 4, include_bias = False)\nX_poly = poly_features.fit_transform(X)\n\nreg2 = LinearRegression()\nreg2.fit(X_poly, y)\n\nX_vals = np.linspace(-2, 2, 50).reshape(-1,1)\nX_vals_poly = poly_features.transform(X_vals)\ny_vals = reg2.predict(X_vals_poly)\nplt.scatter(X, y)\nplt.plot(X_vals, y_vals, color ='g')\nplt.show()"
  },
  {
    "objectID": "hyperparameteroptimization.html",
    "href": "hyperparameteroptimization.html",
    "title": "Hyper Parameter Optimization",
    "section": "",
    "text": "For iris flower dataset in sklearn library, we are going to find out best model and best hyper parameters using GridSearchCV\nLoad iris flower dataset\nfrom sklearn import svm, datasets\niris = datasets.load_iris()\nimport pandas as pd\ndf = pd.DataFrame(iris.data,columns=iris.feature_names)\ndf['flower'] = iris.target\ndf['flower'] = df['flower'].apply(lambda x: iris.target_names[x])\ndf[47:150]\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\nflower\n\n\n\n\n47\n4.6\n3.2\n1.4\n0.2\nsetosa\n\n\n48\n5.3\n3.7\n1.5\n0.2\nsetosa\n\n\n49\n5.0\n3.3\n1.4\n0.2\nsetosa\n\n\n50\n7.0\n3.2\n4.7\n1.4\nversicolor\n\n\n51\n6.4\n3.2\n4.5\n1.5\nversicolor\n\n\n...\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n146\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n147\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n148\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n149\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n\n103 rows × 5 columns"
  },
  {
    "objectID": "hyperparameteroptimization.html#approach-1-use-train_test_split-and-manually-tune-parameters-by-trial-and-error",
    "href": "hyperparameteroptimization.html#approach-1-use-train_test_split-and-manually-tune-parameters-by-trial-and-error",
    "title": "Hyper Parameter Optimization",
    "section": "Approach 1: Use train_test_split and manually tune parameters by trial and error",
    "text": "Approach 1: Use train_test_split and manually tune parameters by trial and error\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3)\n\n\nmodel = svm.SVC(kernel='rbf',C=30,gamma='auto')\nmodel.fit(X_train,y_train)\nmodel.score(X_test, y_test)\n\n0.9555555555555556"
  },
  {
    "objectID": "hyperparameteroptimization.html#approach-2-use-k-fold-cross-validation",
    "href": "hyperparameteroptimization.html#approach-2-use-k-fold-cross-validation",
    "title": "Hyper Parameter Optimization",
    "section": "Approach 2: Use K Fold Cross validation",
    "text": "Approach 2: Use K Fold Cross validation\nManually try suppling models with different parameters to cross_val_score function with 5 fold cross validation\n\ncross_val_score(svm.SVC(kernel='linear',C=10,gamma='auto'),iris.data, iris.target, cv=5)\n\n\n\n\narray([1.        , 1.        , 0.9       , 0.96666667, 1.        ])\n\n\n\ncross_val_score(svm.SVC(kernel='rbf',C=10,gamma='auto'),iris.data, iris.target, cv=5)\n\n\n\n\narray([0.96666667, 1.        , 0.96666667, 0.96666667, 1.        ])\n\n\n\ncross_val_score(svm.SVC(kernel='rbf',C=20,gamma='auto'),iris.data, iris.target, cv=5)\n\n\n\n\narray([0.96666667, 1.        , 0.9       , 0.96666667, 1.        ])\n\n\nAbove approach is tiresome and very manual. We can use for loop as an alternative\n\nkernels = ['rbf', 'linear']\nC = [1,10,20]\navg_scores = {}\nfor kval in kernels:\n    for cval in C:\n        cv_scores = cross_val_score(svm.SVC(kernel=kval,C=cval,gamma='auto'),iris.data, iris.target, cv=5)\n        avg_scores[kval + '_' + str(cval)] = np.average(cv_scores)\n\navg_scores\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{'rbf_1': 0.9800000000000001,\n 'rbf_10': 0.9800000000000001,\n 'rbf_20': 0.9666666666666668,\n 'linear_1': 0.9800000000000001,\n 'linear_10': 0.9733333333333334,\n 'linear_20': 0.9666666666666666}\n\n\nFrom above results we can say that rbf with C=1 or 10 or linear with C=1 will give best performance"
  },
  {
    "objectID": "hyperparameteroptimization.html#approach-3-use-gridsearchcv",
    "href": "hyperparameteroptimization.html#approach-3-use-gridsearchcv",
    "title": "Hyper Parameter Optimization",
    "section": "Approach 3: Use GridSearchCV",
    "text": "Approach 3: Use GridSearchCV\nGridSearchCV does exactly same thing as for loop above but in a single line of code\n\nfrom sklearn.model_selection import GridSearchCV\nclf = GridSearchCV(svm.SVC(gamma='auto'), {\n    'C': [1,10,20],\n    'kernel': ['rbf','linear']\n}, cv=5, return_train_score=False)\nclf.fit(iris.data, iris.target)\nclf.cv_results_\n\n{'mean_fit_time': array([0.00118256, 0.00104566, 0.0007266 , 0.00084271, 0.00088511,\n        0.00065002]),\n 'std_fit_time': array([6.73577797e-04, 4.38131552e-04, 1.68876617e-04, 3.96551972e-04,\n        4.28567104e-04, 8.82425265e-05]),\n 'mean_score_time': array([0.00081396, 0.00048532, 0.00048227, 0.00054522, 0.00063434,\n        0.00044188]),\n 'std_score_time': array([4.28254909e-04, 1.10279724e-04, 1.84006103e-04, 1.59358051e-04,\n        3.89407295e-04, 6.20551122e-05]),\n 'param_C': masked_array(data=[1, 1, 10, 10, 20, 20],\n              mask=[False, False, False, False, False, False],\n        fill_value='?',\n             dtype=object),\n 'param_kernel': masked_array(data=['rbf', 'linear', 'rbf', 'linear', 'rbf', 'linear'],\n              mask=[False, False, False, False, False, False],\n        fill_value='?',\n             dtype=object),\n 'params': [{'C': 1, 'kernel': 'rbf'},\n  {'C': 1, 'kernel': 'linear'},\n  {'C': 10, 'kernel': 'rbf'},\n  {'C': 10, 'kernel': 'linear'},\n  {'C': 20, 'kernel': 'rbf'},\n  {'C': 20, 'kernel': 'linear'}],\n 'split0_test_score': array([0.96666667, 0.96666667, 0.96666667, 1.        , 0.96666667,\n        1.        ]),\n 'split1_test_score': array([1., 1., 1., 1., 1., 1.]),\n 'split2_test_score': array([0.96666667, 0.96666667, 0.96666667, 0.9       , 0.9       ,\n        0.9       ]),\n 'split3_test_score': array([0.96666667, 0.96666667, 0.96666667, 0.96666667, 0.96666667,\n        0.93333333]),\n 'split4_test_score': array([1., 1., 1., 1., 1., 1.]),\n 'mean_test_score': array([0.98      , 0.98      , 0.98      , 0.97333333, 0.96666667,\n        0.96666667]),\n 'std_test_score': array([0.01632993, 0.01632993, 0.01632993, 0.03887301, 0.03651484,\n        0.0421637 ]),\n 'rank_test_score': array([1, 1, 1, 4, 5, 6], dtype=int32)}\n\n\n\ndf = pd.DataFrame(clf.cv_results_)\ndf\n\n\n\n\n\n\n\n\nmean_fit_time\nstd_fit_time\nmean_score_time\nstd_score_time\nparam_C\nparam_kernel\nparams\nsplit0_test_score\nsplit1_test_score\nsplit2_test_score\nsplit3_test_score\nsplit4_test_score\nmean_test_score\nstd_test_score\nrank_test_score\n\n\n\n\n0\n0.001183\n0.000674\n0.000814\n0.000428\n1\nrbf\n{'C': 1, 'kernel': 'rbf'}\n0.966667\n1.0\n0.966667\n0.966667\n1.0\n0.980000\n0.016330\n1\n\n\n1\n0.001046\n0.000438\n0.000485\n0.000110\n1\nlinear\n{'C': 1, 'kernel': 'linear'}\n0.966667\n1.0\n0.966667\n0.966667\n1.0\n0.980000\n0.016330\n1\n\n\n2\n0.000727\n0.000169\n0.000482\n0.000184\n10\nrbf\n{'C': 10, 'kernel': 'rbf'}\n0.966667\n1.0\n0.966667\n0.966667\n1.0\n0.980000\n0.016330\n1\n\n\n3\n0.000843\n0.000397\n0.000545\n0.000159\n10\nlinear\n{'C': 10, 'kernel': 'linear'}\n1.000000\n1.0\n0.900000\n0.966667\n1.0\n0.973333\n0.038873\n4\n\n\n4\n0.000885\n0.000429\n0.000634\n0.000389\n20\nrbf\n{'C': 20, 'kernel': 'rbf'}\n0.966667\n1.0\n0.900000\n0.966667\n1.0\n0.966667\n0.036515\n5\n\n\n5\n0.000650\n0.000088\n0.000442\n0.000062\n20\nlinear\n{'C': 20, 'kernel': 'linear'}\n1.000000\n1.0\n0.900000\n0.933333\n1.0\n0.966667\n0.042164\n6\n\n\n\n\n\n\n\n\ndf[['param_C','param_kernel','mean_test_score']]\n\n\n\n\n\n\n\n\nparam_C\nparam_kernel\nmean_test_score\n\n\n\n\n0\n1\nrbf\n0.980000\n\n\n1\n1\nlinear\n0.980000\n\n\n2\n10\nrbf\n0.980000\n\n\n3\n10\nlinear\n0.973333\n\n\n4\n20\nrbf\n0.966667\n\n\n5\n20\nlinear\n0.966667\n\n\n\n\n\n\n\n\nclf.best_params_\n\n{'C': 1, 'kernel': 'rbf'}\n\n\n\nclf.best_score_\n\n0.9800000000000001\n\n\n\ndir(clf)\n\n['__abstractmethods__',\n '__annotations__',\n '__class__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getstate__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__setstate__',\n '__sizeof__',\n '__sklearn_clone__',\n '__str__',\n '__subclasshook__',\n '__weakref__',\n '_abc_impl',\n '_build_request_for_signature',\n '_check_feature_names',\n '_check_n_features',\n '_check_refit_for_multimetric',\n '_estimator_type',\n '_format_results',\n '_get_default_requests',\n '_get_metadata_request',\n '_get_param_names',\n '_get_tags',\n '_more_tags',\n '_parameter_constraints',\n '_repr_html_',\n '_repr_html_inner',\n '_repr_mimebundle_',\n '_required_parameters',\n '_run_search',\n '_select_best_index',\n '_validate_data',\n '_validate_params',\n 'best_estimator_',\n 'best_index_',\n 'best_params_',\n 'best_score_',\n 'classes_',\n 'cv',\n 'cv_results_',\n 'decision_function',\n 'error_score',\n 'estimator',\n 'fit',\n 'get_metadata_routing',\n 'get_params',\n 'inverse_transform',\n 'multimetric_',\n 'n_features_in_',\n 'n_jobs',\n 'n_splits_',\n 'param_grid',\n 'pre_dispatch',\n 'predict',\n 'predict_log_proba',\n 'predict_proba',\n 'refit',\n 'refit_time_',\n 'return_train_score',\n 'score',\n 'score_samples',\n 'scorer_',\n 'scoring',\n 'set_fit_request',\n 'set_params',\n 'transform',\n 'verbose']\n\n\nUse RandomizedSearchCV to reduce number of iterations and with random combination of parameters. This is useful when you have too many parameters to try and your training time is longer. It helps reduce the cost of computation\n\nfrom sklearn.model_selection import RandomizedSearchCV\nrs = RandomizedSearchCV(svm.SVC(gamma='auto'), {\n        'C': [1,10,20],\n        'kernel': ['rbf','linear']\n    }, \n    cv=5, \n    return_train_score=False, \n    n_iter=2\n)\nrs.fit(iris.data, iris.target)\npd.DataFrame(rs.cv_results_)[['param_C','param_kernel','mean_test_score']]\n\n\n\n\n\n\n\n\nparam_C\nparam_kernel\nmean_test_score\n\n\n\n\n0\n20\nrbf\n0.966667\n\n\n1\n1\nrbf\n0.980000\n\n\n\n\n\n\n\nHow about different models with different hyperparameters?\n\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier\n\n\nmodel_params = {\n    'svm': {\n        'model': svm.SVC(gamma='auto'),\n        'params' : {\n            'C': list(range(1, 21)),\n            'kernel': ['rbf','linear']\n        }  \n    },\n    'random_forest': {\n        'model': RandomForestClassifier(),\n        'params' : {\n            'n_estimators': list(range(1, 11))\n        }\n    },\n    'logistic_regression' : {\n        'model': LogisticRegression(solver='liblinear',multi_class='auto'),\n        'params': {\n            'C': list(range(1, 11))\n        }\n    },\n    'naive_bayes_gaussian': {\n        'model': GaussianNB(),\n        'params': {}\n    },\n    'naive_bayes_multinomial': {\n        'model': MultinomialNB(),\n        'params': {}\n    },\n    'decision_tree': {\n        'model': DecisionTreeClassifier(),\n        'params': {\n            'criterion': ['gini','entropy'],\n            \n        }\n    }     \n}\n\n\nscores = []\n\nfor model_name, mp in model_params.items():\n    clf =  GridSearchCV(mp['model'], mp['params'], cv=5, return_train_score=False)\n    clf.fit(iris.data, iris.target)\n    scores.append({\n        'model': model_name,\n        'best_score': clf.best_score_,\n        'best_params': clf.best_params_\n    })\n    \ndf = pd.DataFrame(scores,columns=['model','best_score','best_params'])\ndf\n\n\n\n\n\n\n\n\nmodel\nbest_score\nbest_params\n\n\n\n\n0\nsvm\n0.986667\n{'C': 4, 'kernel': 'rbf'}\n\n\n1\nrandom_forest\n0.966667\n{'n_estimators': 4}\n\n\n2\nlogistic_regression\n0.966667\n{'C': 4}\n\n\n3\nnaive_bayes_gaussian\n0.953333\n{}\n\n\n4\nnaive_bayes_multinomial\n0.953333\n{}\n\n\n5\ndecision_tree\n0.966667\n{'criterion': 'gini'}\n\n\n\n\n\n\n\nBased on above, I can conclude that SVM with C=1 and kernel=‘rbf’ is the best model for solving my problem of iris flower classification"
  },
  {
    "objectID": "k-nn.html",
    "href": "k-nn.html",
    "title": "K-NN",
    "section": "",
    "text": "def sayhello(name): return f'Hello {name}'"
  },
  {
    "objectID": "k-nn.html#advantages",
    "href": "k-nn.html#advantages",
    "title": "K-NN",
    "section": "Advantages",
    "text": "Advantages\n\nEasy to implement: Given the algorithm’s simplicity and accuracy, it is one of the first classifiers that a new data scientist will learn.\nAdapts easily: As new training samples are added, the algorithm adjusts to account for any new data since all training data is stored into memory.\nFew hyperparameters: KNN only requires a k value and a distance metric, which is low when compared to other machine learning algorithms."
  },
  {
    "objectID": "k-nn.html#disadvantages",
    "href": "k-nn.html#disadvantages",
    "title": "K-NN",
    "section": "Disadvantages",
    "text": "Disadvantages\n\nDoes not scale well: Since KNN is a lazy algorithm, it takes up more memory and data storage compared to other classifiers. This can be costly from both a time and money perspective. More memory and storage will drive up business expenses and more data can take longer to compute. While different data structures, such as Ball-Tree, have been created to address the computational inefficiencies, a different classifier may be ideal depending on the business problem.\nCurse of dimensionality: The KNN algorithm tends to fall victim to the curse of dimensionality, which means that it doesn’t perform well with high-dimensional data inputs. This is sometimes also referred to as the peaking phenomenon (PDF, 340 MB) (link resides outside of ibm.com), where after the algorithm attains the optimal number of features, additional features increases the amount of classification errors, especially when the sample size is smaller.\nProne to overfitting: Due to the “curse of dimensionality”, KNN is also more prone to overfitting. While feature selection and dimensionality reduction techniques are leveraged to prevent this from occurring, the value of k can also impact the model’s behavior. Lower values of k can overfit the data, whereas higher values of k tend to “smooth out” the prediction values since it is averaging the values over a greater area, or neighborhood. However, if the value of k is too high, then it can underfit the data."
  },
  {
    "objectID": "k-nn.html#distance-metrics",
    "href": "k-nn.html#distance-metrics",
    "title": "K-NN",
    "section": "Distance Metrics",
    "text": "Distance Metrics\n\nEuclidean distance (p=2): This is the most commonly used distance measure\nManhattan distance (p=1): This is also another popular distance metric, which measures the absolute value between two points.\nMinkowski distance: This distance measure is the generalized form of Euclidean and Manhattan distance metrics.\nHamming distance: This technique is used typically used with Boolean or string vectors, identifying the points where the vectors do not match."
  },
  {
    "objectID": "k-nn.html#compute-knn-defining-k",
    "href": "k-nn.html#compute-knn-defining-k",
    "title": "K-NN",
    "section": "Compute KNN: defining k",
    "text": "Compute KNN: defining k\nThe k value in the k-NN algorithm defines how many neighbors will be checked to determine the classification of a specific query point. For example, if k=1, the instance will be assigned to the same class as its single nearest neighbor. Defining k can be a balancing act as different values can lead to overfitting or underfitting. Lower values of k can have high variance, but low bias, and larger values of k may lead to high bias and lower variance. The choice of k will largely depend on the input data as data with more outliers or noise will likely perform better with higher values of k. Overall, it is recommended to have an odd number for k to avoid ties in classification, and cross-validation tactics can help you choose the optimal k for your dataset."
  },
  {
    "objectID": "k-nn.html#data-collection",
    "href": "k-nn.html#data-collection",
    "title": "K-NN",
    "section": "Data collection",
    "text": "Data collection\n\n!pip list | grep pandas\n!pip list | grep scikit-learn\n\npandas                        2.0.3\nscikit-learn                  1.3.0\n\n\n\nimport pandas as pd\nfrom sklearn.datasets import load_iris\n\n\niris = load_iris()\n\n\niris.feature_names\n\n['sepal length (cm)',\n 'sepal width (cm)',\n 'petal length (cm)',\n 'petal width (cm)']\n\n\n\niris.target_names\n\narray(['setosa', 'versicolor', 'virginica'], dtype='&lt;U10')\n\n\n\niris.target\n\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n\n\n\ndf = pd.DataFrame(iris.data,columns=iris.feature_names)\ndf.head()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n4.9\n3.0\n1.4\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\n\n\n3\n4.6\n3.1\n1.5\n0.2\n\n\n4\n5.0\n3.6\n1.4\n0.2\n\n\n\n\n\n\n\n\ndf['target'] = iris.target\ndf.head()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n0\n\n\n1\n4.9\n3.0\n1.4\n0.2\n0\n\n\n2\n4.7\n3.2\n1.3\n0.2\n0\n\n\n3\n4.6\n3.1\n1.5\n0.2\n0\n\n\n4\n5.0\n3.6\n1.4\n0.2\n0\n\n\n\n\n\n\n\n\ndf[df.target==1].head()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\n50\n7.0\n3.2\n4.7\n1.4\n1\n\n\n51\n6.4\n3.2\n4.5\n1.5\n1\n\n\n52\n6.9\n3.1\n4.9\n1.5\n1\n\n\n53\n5.5\n2.3\n4.0\n1.3\n1\n\n\n54\n6.5\n2.8\n4.6\n1.5\n1\n\n\n\n\n\n\n\n\ndf[df.target==2].head()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\n100\n6.3\n3.3\n6.0\n2.5\n2\n\n\n101\n5.8\n2.7\n5.1\n1.9\n2\n\n\n102\n7.1\n3.0\n5.9\n2.1\n2\n\n\n103\n6.3\n2.9\n5.6\n1.8\n2\n\n\n104\n6.5\n3.0\n5.8\n2.2\n2\n\n\n\n\n\n\n\n\ndf['flower_name'] =df.target.apply(lambda x: iris.target_names[x])\ndf.head()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\nflower_name\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n0\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\n0\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\n0\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\n0\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\n0\nsetosa\n\n\n\n\n\n\n\n\ndf0 = df[:50]\ndf1 = df[50:100]\ndf2 = df[100:]"
  },
  {
    "objectID": "k-nn.html#data-visuals",
    "href": "k-nn.html#data-visuals",
    "title": "K-NN",
    "section": "Data Visuals",
    "text": "Data Visuals\n\nimport matplotlib.pyplot as plt\n\n\nplt.xlabel('Sepal Length')\nplt.ylabel('Sepal Width')\nplt.scatter(df0['sepal length (cm)'],\n            df0['sepal width (cm)'],\n            color=\"green\",marker='+')\n\nplt.scatter(df1['sepal length (cm)'],\n            df1['sepal width (cm)'],\n            color=\"blue\",marker='.')\n\nplt.scatter(df2['sepal length (cm)'],\n            df2['sepal width (cm)'],\n            color=\"red\",marker='*')\n\n&lt;matplotlib.collections.PathCollection&gt;\n\n\n\n\n\n\nplt.xlabel('Petal Length')\nplt.ylabel('Petal Width')\nplt.scatter(df0['petal length (cm)'],\n            df0['petal width (cm)'],\n            color=\"green\",marker='+')\n\nplt.scatter(df1['petal length (cm)'],\n            df1['petal width (cm)'],\n            color=\"blue\",marker='.')\n\nplt.scatter(df2['sepal length (cm)'],\n            df2['sepal width (cm)'],\n            color=\"red\",marker='*')\n\n&lt;matplotlib.collections.PathCollection&gt;"
  },
  {
    "objectID": "k-nn.html#model-design",
    "href": "k-nn.html#model-design",
    "title": "K-NN",
    "section": "Model Design",
    "text": "Model Design\n\nfrom sklearn.model_selection import train_test_split\n\n\nX = df.drop(['target','flower_name'], axis='columns')\ny = df.target\n\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,\n                                                    test_size=0.2,\n                                                    random_state=1)\n\n\nfrom sklearn.preprocessing import StandardScaler\n\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n\nlen(X_train)\n\n120\n\n\n\nlen(X_test)\n\n30"
  },
  {
    "objectID": "k-nn.html#first-model-k-nn-10",
    "href": "k-nn.html#first-model-k-nn-10",
    "title": "K-NN",
    "section": "First Model: K-nn 10",
    "text": "First Model: K-nn 10\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\nn_neighbors=10\nknn = KNeighborsClassifier(n_neighbors)\nknn.fit(X_train, y_train)\n\nKNeighborsClassifier(n_neighbors=10)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifierKNeighborsClassifier(n_neighbors=10)\n\n\n\nknn.score(X_test, y_test)\n\n1.0\n\n\n\nknn.predict([[4.8,3.0,1.5,0.3]])\n\narray([2])\n\n\n\nfrom sklearn.metrics import confusion_matrix\n\n\ny_pred = knn.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\ncm\n\narray([[11,  0,  0],\n       [ 0, 13,  0],\n       [ 0,  0,  6]])\n\n\n\n!pip list | grep seaborn || pip install seaborn\n\nseaborn                       0.12.2\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sn\n\n\nplt.figure(figsize=(7,5))\nsn.heatmap(cm, annot=True)\nplt.xlabel('Predicted')\nplt.ylabel('Truth')\n\nText(58.222222222222214, 0.5, 'Truth')\n\n\n\n\n\n\nfrom sklearn.metrics import classification_report\n\n\nprint(classification_report(y_test, y_pred))\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        11\n           1       1.00      1.00      1.00        13\n           2       1.00      1.00      1.00         6\n\n    accuracy                           1.00        30\n   macro avg       1.00      1.00      1.00        30\nweighted avg       1.00      1.00      1.00        30\n\n\n\n\nfrom matplotlib.colors import ListedColormap\nimport numpy as np\n\n\n# Plot the decision regions\ncmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\ncmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n\n# Define the resolution of the grid\nh = 0.02\nx_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\ny_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\nx1_min, x1_max = X_train[:, 2].min() - 1, X_train[:, 2].max() + 1\ny1_min, y1_max = X_train[:, 3].min() - 1, X_train[:, 3].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nxx1, yy1 = np.meshgrid(np.arange(x1_min, x1_max, h), np.arange(y1_min, y1_max, h))\n\n\n# Predict the class labels for each point in the grid\nZ = knn.predict(np.c_[xx.ravel(), yy.ravel(),xx.ravel(), yy.ravel() ])\nZ = Z.reshape(xx.shape)\n\nprint(f'{len(xx)} {len(yy)} {len(xx1)} {len(yy1)} ')\n\n387 387 256 256 \n\n\n\n# Plot the decision regions\nplt.figure()\nplt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n# Plot the training points\nplt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cmap_bold,\n            edgecolor='k', s=20)\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.title(\"k-NN classifier with k={}\".format(10))\n\n\nplt.show()\n\n\n\n\n\nlen(xx1.ravel())\n\n68608\n\n\n\n# Plot the decision regions\nplt.figure()\nplt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n# Plot the training points\nplt.scatter(X_train[:, 2], X_train[:, 3], c=y_train, cmap=cmap_bold,\n            edgecolor='k', s=20)\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.title(\"k-NN classifier with k={}\".format(10))\n\nText(0.5, 1.0, 'k-NN classifier with k=10')"
  },
  {
    "objectID": "k-nn.html#knn-model-with-gridsearchcv-finding-optimum-k",
    "href": "k-nn.html#knn-model-with-gridsearchcv-finding-optimum-k",
    "title": "K-NN",
    "section": "Knn model with GridSearchCV: finding optimum K",
    "text": "Knn model with GridSearchCV: finding optimum K\n\nfrom sklearn.model_selection import GridSearchCV\n\n\ncreate a dictionary of all values we want to test for n_neighbors\n\n\nknn2 = KNeighborsClassifier()\nparam_grid = {'n_neighbors': np.arange(1, 25)}\n\n\nuse gridsearch to test all values for n_neighbors\n\n\nknn_gscv = GridSearchCV(knn2, param_grid, cv=5)#fit model to data\nknn_gscv.fit(X, y)\n\nGridSearchCV(cv=5, estimator=KNeighborsClassifier(),\n             param_grid={'n_neighbors': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24])})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5, estimator=KNeighborsClassifier(),\n             param_grid={'n_neighbors': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24])})estimator: KNeighborsClassifierKNeighborsClassifier()KNeighborsClassifierKNeighborsClassifier()\n\n\n\n#check top performing n_neighbors value\nknn_gscv.best_params_\n\n{'n_neighbors': 6}"
  },
  {
    "objectID": "decisiontree.html",
    "href": "decisiontree.html",
    "title": "Decision Tree",
    "section": "",
    "text": "Simple to understand, interpret, visualize.\nDecision trees implicitly performvariable screening or feature selection.\nCan handle both numerical and categorical data. Can also handle multi-output problems.\nDecision trees require relatively little effort from users for data preparation.\nNonlinear relationships between parameters do not affect tree performance."
  },
  {
    "objectID": "decisiontree.html#advantages-of-cart",
    "href": "decisiontree.html#advantages-of-cart",
    "title": "Decision Tree",
    "section": "",
    "text": "Simple to understand, interpret, visualize.\nDecision trees implicitly performvariable screening or feature selection.\nCan handle both numerical and categorical data. Can also handle multi-output problems.\nDecision trees require relatively little effort from users for data preparation.\nNonlinear relationships between parameters do not affect tree performance."
  },
  {
    "objectID": "decisiontree.html#disadvantages-of-cart",
    "href": "decisiontree.html#disadvantages-of-cart",
    "title": "Decision Tree",
    "section": "Disadvantages of CART",
    "text": "Disadvantages of CART\n\nDecision-tree learners can create over-complex trees that do not generalize the data well. This is called overfitting.\nDecision trees can be unstable because small variations in the data might result in a completely different tree being generated. This is called variance, which needs to be lowered by methods like bagging and boosting.\nGreedy algorithms cannot guarantee to return the globally optimal decision tree. This can be mitigated by training multiple trees, where the features and samples are randomly sampled with replacement.\nDecision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the data set prior to fitting with the decision tree.\n\n\nimport pandas as pd\nfrom fastai.vision.all import *\n\n\npath = Path('Data')\nname = 'salaries.csv'\n\n\ndf = pd.read_csv(path/name)\ndf.head()\n\n\n\n\n\n\n\n\ncompany\njob\ndegree\nsalary_more_then_100k\n\n\n\n\n0\ngoogle\nsales executive\nbachelors\n0\n\n\n1\ngoogle\nsales executive\nmasters\n0\n\n\n2\ngoogle\nbusiness manager\nbachelors\n1\n\n\n3\ngoogle\nbusiness manager\nmasters\n1\n\n\n4\ngoogle\ncomputer programmer\nbachelors\n0\n\n\n\n\n\n\n\n\ninputs = df.drop('salary_more_then_100k',axis='columns')\n\n\ntarget = df['salary_more_then_100k']\n\n\nfrom sklearn.preprocessing import LabelEncoder\n\n\nle_company = LabelEncoder()\nle_job = LabelEncoder()\nle_degree = LabelEncoder()\n\n\ninputs['company_n'] = le_company.fit_transform(inputs['company'])\ninputs['job_n'] = le_job.fit_transform(inputs['job'])\ninputs['degree_n'] = le_degree.fit_transform(inputs['degree'])\n\n\ninputs\n\n\n\n\n\n\n\n\ncompany\njob\ndegree\ncompany_n\njob_n\ndegree_n\n\n\n\n\n0\ngoogle\nsales executive\nbachelors\n2\n2\n0\n\n\n1\ngoogle\nsales executive\nmasters\n2\n2\n1\n\n\n2\ngoogle\nbusiness manager\nbachelors\n2\n0\n0\n\n\n3\ngoogle\nbusiness manager\nmasters\n2\n0\n1\n\n\n4\ngoogle\ncomputer programmer\nbachelors\n2\n1\n0\n\n\n5\ngoogle\ncomputer programmer\nmasters\n2\n1\n1\n\n\n6\nabc pharma\nsales executive\nmasters\n0\n2\n1\n\n\n7\nabc pharma\ncomputer programmer\nbachelors\n0\n1\n0\n\n\n8\nabc pharma\nbusiness manager\nbachelors\n0\n0\n0\n\n\n9\nabc pharma\nbusiness manager\nmasters\n0\n0\n1\n\n\n10\nfacebook\nsales executive\nbachelors\n1\n2\n0\n\n\n11\nfacebook\nsales executive\nmasters\n1\n2\n1\n\n\n12\nfacebook\nbusiness manager\nbachelors\n1\n0\n0\n\n\n13\nfacebook\nbusiness manager\nmasters\n1\n0\n1\n\n\n14\nfacebook\ncomputer programmer\nbachelors\n1\n1\n0\n\n\n15\nfacebook\ncomputer programmer\nmasters\n1\n1\n1\n\n\n\n\n\n\n\n\ninputs_n = inputs.drop(['company','job','degree'],axis='columns')\n\n\ninputs_n\n\n\n\n\n\n\n\n\ncompany_n\njob_n\ndegree_n\n\n\n\n\n0\n2\n2\n0\n\n\n1\n2\n2\n1\n\n\n2\n2\n0\n0\n\n\n3\n2\n0\n1\n\n\n4\n2\n1\n0\n\n\n5\n2\n1\n1\n\n\n6\n0\n2\n1\n\n\n7\n0\n1\n0\n\n\n8\n0\n0\n0\n\n\n9\n0\n0\n1\n\n\n10\n1\n2\n0\n\n\n11\n1\n2\n1\n\n\n12\n1\n0\n0\n\n\n13\n1\n0\n1\n\n\n14\n1\n1\n0\n\n\n15\n1\n1\n1\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\nnp.random.seed(42)\nn_points = len(inputs_n)\nx = np.random.rand(n_points)\ny = np.random.rand(n_points)\nz = np.random.rand(n_points)\n\n\ninputs_n['company_n'] += (x-0.5)/10\ninputs_n['job_n'] += (y-0.5)/10\ninputs_n['degree_n'] += (z-0.5)/10\n\n\n# Create a 3D scatter plot\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\nax.set_xlabel('degree_n')\nax.set_ylabel('job_n')\nax.set_zlabel('company_n')\n\nscatter = ax.scatter(inputs_n['degree_n'],\n           inputs_n['job_n'],\n           inputs_n['company_n'],\n           c=target,\n           cmap='viridis',\n           marker='+')\n\n# Adding a color bar to show the mapping of colors to values in 'color_column'\ncbar = fig.colorbar(scatter, ax=ax)\ncbar.set_label('Color Column')\n\n\n\n\n\ntarget\n\n0     0\n1     0\n2     1\n3     1\n4     0\n5     1\n6     0\n7     0\n8     0\n9     1\n10    1\n11    1\n12    1\n13    1\n14    1\n15    1\nName: salary_more_then_100k, dtype: int64\n\n\n\nfrom sklearn import tree\n\n\nmodel = tree.DecisionTreeClassifier()\nmodel.fit(inputs_n, target)\n\nDecisionTreeClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifierDecisionTreeClassifier()\n\n\n\nmodel.score(inputs_n,target)\n\n1.0\n\n\n\nfrom sklearn.tree import export_graphviz\n\n\nFEATURE_NAMES = ['company_n', 'job_n', 'degree_n']\nexport_graphviz(model, './Data/salary.dot', feature_names = FEATURE_NAMES)\n\n\n!dot -Tpng ./Data/salary.dot -o ./Data/salary.png\n\n\nimport matplotlib.pyplot as plt\nimport cv2 as cv\n\n\nimg = cv.imread('./Data/salary.png')\nplt.figure(figsize = (20, 20))\nplt.imshow(img)\n\n&lt;matplotlib.image.AxesImage&gt;"
  },
  {
    "objectID": "decisiontree.html#predict",
    "href": "decisiontree.html#predict",
    "title": "Decision Tree",
    "section": "Predict",
    "text": "Predict\n\nmodel.predict([[2,1,0]])\n\n/home/ben/mambaforge/envs/cfast/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n  warnings.warn(\n\n\narray([0])\n\n\n\nmodel.predict([[2,1,1]])\n\n/home/ben/mambaforge/envs/cfast/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n  warnings.warn(\n\n\narray([0])"
  },
  {
    "objectID": "naive_bayes_2_email_spam_filter.html",
    "href": "naive_bayes_2_email_spam_filter.html",
    "title": "The Naive Bayes Approach",
    "section": "",
    "text": "import pandas as pd\n\n\ndf = pd.read_csv(\"Data/spam.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nCategory\nMessage\n\n\n\n\n0\nham\nGo until jurong point, crazy.. Available only ...\n\n\n1\nham\nOk lar... Joking wif u oni...\n\n\n2\nspam\nFree entry in 2 a wkly comp to win FA Cup fina...\n\n\n3\nham\nU dun say so early hor... U c already then say...\n\n\n4\nham\nNah I don't think he goes to usf, he lives aro...\n\n\n\n\n\n\n\n\ndf.groupby('Category').describe()\n\n\n\n\n\n\n\n\nMessage\n\n\n\ncount\nunique\ntop\nfreq\n\n\nCategory\n\n\n\n\n\n\n\n\nham\n4825\n4516\nSorry, I'll call later\n30\n\n\nspam\n747\n641\nPlease call our customer service representativ...\n4\n\n\n\n\n\n\n\n\ndf['spam']=df['Category'].apply(lambda x: 1 if x=='spam' else 0)\ndf.head()\n\n\n\n\n\n\n\n\nCategory\nMessage\nspam\n\n\n\n\n0\nham\nGo until jurong point, crazy.. Available only ...\n0\n\n\n1\nham\nOk lar... Joking wif u oni...\n0\n\n\n2\nspam\nFree entry in 2 a wkly comp to win FA Cup fina...\n1\n\n\n3\nham\nU dun say so early hor... U c already then say...\n0\n\n\n4\nham\nNah I don't think he goes to usf, he lives aro...\n0\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df.Message,df.spam)\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nv = CountVectorizer()\nX_train_count = v.fit_transform(X_train.values)\nX_train_count.toarray()[:2]\n\narray([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]])\n\n\n\nfrom sklearn.naive_bayes import MultinomialNB\nmodel = MultinomialNB()\nmodel.fit(X_train_count,y_train)\n\nMultinomialNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MultinomialNBMultinomialNB()\n\n\n\nemails = [\n    'Hey mohan, can we get together to watch footbal game tomorrow?',\n    'Upto 20% discount on parking, exclusive offer just for you. Dont miss this reward!'\n]\nemails_count = v.transform(emails)\nmodel.predict(emails_count)\n\narray([0, 1])\n\n\n\nX_test_count = v.transform(X_test)\nmodel.score(X_test_count, y_test)\n\n0.9863603732950467\n\n\nSklearn Pipeline\n\nfrom sklearn.pipeline import Pipeline\nclf = Pipeline([\n    ('vectorizer', CountVectorizer()),\n    ('nb', MultinomialNB())\n])\n\n\nclf.fit(X_train, y_train)\n\nPipeline(steps=[('vectorizer', CountVectorizer()), ('nb', MultinomialNB())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('vectorizer', CountVectorizer()), ('nb', MultinomialNB())])CountVectorizerCountVectorizer()MultinomialNBMultinomialNB()\n\n\n\nclf.score(X_test,y_test)\n\n0.9863603732950467\n\n\n\nclf.predict(emails)\n\narray([0, 1])\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "ensemble.html",
    "href": "ensemble.html",
    "title": "Ensemble Methods",
    "section": "",
    "text": "import pandas as pd\n\ndf = pd.read_csv(\"Data/diabetes.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nPregnancies\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nOutcome\n\n\n\n\n0\n6\n148\n72\n35\n0\n33.6\n0.627\n50\n1\n\n\n1\n1\n85\n66\n29\n0\n26.6\n0.351\n31\n0\n\n\n2\n8\n183\n64\n0\n0\n23.3\n0.672\n32\n1\n\n\n3\n1\n89\n66\n23\n94\n28.1\n0.167\n21\n0\n\n\n4\n0\n137\n40\n35\n168\n43.1\n2.288\n33\n1\n\n\n\n\n\n\n\n\ndf.isnull().sum()\n\nPregnancies                 0\nGlucose                     0\nBloodPressure               0\nSkinThickness               0\nInsulin                     0\nBMI                         0\nDiabetesPedigreeFunction    0\nAge                         0\nOutcome                     0\ndtype: int64\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nPregnancies\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nOutcome\n\n\n\n\ncount\n768.000000\n768.000000\n768.000000\n768.000000\n768.000000\n768.000000\n768.000000\n768.000000\n768.000000\n\n\nmean\n3.845052\n120.894531\n69.105469\n20.536458\n79.799479\n31.992578\n0.471876\n33.240885\n0.348958\n\n\nstd\n3.369578\n31.972618\n19.355807\n15.952218\n115.244002\n7.884160\n0.331329\n11.760232\n0.476951\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.078000\n21.000000\n0.000000\n\n\n25%\n1.000000\n99.000000\n62.000000\n0.000000\n0.000000\n27.300000\n0.243750\n24.000000\n0.000000\n\n\n50%\n3.000000\n117.000000\n72.000000\n23.000000\n30.500000\n32.000000\n0.372500\n29.000000\n0.000000\n\n\n75%\n6.000000\n140.250000\n80.000000\n32.000000\n127.250000\n36.600000\n0.626250\n41.000000\n1.000000\n\n\nmax\n17.000000\n199.000000\n122.000000\n99.000000\n846.000000\n67.100000\n2.420000\n81.000000\n1.000000\n\n\n\n\n\n\n\n\ndf.Outcome.value_counts()\n\n0    500\n1    268\nName: Outcome, dtype: int64\n\n\nThere is slight imbalance in our dataset but since it is not major we will not worry about it!\n\n\nX = df.drop(\"Outcome\",axis=\"columns\")\ny = df.Outcome\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nX_scaled[:3]\n\narray([[ 0.63994726,  0.84832379,  0.14964075,  0.90726993, -0.69289057,\n         0.20401277,  0.46849198,  1.4259954 ],\n       [-0.84488505, -1.12339636, -0.16054575,  0.53090156, -0.69289057,\n        -0.68442195, -0.36506078, -0.19067191],\n       [ 1.23388019,  1.94372388, -0.26394125, -1.28821221, -0.69289057,\n        -1.10325546,  0.60439732, -0.10558415]])\n\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, stratify=y, random_state=10)\n\n\nX_train.shape\n\n(576, 8)\n\n\n\nX_test.shape\n\n(192, 8)\n\n\n\ny_train.value_counts()\n\n0    375\n1    201\nName: Outcome, dtype: int64\n\n\n\n201/375\n\n0.536\n\n\n\ny_test.value_counts()\n\n0    125\n1     67\nName: Outcome, dtype: int64\n\n\n\n67/125\n\n0.536\n\n\n\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\n\nscores = cross_val_score(DecisionTreeClassifier(), X, y, cv=5)\nscores\n\narray([0.68831169, 0.68181818, 0.68831169, 0.78431373, 0.73202614])\n\n\n\nscores.mean()\n\n0.7149562855445208\n\n\n\n\nfrom sklearn.ensemble import BaggingClassifier\n\nbag_model = BaggingClassifier(\n   estimator=DecisionTreeClassifier(), \n    n_estimators=100, \n    max_samples=0.8, \n    oob_score=True,\n    random_state=0\n)\nbag_model.fit(X_train, y_train)\nbag_model.oob_score_\n\n0.7534722222222222\n\n\n\nbag_model.score(X_test, y_test)\n\n0.7760416666666666\n\n\n\nbag_model = BaggingClassifier(\n    estimator=DecisionTreeClassifier(), \n    n_estimators=100, \n    max_samples=0.8, \n    oob_score=True,\n    random_state=0\n)\nscores = cross_val_score(bag_model, X, y, cv=5)\nscores\n\narray([0.75324675, 0.72727273, 0.74675325, 0.82352941, 0.73856209])\n\n\n\nscores.mean()\n\n0.7578728461081402\n\n\nWe can see some improvement in test score with bagging classifier as compared to a standalone classifier\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nscores = cross_val_score(RandomForestClassifier(n_estimators=100), X, y, cv=5)\nscores.mean()\n\n0.772192513368984"
  },
  {
    "objectID": "ensemble.html#bagging",
    "href": "ensemble.html#bagging",
    "title": "Ensemble Methods",
    "section": "",
    "text": "import pandas as pd\n\ndf = pd.read_csv(\"Data/diabetes.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nPregnancies\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nOutcome\n\n\n\n\n0\n6\n148\n72\n35\n0\n33.6\n0.627\n50\n1\n\n\n1\n1\n85\n66\n29\n0\n26.6\n0.351\n31\n0\n\n\n2\n8\n183\n64\n0\n0\n23.3\n0.672\n32\n1\n\n\n3\n1\n89\n66\n23\n94\n28.1\n0.167\n21\n0\n\n\n4\n0\n137\n40\n35\n168\n43.1\n2.288\n33\n1\n\n\n\n\n\n\n\n\ndf.isnull().sum()\n\nPregnancies                 0\nGlucose                     0\nBloodPressure               0\nSkinThickness               0\nInsulin                     0\nBMI                         0\nDiabetesPedigreeFunction    0\nAge                         0\nOutcome                     0\ndtype: int64\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nPregnancies\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nOutcome\n\n\n\n\ncount\n768.000000\n768.000000\n768.000000\n768.000000\n768.000000\n768.000000\n768.000000\n768.000000\n768.000000\n\n\nmean\n3.845052\n120.894531\n69.105469\n20.536458\n79.799479\n31.992578\n0.471876\n33.240885\n0.348958\n\n\nstd\n3.369578\n31.972618\n19.355807\n15.952218\n115.244002\n7.884160\n0.331329\n11.760232\n0.476951\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.078000\n21.000000\n0.000000\n\n\n25%\n1.000000\n99.000000\n62.000000\n0.000000\n0.000000\n27.300000\n0.243750\n24.000000\n0.000000\n\n\n50%\n3.000000\n117.000000\n72.000000\n23.000000\n30.500000\n32.000000\n0.372500\n29.000000\n0.000000\n\n\n75%\n6.000000\n140.250000\n80.000000\n32.000000\n127.250000\n36.600000\n0.626250\n41.000000\n1.000000\n\n\nmax\n17.000000\n199.000000\n122.000000\n99.000000\n846.000000\n67.100000\n2.420000\n81.000000\n1.000000\n\n\n\n\n\n\n\n\ndf.Outcome.value_counts()\n\n0    500\n1    268\nName: Outcome, dtype: int64\n\n\nThere is slight imbalance in our dataset but since it is not major we will not worry about it!\n\n\nX = df.drop(\"Outcome\",axis=\"columns\")\ny = df.Outcome\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nX_scaled[:3]\n\narray([[ 0.63994726,  0.84832379,  0.14964075,  0.90726993, -0.69289057,\n         0.20401277,  0.46849198,  1.4259954 ],\n       [-0.84488505, -1.12339636, -0.16054575,  0.53090156, -0.69289057,\n        -0.68442195, -0.36506078, -0.19067191],\n       [ 1.23388019,  1.94372388, -0.26394125, -1.28821221, -0.69289057,\n        -1.10325546,  0.60439732, -0.10558415]])\n\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, stratify=y, random_state=10)\n\n\nX_train.shape\n\n(576, 8)\n\n\n\nX_test.shape\n\n(192, 8)\n\n\n\ny_train.value_counts()\n\n0    375\n1    201\nName: Outcome, dtype: int64\n\n\n\n201/375\n\n0.536\n\n\n\ny_test.value_counts()\n\n0    125\n1     67\nName: Outcome, dtype: int64\n\n\n\n67/125\n\n0.536\n\n\n\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\n\nscores = cross_val_score(DecisionTreeClassifier(), X, y, cv=5)\nscores\n\narray([0.68831169, 0.68181818, 0.68831169, 0.78431373, 0.73202614])\n\n\n\nscores.mean()\n\n0.7149562855445208\n\n\n\n\nfrom sklearn.ensemble import BaggingClassifier\n\nbag_model = BaggingClassifier(\n   estimator=DecisionTreeClassifier(), \n    n_estimators=100, \n    max_samples=0.8, \n    oob_score=True,\n    random_state=0\n)\nbag_model.fit(X_train, y_train)\nbag_model.oob_score_\n\n0.7534722222222222\n\n\n\nbag_model.score(X_test, y_test)\n\n0.7760416666666666\n\n\n\nbag_model = BaggingClassifier(\n    estimator=DecisionTreeClassifier(), \n    n_estimators=100, \n    max_samples=0.8, \n    oob_score=True,\n    random_state=0\n)\nscores = cross_val_score(bag_model, X, y, cv=5)\nscores\n\narray([0.75324675, 0.72727273, 0.74675325, 0.82352941, 0.73856209])\n\n\n\nscores.mean()\n\n0.7578728461081402\n\n\nWe can see some improvement in test score with bagging classifier as compared to a standalone classifier\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nscores = cross_val_score(RandomForestClassifier(n_estimators=100), X, y, cv=5)\nscores.mean()\n\n0.772192513368984"
  },
  {
    "objectID": "ensemble.html#boosting",
    "href": "ensemble.html#boosting",
    "title": "Ensemble Methods",
    "section": "Boosting",
    "text": "Boosting\n\nGradient Boosting – It is a boosting technique that builds a final model from the sum of several weak learning algorithms that were trained on the same dataset. It operates on the idea of stagewise addition. The first weak learner in the gradient boosting algorithm will not be trained on the dataset; instead, it will simply return the mean of the relevant column. The residual for the first weak learner algorithm’s output will then be calculated and used as the output column or target column for the next weak learning algorithm that will be trained. The second weak learner will be trained using the same methodology, and the residuals will be computed and utilized as an output column once more for the third weak learner, and so on until we achieve zero residuals. The dataset for gradient boosting must be in the form of numerical or categorical data, and the loss function used to generate the residuals must be differential at all times.\nXGBoost – In addition to the gradient boosting technique, XGBoost is another boosting machine learning approach. The full name of the XGBoost algorithm is the eXtreme Gradient Boosting algorithm, which is an extreme variation of the previous gradient boosting technique. The key distinction between XGBoost and GradientBoosting is that XGBoost applies a regularisation approach. It is a regularised version of the current gradient-boosting technique. Because of this, XGBoost outperforms a standard gradient boosting method, which explains why it is also faster than that. Additionally, it works better when the dataset contains both numerical and categorical variables.\nAdaboost – AdaBoost is a boosting algorithm that also works on the principle of the stagewise addition method where multiple weak learners are used for getting strong learners. The value of the alpha parameter, in this case, will be indirectly proportional to the error of the weak learner, Unlike Gradient Boosting in XGBoost, the alpha parameter calculated is related to the errors of the weak learner, here the value of the alpha parameter will be indirectly proportional to the error of the weak learner.\nCatBoost – The growth of decision trees inside CatBoost is the primary distinction that sets it apart from and improves upon competitors. The decision trees that are created in CatBoost are symmetric. As there is a unique sort of approach for handling categorical datasets, CatBoost works very well on categorical datasets compared to any other algorithm in the field of machine learning. The categorical features in CatBoost are encoded based on the output columns. As a result, the output column’s weight will be taken into account while training or encoding the categorical features, increasing its accuracy on categorical datasets.\n\n\n# for classification\nfrom sklearn.ensemble import GradientBoostingClassifier\nmodel = GradientBoostingClassifier(n_estimators=100)\nmodel.fit(X_train, y_train)\n\nGradientBoostingClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GradientBoostingClassifierGradientBoostingClassifier()\n\n\n\nmodel.score(X_test, y_test)\n\n0.796875\n\n\n\nfrom sklearn.ensemble import AdaBoostClassifier\n\n\nclf = AdaBoostClassifier(\n    n_estimators=100,\n    random_state=0,\n    algorithm='SAMME')\n\n\nclf.fit(X_train, y_train)\n\nAdaBoostClassifier(algorithm='SAMME', n_estimators=100, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.AdaBoostClassifierAdaBoostClassifier(algorithm='SAMME', n_estimators=100, random_state=0)\n\n\n\nclf.score(X_test, y_test)\n\n0.796875"
  },
  {
    "objectID": "random_forest.html",
    "href": "random_forest.html",
    "title": "Random Forest",
    "section": "",
    "text": "Digits dataset from sklearn\n\nimport pandas as pd\nfrom sklearn.datasets import load_digits\ndigits = load_digits()\n\n\ndir(digits)\n\n['DESCR', 'data', 'feature_names', 'frame', 'images', 'target', 'target_names']\n\n\n\nimport matplotlib.pyplot as plt\n\n\nplt.gray() \nfor i in range(4):\n    plt.matshow(digits.images[i])\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf = pd.DataFrame(digits.data)\ndf.head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n\n\n\n\n0\n0.0\n0.0\n5.0\n13.0\n9.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n6.0\n13.0\n10.0\n0.0\n0.0\n0.0\n\n\n1\n0.0\n0.0\n0.0\n12.0\n13.0\n5.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n11.0\n16.0\n10.0\n0.0\n0.0\n\n\n2\n0.0\n0.0\n0.0\n4.0\n15.0\n12.0\n0.0\n0.0\n0.0\n0.0\n...\n5.0\n0.0\n0.0\n0.0\n0.0\n3.0\n11.0\n16.0\n9.0\n0.0\n\n\n3\n0.0\n0.0\n7.0\n15.0\n13.0\n1.0\n0.0\n0.0\n0.0\n8.0\n...\n9.0\n0.0\n0.0\n0.0\n7.0\n13.0\n13.0\n9.0\n0.0\n0.0\n\n\n4\n0.0\n0.0\n0.0\n1.0\n11.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n2.0\n16.0\n4.0\n0.0\n0.0\n\n\n\n\n5 rows × 64 columns\n\n\n\n\ndf['target'] = digits.target\n\n\ndf[0:12]\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n55\n56\n57\n58\n59\n60\n61\n62\n63\ntarget\n\n\n\n\n0\n0.0\n0.0\n5.0\n13.0\n9.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n6.0\n13.0\n10.0\n0.0\n0.0\n0.0\n0\n\n\n1\n0.0\n0.0\n0.0\n12.0\n13.0\n5.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n11.0\n16.0\n10.0\n0.0\n0.0\n1\n\n\n2\n0.0\n0.0\n0.0\n4.0\n15.0\n12.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n3.0\n11.0\n16.0\n9.0\n0.0\n2\n\n\n3\n0.0\n0.0\n7.0\n15.0\n13.0\n1.0\n0.0\n0.0\n0.0\n8.0\n...\n0.0\n0.0\n0.0\n7.0\n13.0\n13.0\n9.0\n0.0\n0.0\n3\n\n\n4\n0.0\n0.0\n0.0\n1.0\n11.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n2.0\n16.0\n4.0\n0.0\n0.0\n4\n\n\n5\n0.0\n0.0\n12.0\n10.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n9.0\n16.0\n16.0\n10.0\n0.0\n0.0\n5\n\n\n6\n0.0\n0.0\n0.0\n12.0\n13.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n1.0\n9.0\n15.0\n11.0\n3.0\n0.0\n6\n\n\n7\n0.0\n0.0\n7.0\n8.0\n13.0\n16.0\n15.0\n1.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n13.0\n5.0\n0.0\n0.0\n0.0\n0.0\n7\n\n\n8\n0.0\n0.0\n9.0\n14.0\n8.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n11.0\n16.0\n15.0\n11.0\n1.0\n0.0\n8\n\n\n9\n0.0\n0.0\n11.0\n12.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2.0\n...\n0.0\n0.0\n0.0\n9.0\n12.0\n13.0\n3.0\n0.0\n0.0\n9\n\n\n10\n0.0\n0.0\n1.0\n9.0\n15.0\n11.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n1.0\n10.0\n13.0\n3.0\n0.0\n0.0\n0\n\n\n11\n0.0\n0.0\n0.0\n0.0\n14.0\n13.0\n1.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n1.0\n13.0\n16.0\n1.0\n0.0\n1\n\n\n\n\n12 rows × 65 columns\n\n\n\nTrain and the model and prediction\n\nX = df.drop('target',axis='columns')\ny = df.target\n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier(n_estimators=20)\nmodel.fit(X_train, y_train)\n\nRandomForestClassifier(n_estimators=20)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(n_estimators=20)\n\n\n\n?RandomForestClassifier\n\n\nInit signature:\nRandomForestClassifier(\n    n_estimators=100,\n    *,\n    criterion='gini',\n    max_depth=None,\n    min_samples_split=2,\n    min_samples_leaf=1,\n    min_weight_fraction_leaf=0.0,\n    max_features='sqrt',\n    max_leaf_nodes=None,\n    min_impurity_decrease=0.0,\n    bootstrap=True,\n    oob_score=False,\n    n_jobs=None,\n    random_state=None,\n    verbose=0,\n    warm_start=False,\n    class_weight=None,\n    ccp_alpha=0.0,\n    max_samples=None,\n)\nDocstring:     \nA random forest classifier.\nA random forest is a meta estimator that fits a number of decision tree\nclassifiers on various sub-samples of the dataset and uses averaging to\nimprove the predictive accuracy and control over-fitting.\nThe sub-sample size is controlled with the `max_samples` parameter if\n`bootstrap=True` (default), otherwise the whole dataset is used to build\neach tree.\nFor a comparison between tree-based ensemble models see the example\n:ref:`sphx_glr_auto_examples_ensemble_plot_forest_hist_grad_boosting_comparison.py`.\nRead more in the :ref:`User Guide &lt;forest&gt;`.\nParameters\n----------\nn_estimators : int, default=100\n    The number of trees in the forest.\n    .. versionchanged:: 0.22\n       The default value of ``n_estimators`` changed from 10 to 100\n       in 0.22.\ncriterion : {\"gini\", \"entropy\", \"log_loss\"}, default=\"gini\"\n    The function to measure the quality of a split. Supported criteria are\n    \"gini\" for the Gini impurity and \"log_loss\" and \"entropy\" both for the\n    Shannon information gain, see :ref:`tree_mathematical_formulation`.\n    Note: This parameter is tree-specific.\nmax_depth : int, default=None\n    The maximum depth of the tree. If None, then nodes are expanded until\n    all leaves are pure or until all leaves contain less than\n    min_samples_split samples.\nmin_samples_split : int or float, default=2\n    The minimum number of samples required to split an internal node:\n    - If int, then consider `min_samples_split` as the minimum number.\n    - If float, then `min_samples_split` is a fraction and\n      `ceil(min_samples_split * n_samples)` are the minimum\n      number of samples for each split.\n    .. versionchanged:: 0.18\n       Added float values for fractions.\nmin_samples_leaf : int or float, default=1\n    The minimum number of samples required to be at a leaf node.\n    A split point at any depth will only be considered if it leaves at\n    least ``min_samples_leaf`` training samples in each of the left and\n    right branches.  This may have the effect of smoothing the model,\n    especially in regression.\n    - If int, then consider `min_samples_leaf` as the minimum number.\n    - If float, then `min_samples_leaf` is a fraction and\n      `ceil(min_samples_leaf * n_samples)` are the minimum\n      number of samples for each node.\n    .. versionchanged:: 0.18\n       Added float values for fractions.\nmin_weight_fraction_leaf : float, default=0.0\n    The minimum weighted fraction of the sum total of weights (of all\n    the input samples) required to be at a leaf node. Samples have\n    equal weight when sample_weight is not provided.\nmax_features : {\"sqrt\", \"log2\", None}, int or float, default=\"sqrt\"\n    The number of features to consider when looking for the best split:\n    - If int, then consider `max_features` features at each split.\n    - If float, then `max_features` is a fraction and\n      `max(1, int(max_features * n_features_in_))` features are considered at each\n      split.\n    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n    - If \"log2\", then `max_features=log2(n_features)`.\n    - If None, then `max_features=n_features`.\n    .. versionchanged:: 1.1\n        The default of `max_features` changed from `\"auto\"` to `\"sqrt\"`.\n    Note: the search for a split does not stop until at least one\n    valid partition of the node samples is found, even if it requires to\n    effectively inspect more than ``max_features`` features.\nmax_leaf_nodes : int, default=None\n    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n    Best nodes are defined as relative reduction in impurity.\n    If None then unlimited number of leaf nodes.\nmin_impurity_decrease : float, default=0.0\n    A node will be split if this split induces a decrease of the impurity\n    greater than or equal to this value.\n    The weighted impurity decrease equation is the following::\n        N_t / N * (impurity - N_t_R / N_t * right_impurity\n                            - N_t_L / N_t * left_impurity)\n    where ``N`` is the total number of samples, ``N_t`` is the number of\n    samples at the current node, ``N_t_L`` is the number of samples in the\n    left child, and ``N_t_R`` is the number of samples in the right child.\n    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n    if ``sample_weight`` is passed.\n    .. versionadded:: 0.19\nbootstrap : bool, default=True\n    Whether bootstrap samples are used when building trees. If False, the\n    whole dataset is used to build each tree.\noob_score : bool or callable, default=False\n    Whether to use out-of-bag samples to estimate the generalization score.\n    By default, :func:`~sklearn.metrics.accuracy_score` is used.\n    Provide a callable with signature `metric(y_true, y_pred)` to use a\n    custom metric. Only available if `bootstrap=True`.\nn_jobs : int, default=None\n    The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n    :meth:`decision_path` and :meth:`apply` are all parallelized over the\n    trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n    context. ``-1`` means using all processors. See :term:`Glossary\n    &lt;n_jobs&gt;` for more details.\nrandom_state : int, RandomState instance or None, default=None\n    Controls both the randomness of the bootstrapping of the samples used\n    when building trees (if ``bootstrap=True``) and the sampling of the\n    features to consider when looking for the best split at each node\n    (if ``max_features &lt; n_features``).\n    See :term:`Glossary &lt;random_state&gt;` for details.\nverbose : int, default=0\n    Controls the verbosity when fitting and predicting.\nwarm_start : bool, default=False\n    When set to ``True``, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble, otherwise, just fit a whole\n    new forest. See :term:`Glossary &lt;warm_start&gt;` and\n    :ref:`gradient_boosting_warm_start` for details.\nclass_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts,             default=None\n    Weights associated with classes in the form ``{class_label: weight}``.\n    If not given, all classes are supposed to have weight one. For\n    multi-output problems, a list of dicts can be provided in the same\n    order as the columns of y.\n    Note that for multioutput (including multilabel) weights should be\n    defined for each class of every column in its own dict. For example,\n    for four-class multilabel classification weights should be\n    [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n    [{1:1}, {2:5}, {3:1}, {4:1}].\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``\n    The \"balanced_subsample\" mode is the same as \"balanced\" except that\n    weights are computed based on the bootstrap sample for every tree\n    grown.\n    For multi-output, the weights of each column of y will be multiplied.\n    Note that these weights will be multiplied with sample_weight (passed\n    through the fit method) if sample_weight is specified.\nccp_alpha : non-negative float, default=0.0\n    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n    subtree with the largest cost complexity that is smaller than\n    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n    :ref:`minimal_cost_complexity_pruning` for details.\n    .. versionadded:: 0.22\nmax_samples : int or float, default=None\n    If bootstrap is True, the number of samples to draw from X\n    to train each base estimator.\n    - If None (default), then draw `X.shape[0]` samples.\n    - If int, then draw `max_samples` samples.\n    - If float, then draw `max(round(n_samples * max_samples), 1)` samples. Thus,\n      `max_samples` should be in the interval `(0.0, 1.0]`.\n    .. versionadded:: 0.22\nAttributes\n----------\nestimator_ : :class:`~sklearn.tree.DecisionTreeClassifier`\n    The child estimator template used to create the collection of fitted\n    sub-estimators.\n    .. versionadded:: 1.2\n       `base_estimator_` was renamed to `estimator_`.\nbase_estimator_ : DecisionTreeClassifier\n    The child estimator template used to create the collection of fitted\n    sub-estimators.\n    .. deprecated:: 1.2\n        `base_estimator_` is deprecated and will be removed in 1.4.\n        Use `estimator_` instead.\nestimators_ : list of DecisionTreeClassifier\n    The collection of fitted sub-estimators.\nclasses_ : ndarray of shape (n_classes,) or a list of such arrays\n    The classes labels (single output problem), or a list of arrays of\n    class labels (multi-output problem).\nn_classes_ : int or list\n    The number of classes (single output problem), or a list containing the\n    number of classes for each output (multi-output problem).\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n    .. versionadded:: 0.24\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n    .. versionadded:: 1.0\nn_outputs_ : int\n    The number of outputs when ``fit`` is performed.\nfeature_importances_ : ndarray of shape (n_features,)\n    The impurity-based feature importances.\n    The higher, the more important the feature.\n    The importance of a feature is computed as the (normalized)\n    total reduction of the criterion brought by that feature.  It is also\n    known as the Gini importance.\n    Warning: impurity-based feature importances can be misleading for\n    high cardinality features (many unique values). See\n    :func:`sklearn.inspection.permutation_importance` as an alternative.\noob_score_ : float\n    Score of the training dataset obtained using an out-of-bag estimate.\n    This attribute exists only when ``oob_score`` is True.\noob_decision_function_ : ndarray of shape (n_samples, n_classes) or             (n_samples, n_classes, n_outputs)\n    Decision function computed with out-of-bag estimate on the training\n    set. If n_estimators is small it might be possible that a data point\n    was never left out during the bootstrap. In this case,\n    `oob_decision_function_` might contain NaN. This attribute exists\n    only when ``oob_score`` is True.\nSee Also\n--------\nsklearn.tree.DecisionTreeClassifier : A decision tree classifier.\nsklearn.ensemble.ExtraTreesClassifier : Ensemble of extremely randomized\n    tree classifiers.\nsklearn.ensemble.HistGradientBoostingClassifier : A Histogram-based Gradient\n    Boosting Classification Tree, very fast for big datasets (n_samples &gt;=\n    10_000).\nNotes\n-----\nThe default values for the parameters controlling the size of the trees\n(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\nunpruned trees which can potentially be very large on some data sets. To\nreduce memory consumption, the complexity and size of the trees should be\ncontrolled by setting those parameter values.\nThe features are always randomly permuted at each split. Therefore,\nthe best found split may vary, even with the same training data,\n``max_features=n_features`` and ``bootstrap=False``, if the improvement\nof the criterion is identical for several splits enumerated during the\nsearch of the best split. To obtain a deterministic behaviour during\nfitting, ``random_state`` has to be fixed.\nReferences\n----------\n.. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\nExamples\n--------\n&gt;&gt;&gt; from sklearn.ensemble import RandomForestClassifier\n&gt;&gt;&gt; from sklearn.datasets import make_classification\n&gt;&gt;&gt; X, y = make_classification(n_samples=1000, n_features=4,\n...                            n_informative=2, n_redundant=0,\n...                            random_state=0, shuffle=False)\n&gt;&gt;&gt; clf = RandomForestClassifier(max_depth=2, random_state=0)\n&gt;&gt;&gt; clf.fit(X, y)\nRandomForestClassifier(...)\n&gt;&gt;&gt; print(clf.predict([[0, 0, 0, 0]]))\n[1]\nFile:           ~/mambaforge/envs/cfast/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\nType:           ABCMeta\nSubclasses:     \n\n\n\n\nmodel.score(X_test, y_test)\n\n0.9638888888888889\n\n\n\ny_predicted = model.predict(X_test)\n\nConfusion Matrix\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_predicted)\ncm\n\narray([[37,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n       [ 0, 33,  0,  0,  1,  0,  0,  1,  0,  1],\n       [ 1,  0, 32,  2,  0,  0,  0,  0,  1,  0],\n       [ 0,  0,  0, 35,  0,  0,  0,  0,  0,  0],\n       [ 0,  0,  0,  0, 29,  0,  0,  1,  0,  0],\n       [ 0,  0,  0,  0,  0, 40,  0,  0,  1,  0],\n       [ 0,  0,  0,  0,  0,  0, 33,  0,  0,  0],\n       [ 0,  0,  0,  0,  0,  0,  0, 37,  0,  0],\n       [ 0,  0,  1,  0,  0,  0,  0,  1, 35,  0],\n       [ 0,  0,  0,  1,  0,  0,  0,  0,  1, 36]])\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sn\nplt.figure(figsize=(10,7))\nsn.heatmap(cm, annot=True)\nplt.xlabel('Predicted')\nplt.ylabel('Truth')\n\nText(95.72222222222221, 0.5, 'Truth')\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "linearregression_multivariate.html",
    "href": "linearregression_multivariate.html",
    "title": "Linear Regression Multiple Variables",
    "section": "",
    "text": "Below is the table containing home prices in monroe twp, NJ. Here price depends on area (square feet), bed rooms and age of the home (in years). Given these prices we have to predict prices of new homes based on area, bed rooms and age.\n\nGiven these home prices find out price of a home that has,\n3000 sqr ft area, 3 bedrooms, 40 year old\n2500 sqr ft area, 4 bedrooms, 5 year old\nWe will use regression with multiple variables here. Price can be calculated using following equation,\n\nHere area, bedrooms, age are called independant variables or features whereas price is a dependant variable\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn import linear_model\n\n\ndf = pd.read_csv('./Data/homeprices.csv')\ndf\n\n\n\n\n\n\n\n\narea\nbedrooms\nage\nprice\n\n\n\n\n0\n2600\n3.0\n20\n550000\n\n\n1\n3000\n4.0\n15\n565000\n\n\n2\n3200\nNaN\n18\n610000\n\n\n3\n3600\n3.0\n30\n595000\n\n\n4\n4000\n5.0\n8\n760000\n\n\n5\n4100\n6.0\n8\n810000\n\n\n\n\n\n\n\nData Preprocessing: Fill NA values with median value of a column\n\ndf.bedrooms.median()\n\n4.0\n\n\n\ndf.bedrooms = df.bedrooms.fillna(df.bedrooms.median())\ndf\n\n\n\n\n\n\n\n\narea\nbedrooms\nage\nprice\n\n\n\n\n0\n2600\n3.0\n20\n550000\n\n\n1\n3000\n4.0\n15\n565000\n\n\n2\n3200\n4.0\n18\n610000\n\n\n3\n3600\n3.0\n30\n595000\n\n\n4\n4000\n5.0\n8\n760000\n\n\n5\n4100\n6.0\n8\n810000\n\n\n\n\n\n\n\n\nreg = linear_model.LinearRegression()\nreg.fit(df.drop('price',axis='columns'),df.price)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nreg.coef_\n\narray([  112.06244194, 23388.88007794, -3231.71790863])\n\n\n\nreg.intercept_\n\n221323.00186540408\n\n\nFind price of home with 3000 sqr ft area, 3 bedrooms, 40 year old\n\nreg.predict([[3000, 3, 40]])\n\n/home/ben/mambaforge/envs/cfast/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n  warnings.warn(\n\n\narray([498408.25158031])\n\n\n\n112.06244194*3000 + 23388.88007794*3 + -3231.71790863*40 + 221323.00186540384\n\n498408.25157402386\n\n\nFind price of home with 2500 sqr ft area, 4 bedrooms, 5 year old\n\nreg.predict([[2500, 4, 5]])\n\n/home/ben/mambaforge/envs/cfast/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n  warnings.warn(\n\n\narray([578876.03748933])\n\n\n\nnp.set_printoptions(precision=2, suppress=True)\ncorrcoef = np.corrcoef(df, rowvar=False)\ncorrcoef\n\narray([[ 1.  ,  0.75, -0.45,  0.9 ],\n       [ 0.75,  1.  , -0.88,  0.92],\n       [-0.45, -0.88,  1.  , -0.73],\n       [ 0.9 ,  0.92, -0.73,  1.  ]])\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sn\n\n\ndf.columns\n\nIndex(['area', 'bedrooms', 'age', 'price'], dtype='object')\n\n\n\nplt.figure(figsize=(7,5))\nsn.heatmap(corrcoef, annot=True)\n\nplt.xticks(range(len(df.columns)), df.columns)\nplt.yticks(range(len(df.columns)), df.columns)\n# Move x-axis ticks and labels to the top\nplt.gca().xaxis.set_ticks_position('top')\n\n\nplt.show()"
  },
  {
    "objectID": "linearregression_multivariate.html#sample-problem-of-predicting-home-price-in-monroe-new-jersey-usa",
    "href": "linearregression_multivariate.html#sample-problem-of-predicting-home-price-in-monroe-new-jersey-usa",
    "title": "Linear Regression Multiple Variables",
    "section": "",
    "text": "Below is the table containing home prices in monroe twp, NJ. Here price depends on area (square feet), bed rooms and age of the home (in years). Given these prices we have to predict prices of new homes based on area, bed rooms and age.\n\nGiven these home prices find out price of a home that has,\n3000 sqr ft area, 3 bedrooms, 40 year old\n2500 sqr ft area, 4 bedrooms, 5 year old\nWe will use regression with multiple variables here. Price can be calculated using following equation,\n\nHere area, bedrooms, age are called independant variables or features whereas price is a dependant variable\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn import linear_model\n\n\ndf = pd.read_csv('./Data/homeprices.csv')\ndf\n\n\n\n\n\n\n\n\narea\nbedrooms\nage\nprice\n\n\n\n\n0\n2600\n3.0\n20\n550000\n\n\n1\n3000\n4.0\n15\n565000\n\n\n2\n3200\nNaN\n18\n610000\n\n\n3\n3600\n3.0\n30\n595000\n\n\n4\n4000\n5.0\n8\n760000\n\n\n5\n4100\n6.0\n8\n810000\n\n\n\n\n\n\n\nData Preprocessing: Fill NA values with median value of a column\n\ndf.bedrooms.median()\n\n4.0\n\n\n\ndf.bedrooms = df.bedrooms.fillna(df.bedrooms.median())\ndf\n\n\n\n\n\n\n\n\narea\nbedrooms\nage\nprice\n\n\n\n\n0\n2600\n3.0\n20\n550000\n\n\n1\n3000\n4.0\n15\n565000\n\n\n2\n3200\n4.0\n18\n610000\n\n\n3\n3600\n3.0\n30\n595000\n\n\n4\n4000\n5.0\n8\n760000\n\n\n5\n4100\n6.0\n8\n810000\n\n\n\n\n\n\n\n\nreg = linear_model.LinearRegression()\nreg.fit(df.drop('price',axis='columns'),df.price)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nreg.coef_\n\narray([  112.06244194, 23388.88007794, -3231.71790863])\n\n\n\nreg.intercept_\n\n221323.00186540408\n\n\nFind price of home with 3000 sqr ft area, 3 bedrooms, 40 year old\n\nreg.predict([[3000, 3, 40]])\n\n/home/ben/mambaforge/envs/cfast/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n  warnings.warn(\n\n\narray([498408.25158031])\n\n\n\n112.06244194*3000 + 23388.88007794*3 + -3231.71790863*40 + 221323.00186540384\n\n498408.25157402386\n\n\nFind price of home with 2500 sqr ft area, 4 bedrooms, 5 year old\n\nreg.predict([[2500, 4, 5]])\n\n/home/ben/mambaforge/envs/cfast/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n  warnings.warn(\n\n\narray([578876.03748933])\n\n\n\nnp.set_printoptions(precision=2, suppress=True)\ncorrcoef = np.corrcoef(df, rowvar=False)\ncorrcoef\n\narray([[ 1.  ,  0.75, -0.45,  0.9 ],\n       [ 0.75,  1.  , -0.88,  0.92],\n       [-0.45, -0.88,  1.  , -0.73],\n       [ 0.9 ,  0.92, -0.73,  1.  ]])\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sn\n\n\ndf.columns\n\nIndex(['area', 'bedrooms', 'age', 'price'], dtype='object')\n\n\n\nplt.figure(figsize=(7,5))\nsn.heatmap(corrcoef, annot=True)\n\nplt.xticks(range(len(df.columns)), df.columns)\nplt.yticks(range(len(df.columns)), df.columns)\n# Move x-axis ticks and labels to the top\nplt.gca().xaxis.set_ticks_position('top')\n\n\nplt.show()"
  },
  {
    "objectID": "kernals.html",
    "href": "kernals.html",
    "title": "Kernels",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "07_L1 and L2 Regularization-Copy1.html",
    "href": "07_L1 and L2 Regularization-Copy1.html",
    "title": "L1 and L2 Regularization",
    "section": "",
    "text": "L1 and L2 Regularization\n# import libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n# Suppress Warnings for clean notebook\nimport warnings\nwarnings.filterwarnings('ignore')\nWe are going to use Melbourne House Price Dataset where we’ll predict House Predictions based on various features. #### The Dataset Link is https://www.kaggle.com/anthonypino/melbourne-housing-market\n# read dataset\ndataset_og = pd.read_csv('./Data/Melbourne_housing_FULL.csv')\ndataset_og.head()\n\n\n\n\n\n\n\n\nSuburb\nAddress\nRooms\nType\nPrice\nMethod\nSellerG\nDate\nDistance\nPostcode\n...\nBathroom\nCar\nLandsize\nBuildingArea\nYearBuilt\nCouncilArea\nLattitude\nLongtitude\nRegionname\nPropertycount\n\n\n\n\n0\nAbbotsford\n68 Studley St\n2\nh\nNaN\nSS\nJellis\n3/09/2016\n2.5\n3067.0\n...\n1.0\n1.0\n126.0\nNaN\nNaN\nYarra City Council\n-37.8014\n144.9958\nNorthern Metropolitan\n4019.0\n\n\n1\nAbbotsford\n85 Turner St\n2\nh\n1480000.0\nS\nBiggin\n3/12/2016\n2.5\n3067.0\n...\n1.0\n1.0\n202.0\nNaN\nNaN\nYarra City Council\n-37.7996\n144.9984\nNorthern Metropolitan\n4019.0\n\n\n2\nAbbotsford\n25 Bloomburg St\n2\nh\n1035000.0\nS\nBiggin\n4/02/2016\n2.5\n3067.0\n...\n1.0\n0.0\n156.0\n79.0\n1900.0\nYarra City Council\n-37.8079\n144.9934\nNorthern Metropolitan\n4019.0\n\n\n3\nAbbotsford\n18/659 Victoria St\n3\nu\nNaN\nVB\nRounds\n4/02/2016\n2.5\n3067.0\n...\n2.0\n1.0\n0.0\nNaN\nNaN\nYarra City Council\n-37.8114\n145.0116\nNorthern Metropolitan\n4019.0\n\n\n4\nAbbotsford\n5 Charles St\n3\nh\n1465000.0\nSP\nBiggin\n4/03/2017\n2.5\n3067.0\n...\n2.0\n0.0\n134.0\n150.0\n1900.0\nYarra City Council\n-37.8093\n144.9944\nNorthern Metropolitan\n4019.0\n\n\n\n\n5 rows × 21 columns\ndataset_og.nunique()\n\nSuburb             351\nAddress          34009\nRooms               12\nType                 3\nPrice             2871\nMethod               9\nSellerG            388\nDate                78\nDistance           215\nPostcode           211\nBedroom2            15\nBathroom            11\nCar                 15\nLandsize          1684\nBuildingArea       740\nYearBuilt          160\nCouncilArea         33\nLattitude        13402\nLongtitude       14524\nRegionname           8\nPropertycount      342\ndtype: int64\n# let's use limited columns which makes more sense for serving our purpose\ncols_to_use = ['Suburb', 'Rooms', 'Type', 'Method', 'SellerG', 'Regionname', 'Propertycount', \n               'Distance', 'CouncilArea', 'Bedroom2', 'Bathroom', 'Car', 'Landsize', 'BuildingArea', 'Price']\ndataset = dataset_og[cols_to_use]\ndataset.head()\n\n\n\n\n\n\n\n\nSuburb\nRooms\nType\nMethod\nSellerG\nRegionname\nPropertycount\nDistance\nCouncilArea\nBedroom2\nBathroom\nCar\nLandsize\nBuildingArea\nPrice\n\n\n\n\n0\nAbbotsford\n2\nh\nSS\nJellis\nNorthern Metropolitan\n4019.0\n2.5\nYarra City Council\n2.0\n1.0\n1.0\n126.0\nNaN\nNaN\n\n\n1\nAbbotsford\n2\nh\nS\nBiggin\nNorthern Metropolitan\n4019.0\n2.5\nYarra City Council\n2.0\n1.0\n1.0\n202.0\nNaN\n1480000.0\n\n\n2\nAbbotsford\n2\nh\nS\nBiggin\nNorthern Metropolitan\n4019.0\n2.5\nYarra City Council\n2.0\n1.0\n0.0\n156.0\n79.0\n1035000.0\n\n\n3\nAbbotsford\n3\nu\nVB\nRounds\nNorthern Metropolitan\n4019.0\n2.5\nYarra City Council\n3.0\n2.0\n1.0\n0.0\nNaN\nNaN\n\n\n4\nAbbotsford\n3\nh\nSP\nBiggin\nNorthern Metropolitan\n4019.0\n2.5\nYarra City Council\n3.0\n2.0\n0.0\n134.0\n150.0\n1465000.0\ndataset.shape\n\n(34857, 15)"
  },
  {
    "objectID": "07_L1 and L2 Regularization-Copy1.html#normal-regression-is-clearly-overfitting-the-data-lets-try-other-models",
    "href": "07_L1 and L2 Regularization-Copy1.html#normal-regression-is-clearly-overfitting-the-data-lets-try-other-models",
    "title": "L1 and L2 Regularization",
    "section": "Normal Regression is clearly overfitting the data, let’s try other models",
    "text": "Normal Regression is clearly overfitting the data, let’s try other models\n\nUsing Lasso (L1 Regularized) Regression Model\n\nfrom sklearn.linear_model import Lasso\nlasso_reg = Lasso(alpha=50, max_iter=100, tol=0.1)\nlasso_reg.fit(train_X, train_y)\n\nLasso(alpha=50, max_iter=100, tol=0.1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LassoLasso(alpha=50, max_iter=100, tol=0.1)\n\n\n\nlasso_reg.score(test_X, test_y)\n\n0.6636111369404488\n\n\n\nlasso_reg.score(train_X, train_y)\n\n0.6766985624766824\n\n\n\nUsing Ridge (L2 Regularized) Regression Model\n\nfrom sklearn.linear_model import Ridge\nridge_reg= Ridge(alpha=50, max_iter=100, tol=0.1)\nridge_reg.fit(train_X, train_y)\n\nRidge(alpha=50, max_iter=100, tol=0.1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RidgeRidge(alpha=50, max_iter=100, tol=0.1)\n\n\n\nridge_reg.score(test_X, test_y)\n\n0.6670848945194958\n\n\n\nridge_reg.score(train_X, train_y)\n\n0.6622376739684328\n\n\nWe see that Lasso and Ridge Regularizations prove to be beneficial when our Simple Linear Regression Model overfits. These results may not be that contrast but significant in most cases.Also that L1 & L2 Regularizations are used in Neural Networks too"
  },
  {
    "objectID": "171_PCA_heart_disease_prediction_exercise_solution.html",
    "href": "171_PCA_heart_disease_prediction_exercise_solution.html",
    "title": "ML",
    "section": "",
    "text": "PCA Machine Learning Tutorial Exercise Solution\n\n\n\nimport pandas as pd\n\n# https://www.kaggle.com/fedesoriano/heart-failure-prediction\ndf = pd.read_csv(\"Data/heart.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nAge\nSex\nChestPainType\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nHeartDisease\n\n\n\n\n0\n40\nM\nATA\n140\n289\n0\nNormal\n172\nN\n0.0\nUp\n0\n\n\n1\n49\nF\nNAP\n160\n180\n0\nNormal\n156\nN\n1.0\nFlat\n1\n\n\n2\n37\nM\nATA\n130\n283\n0\nST\n98\nN\n0.0\nUp\n0\n\n\n3\n48\nF\nASY\n138\n214\n0\nNormal\n108\nY\n1.5\nFlat\n1\n\n\n4\n54\nM\nNAP\n150\n195\n0\nNormal\n122\nN\n0.0\nUp\n0\n\n\n\n\n\n\n\n\ndf.shape\n\n(918, 12)\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nAge\nRestingBP\nCholesterol\nFastingBS\nMaxHR\nOldpeak\nHeartDisease\n\n\n\n\ncount\n918.000000\n918.000000\n918.000000\n918.000000\n918.000000\n918.000000\n918.000000\n\n\nmean\n53.510893\n132.396514\n198.799564\n0.233115\n136.809368\n0.887364\n0.553377\n\n\nstd\n9.432617\n18.514154\n109.384145\n0.423046\n25.460334\n1.066570\n0.497414\n\n\nmin\n28.000000\n0.000000\n0.000000\n0.000000\n60.000000\n-2.600000\n0.000000\n\n\n25%\n47.000000\n120.000000\n173.250000\n0.000000\n120.000000\n0.000000\n0.000000\n\n\n50%\n54.000000\n130.000000\n223.000000\n0.000000\n138.000000\n0.600000\n1.000000\n\n\n75%\n60.000000\n140.000000\n267.000000\n0.000000\n156.000000\n1.500000\n1.000000\n\n\nmax\n77.000000\n200.000000\n603.000000\n1.000000\n202.000000\n6.200000\n1.000000\n\n\n\n\n\n\n\n\nTreat Outliers\n\n\ndf[df.Cholesterol&gt;(df.Cholesterol.mean()+3*df.Cholesterol.std())]\n\n\n\n\n\n\n\n\nAge\nSex\nChestPainType\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nHeartDisease\n\n\n\n\n76\n32\nM\nASY\n118\n529\n0\nNormal\n130\nN\n0.0\nFlat\n1\n\n\n149\n54\nM\nASY\n130\n603\n1\nNormal\n125\nY\n1.0\nFlat\n1\n\n\n616\n67\nF\nNAP\n115\n564\n0\nLVH\n160\nN\n1.6\nFlat\n0\n\n\n\n\n\n\n\n\ndf.shape\n\n(918, 12)\n\n\n\ndf1 = df[df.Cholesterol&lt;=(df.Cholesterol.mean()+3*df.Cholesterol.std())]\ndf1.shape\n\n(915, 12)\n\n\n\ndf[df.MaxHR&gt;(df.MaxHR.mean()+3*df.MaxHR.std())]\n\n\n\n\n\n\n\n\nAge\nSex\nChestPainType\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nHeartDisease\n\n\n\n\n\n\n\n\n\n\ndf[df.FastingBS&gt;(df.FastingBS.mean()+3*df.FastingBS.std())]\n\n\n\n\n\n\n\n\nAge\nSex\nChestPainType\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nHeartDisease\n\n\n\n\n\n\n\n\n\n\ndf[df.Oldpeak&gt;(df.Oldpeak.mean()+3*df.Oldpeak.std())]\n\n\n\n\n\n\n\n\nAge\nSex\nChestPainType\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nHeartDisease\n\n\n\n\n166\n50\nM\nASY\n140\n231\n0\nST\n140\nY\n5.0\nFlat\n1\n\n\n702\n59\nM\nTA\n178\n270\n0\nLVH\n145\nN\n4.2\nDown\n0\n\n\n771\n55\nM\nASY\n140\n217\n0\nNormal\n111\nY\n5.6\nDown\n1\n\n\n791\n51\nM\nASY\n140\n298\n0\nNormal\n122\nY\n4.2\nFlat\n1\n\n\n850\n62\nF\nASY\n160\n164\n0\nLVH\n145\nN\n6.2\nDown\n1\n\n\n900\n58\nM\nASY\n114\n318\n0\nST\n140\nN\n4.4\nDown\n1\n\n\n\n\n\n\n\n\ndf2 = df1[df1.Oldpeak&lt;=(df1.Oldpeak.mean()+3*df1.Oldpeak.std())]\ndf2.shape\n\n(909, 12)\n\n\n\ndf[df.RestingBP&gt;(df.RestingBP.mean()+3*df.RestingBP.std())]\n\n\n\n\n\n\n\n\nAge\nSex\nChestPainType\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nHeartDisease\n\n\n\n\n109\n39\nM\nATA\n190\n241\n0\nNormal\n106\nN\n0.0\nUp\n0\n\n\n241\n54\nM\nASY\n200\n198\n0\nNormal\n142\nY\n2.0\nFlat\n1\n\n\n365\n64\nF\nASY\n200\n0\n0\nNormal\n140\nY\n1.0\nFlat\n1\n\n\n399\n61\nM\nNAP\n200\n0\n1\nST\n70\nN\n0.0\nFlat\n1\n\n\n592\n61\nM\nASY\n190\n287\n1\nLVH\n150\nY\n2.0\nDown\n1\n\n\n732\n56\nF\nASY\n200\n288\n1\nLVH\n133\nY\n4.0\nDown\n1\n\n\n759\n54\nM\nATA\n192\n283\n0\nLVH\n195\nN\n0.0\nUp\n1\n\n\n\n\n\n\n\n\ndf3 = df2[df2.RestingBP&lt;=(df2.RestingBP.mean()+3*df2.RestingBP.std())]\ndf3.shape\n\n(902, 12)\n\n\n\ndf.ChestPainType.unique()\n\narray(['ATA', 'NAP', 'ASY', 'TA'], dtype=object)\n\n\n\ndf.RestingECG.unique()\n\narray(['Normal', 'ST', 'LVH'], dtype=object)\n\n\n\ndf.ExerciseAngina.unique()\n\narray(['N', 'Y'], dtype=object)\n\n\n\ndf.ST_Slope.unique()\n\narray(['Up', 'Flat', 'Down'], dtype=object)\n\n\n\ndf4 = df3.copy()\ndf4.ExerciseAngina.replace(\n    {\n        'N': 0,\n        'Y': 1\n    },\n    inplace=True)\n\ndf4.ST_Slope.replace(\n    {\n        'Down': 1,\n        'Flat': 2,\n        'Up': 3\n    },\n    inplace=True\n)\n\ndf4.RestingECG.replace(\n    {\n        'Normal': 1,\n        'ST': 2,\n        'LVH': 3\n    },\n    inplace=True)\n\ndf4.head()\n\n\n\n\n\n\n\n\nAge\nSex\nChestPainType\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nHeartDisease\n\n\n\n\n0\n40\nM\nATA\n140\n289\n0\n1\n172\n0\n0.0\n3\n0\n\n\n1\n49\nF\nNAP\n160\n180\n0\n1\n156\n0\n1.0\n2\n1\n\n\n2\n37\nM\nATA\n130\n283\n0\n2\n98\n0\n0.0\n3\n0\n\n\n3\n48\nF\nASY\n138\n214\n0\n1\n108\n1\n1.5\n2\n1\n\n\n4\n54\nM\nNAP\n150\n195\n0\n1\n122\n0\n0.0\n3\n0\n\n\n\n\n\n\n\n\ndf5 = pd.get_dummies(df4, drop_first=True)\ndf5.head()\n\n\n\n\n\n\n\n\nAge\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nHeartDisease\nSex_M\nChestPainType_ATA\nChestPainType_NAP\nChestPainType_TA\n\n\n\n\n0\n40\n140\n289\n0\n1\n172\n0\n0.0\n3\n0\n1\n1\n0\n0\n\n\n1\n49\n160\n180\n0\n1\n156\n0\n1.0\n2\n1\n0\n0\n1\n0\n\n\n2\n37\n130\n283\n0\n2\n98\n0\n0.0\n3\n0\n1\n1\n0\n0\n\n\n3\n48\n138\n214\n0\n1\n108\n1\n1.5\n2\n1\n0\n0\n0\n0\n\n\n4\n54\n150\n195\n0\n1\n122\n0\n0.0\n3\n0\n1\n0\n1\n0\n\n\n\n\n\n\n\n\nX = df5.drop(\"HeartDisease\",axis='columns')\ny = df5.HeartDisease\n\nX.head()\n\n\n\n\n\n\n\n\nAge\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nSex_M\nChestPainType_ATA\nChestPainType_NAP\nChestPainType_TA\n\n\n\n\n0\n40\n140\n289\n0\n1\n172\n0\n0.0\n3\n1\n1\n0\n0\n\n\n1\n49\n160\n180\n0\n1\n156\n0\n1.0\n2\n0\n0\n1\n0\n\n\n2\n37\n130\n283\n0\n2\n98\n0\n0.0\n3\n1\n1\n0\n0\n\n\n3\n48\n138\n214\n0\n1\n108\n1\n1.5\n2\n0\n0\n0\n0\n\n\n4\n54\n150\n195\n0\n1\n122\n0\n0.0\n3\n1\n0\n1\n0\n\n\n\n\n\n\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nX_scaled\n\narray([[-1.42896269,  0.46089071,  0.85238015, ...,  2.06757196,\n        -0.53547478, -0.22914788],\n       [-0.47545956,  1.5925728 , -0.16132855, ..., -0.4836591 ,\n         1.86750159, -0.22914788],\n       [-1.74679706, -0.10495034,  0.79657967, ...,  2.06757196,\n        -0.53547478, -0.22914788],\n       ...,\n       [ 0.37209878, -0.10495034, -0.61703246, ..., -0.4836591 ,\n        -0.53547478, -0.22914788],\n       [ 0.37209878, -0.10495034,  0.35947592, ...,  2.06757196,\n        -0.53547478, -0.22914788],\n       [-1.64085227,  0.3477225 , -0.20782894, ..., -0.4836591 ,\n         1.86750159, -0.22914788]])\n\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=30)\n\n\nX_train.shape\n\n(721, 13)\n\n\n\nX_test.shape\n\n(181, 13)\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel_rf = RandomForestClassifier()\nmodel_rf.fit(X_train, y_train)\nmodel_rf.score(X_test, y_test)\n\n0.8674033149171271\n\n\n\nUse PCA to reduce dimensions\n\n\nX\n\n\n\n\n\n\n\n\nAge\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nSex_M\nChestPainType_ATA\nChestPainType_NAP\nChestPainType_TA\n\n\n\n\n0\n40\n140\n289\n0\n1\n172\n0\n0.0\n3\n1\n1\n0\n0\n\n\n1\n49\n160\n180\n0\n1\n156\n0\n1.0\n2\n0\n0\n1\n0\n\n\n2\n37\n130\n283\n0\n2\n98\n0\n0.0\n3\n1\n1\n0\n0\n\n\n3\n48\n138\n214\n0\n1\n108\n1\n1.5\n2\n0\n0\n0\n0\n\n\n4\n54\n150\n195\n0\n1\n122\n0\n0.0\n3\n1\n0\n1\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n913\n45\n110\n264\n0\n1\n132\n0\n1.2\n2\n1\n0\n0\n1\n\n\n914\n68\n144\n193\n1\n1\n141\n0\n3.4\n2\n1\n0\n0\n0\n\n\n915\n57\n130\n131\n0\n1\n115\n1\n1.2\n2\n1\n0\n0\n0\n\n\n916\n57\n130\n236\n0\n3\n174\n0\n0.0\n2\n0\n1\n0\n0\n\n\n917\n38\n138\n175\n0\n1\n173\n0\n0.0\n3\n1\n0\n1\n0\n\n\n\n\n902 rows × 13 columns\n\n\n\n\nfrom sklearn.decomposition import PCA\n\npca = PCA(0.95)\nX_pca = pca.fit_transform(X)\nX_pca\n\narray([[ 93.82465373, -29.40099458],\n       [-15.58422331, -14.10909233],\n       [ 83.29606634,  38.6867453 ],\n       ...,\n       [-67.57318721,  17.61319354],\n       [ 40.70458237, -33.38750602],\n       [-19.91368346, -37.29085722]])\n\n\n\nX_train_pca, X_test_pca, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=30)\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel_rf = RandomForestClassifier()\nmodel_rf.fit(X_train_pca, y_train)\nmodel_rf.score(X_test_pca, y_test)\n\n0.7182320441988951\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "train_test_split.html",
    "href": "train_test_split.html",
    "title": "Training And Testing Available Data",
    "section": "",
    "text": "We have a dataset containing prices of used BMW cars. We are going to analyze this dataset and build a prediction function that can predict a price by taking mileage and age of the car as input. We will use sklearn train_test_split method to split training and testing dataset\n\n\nimport pandas as pd\ndf = pd.read_csv(\"Data/carprices.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nMileage\nAge(yrs)\nSell Price($)\n\n\n\n\n0\n69000\n6\n18000\n\n\n1\n35000\n3\n34000\n\n\n2\n57000\n5\n26100\n\n\n3\n22500\n2\n40000\n\n\n4\n46000\n4\n31500\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\nCar Mileage Vs Sell Price ($)\n\nplt.scatter(df['Mileage'],df['Sell Price($)'])\n\n&lt;matplotlib.collections.PathCollection&gt;\n\n\n\n\n\nCar Age Vs Sell Price ($)\n\nplt.scatter(df['Age(yrs)'],df['Sell Price($)'])\n\n&lt;matplotlib.collections.PathCollection&gt;\n\n\n\n\n\nLooking at above two scatter plots, using linear regression model makes sense as we can clearly see a linear relationship between our dependant (i.e. Sell Price) and independant variables (i.e. car age and car mileage)\n\nThe approach we are going to use here is to split available data in two sets\n\n&lt;ol&gt;\n    &lt;b&gt;\n    &lt;li&gt;Training: We will train our model on this dataset&lt;/li&gt;\n    &lt;li&gt;Testing: We will use this subset to make actual predictions using trained model&lt;/li&gt;\n    &lt;/b&gt;\n &lt;/ol&gt;\n\nThe reason we don’t use same training set for testing is because our model has seen those samples before, using same samples for making predictions might give us wrong impression about accuracy of our model. It is like you ask same questions in exam paper as you tought the students in the class. \n\n\nX = df[['Mileage','Age(yrs)']]\n\n\ny = df['Sell Price($)']\n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)\n\n\nX_train\n\n\n\n\n\n\n\n\nMileage\nAge(yrs)\n\n\n\n\n2\n57000\n5\n\n\n5\n59000\n5\n\n\n11\n79000\n7\n\n\n1\n35000\n3\n\n\n15\n25400\n3\n\n\n8\n91000\n8\n\n\n14\n82450\n7\n\n\n9\n67000\n6\n\n\n4\n46000\n4\n\n\n7\n72000\n6\n\n\n3\n22500\n2\n\n\n17\n69000\n5\n\n\n10\n83000\n7\n\n\n18\n87600\n8\n\n\n\n\n\n\n\n\nX_test\n\n\n\n\n\n\n\n\nMileage\nAge(yrs)\n\n\n\n\n6\n52000\n5\n\n\n0\n69000\n6\n\n\n19\n52000\n5\n\n\n13\n58780\n4\n\n\n16\n28000\n2\n\n\n12\n59000\n5\n\n\n\n\n\n\n\n\ny_train\n\n11    19500\n17    19700\n10    18700\n1     34000\n0     18000\n8     12000\n7     19300\n16    35500\n6     32000\n4     31500\n19    28200\n2     26100\n5     26750\n15    35000\nName: Sell Price($), dtype: int64\n\n\n\ny_test\n\n3     40000\n12    26000\n14    19400\n13    27500\n9     22000\n18    12800\nName: Sell Price($), dtype: int64\n\n\nLets run linear regression model now\n\nfrom sklearn.linear_model import LinearRegression\nclf = LinearRegression()\nclf.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nX_test\n\n\n\n\n\n\n\n\nMileage\nAge(yrs)\n\n\n\n\n6\n52000\n5\n\n\n0\n69000\n6\n\n\n19\n52000\n5\n\n\n13\n58780\n4\n\n\n16\n28000\n2\n\n\n12\n59000\n5\n\n\n\n\n\n\n\n\nclf.predict(X_test)\n\narray([20795.62497033, 16717.8407409 , 25463.14706945, 27527.22961336,\n       38728.54102971, 14527.21769447])\n\n\n\ny_results = pd.DataFrame(clf.predict(X_test))\ny_results\n\n\n\n\n\n\n\n\n0\n\n\n\n\n0\n20795.624970\n\n\n1\n16717.840741\n\n\n2\n25463.147069\n\n\n3\n27527.229613\n\n\n4\n38728.541030\n\n\n5\n14527.217694\n\n\n\n\n\n\n\n\ny_res = y_results[0].sort_values(ascending=True).reset_index()\ny_res.drop('index', axis = 1, inplace=True)\ny_res\n\n\n\n\n\n\n\n\n0\n\n\n\n\n0\n14527.217694\n\n\n1\n16717.840741\n\n\n2\n20795.624970\n\n\n3\n25463.147069\n\n\n4\n27527.229613\n\n\n5\n38728.541030\n\n\n\n\n\n\n\n\ny_test\n\n7     19300\n10    18700\n5     26750\n6     32000\n3     40000\n18    12800\nName: Sell Price($), dtype: int64\n\n\n\ny_t = y_test.sort_values(ascending=True).reset_index()\ny_t\n\n\n\n\n\n\n\n\nindex\nSell Price($)\n\n\n\n\n0\n18\n12800\n\n\n1\n10\n18700\n\n\n2\n7\n19300\n\n\n3\n5\n26750\n\n\n4\n6\n32000\n\n\n5\n3\n40000\n\n\n\n\n\n\n\n\ny_t.drop('index', axis = 1, inplace=True)\ny_t\n\n\n\n\n\n\n\n\nSell Price($)\n\n\n\n\n0\n12800\n\n\n1\n18700\n\n\n2\n19300\n\n\n3\n26750\n\n\n4\n32000\n\n\n5\n40000\n\n\n\n\n\n\n\n\nplt.plot(y_t)\nplt.plot(y_res)\n\n\n\n\n\nclf.score(X_test, y_test)\n\n0.9348911052302149\n\n\nrandom_state argument\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=10)\nX_test\n\n\n\n\n\n\n\n\nMileage\nAge(yrs)\n\n\n\n\n7\n72000\n6\n\n\n10\n83000\n7\n\n\n5\n59000\n5\n\n\n6\n52000\n5\n\n\n3\n22500\n2\n\n\n18\n87600\n8\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "pca.html",
    "href": "pca.html",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "from sklearn.datasets import load_digits\nimport pandas as pd\n\ndataset = load_digits()\ndataset.keys()\n\ndict_keys(['data', 'target', 'frame', 'feature_names', 'target_names', 'images', 'DESCR'])\n\n\n\ndataset.data.shape\n\n(1797, 64)\n\n\n\ndataset.data[0]\n\narray([ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.,  0.,  0., 13., 15., 10.,\n       15.,  5.,  0.,  0.,  3., 15.,  2.,  0., 11.,  8.,  0.,  0.,  4.,\n       12.,  0.,  0.,  8.,  8.,  0.,  0.,  5.,  8.,  0.,  0.,  9.,  8.,\n        0.,  0.,  4., 11.,  0.,  1., 12.,  7.,  0.,  0.,  2., 14.,  5.,\n       10., 12.,  0.,  0.,  0.,  0.,  6., 13., 10.,  0.,  0.,  0.])\n\n\n\ndataset.data[0].reshape(8,8)\n\narray([[ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.],\n       [ 0.,  0., 13., 15., 10., 15.,  5.,  0.],\n       [ 0.,  3., 15.,  2.,  0., 11.,  8.,  0.],\n       [ 0.,  4., 12.,  0.,  0.,  8.,  8.,  0.],\n       [ 0.,  5.,  8.,  0.,  0.,  9.,  8.,  0.],\n       [ 0.,  4., 11.,  0.,  1., 12.,  7.,  0.],\n       [ 0.,  2., 14.,  5., 10., 12.,  0.,  0.],\n       [ 0.,  0.,  6., 13., 10.,  0.,  0.,  0.]])\n\n\n\nfrom matplotlib import pyplot as plt\n\nplt.gray()\nplt.matshow(dataset.data[0].reshape(8,8))\n\n&lt;matplotlib.image.AxesImage&gt;\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\nplt.matshow(dataset.data[9].reshape(8,8))\n\n&lt;matplotlib.image.AxesImage&gt;\n\n\n\n\n\n\ndataset.target[:5]\n\narray([0, 1, 2, 3, 4])\n\n\n\ndf = pd.DataFrame(dataset.data, columns=dataset.feature_names)\ndf.head()\n\n\n\n\n\n\n\n\npixel_0_0\npixel_0_1\npixel_0_2\npixel_0_3\npixel_0_4\npixel_0_5\npixel_0_6\npixel_0_7\npixel_1_0\npixel_1_1\n...\npixel_6_6\npixel_6_7\npixel_7_0\npixel_7_1\npixel_7_2\npixel_7_3\npixel_7_4\npixel_7_5\npixel_7_6\npixel_7_7\n\n\n\n\n0\n0.0\n0.0\n5.0\n13.0\n9.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n6.0\n13.0\n10.0\n0.0\n0.0\n0.0\n\n\n1\n0.0\n0.0\n0.0\n12.0\n13.0\n5.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n11.0\n16.0\n10.0\n0.0\n0.0\n\n\n2\n0.0\n0.0\n0.0\n4.0\n15.0\n12.0\n0.0\n0.0\n0.0\n0.0\n...\n5.0\n0.0\n0.0\n0.0\n0.0\n3.0\n11.0\n16.0\n9.0\n0.0\n\n\n3\n0.0\n0.0\n7.0\n15.0\n13.0\n1.0\n0.0\n0.0\n0.0\n8.0\n...\n9.0\n0.0\n0.0\n0.0\n7.0\n13.0\n13.0\n9.0\n0.0\n0.0\n\n\n4\n0.0\n0.0\n0.0\n1.0\n11.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n2.0\n16.0\n4.0\n0.0\n0.0\n\n\n\n\n5 rows × 64 columns\n\n\n\n\ndataset.target\n\narray([0, 1, 2, ..., 8, 9, 8])\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\npixel_0_0\npixel_0_1\npixel_0_2\npixel_0_3\npixel_0_4\npixel_0_5\npixel_0_6\npixel_0_7\npixel_1_0\npixel_1_1\n...\npixel_6_6\npixel_6_7\npixel_7_0\npixel_7_1\npixel_7_2\npixel_7_3\npixel_7_4\npixel_7_5\npixel_7_6\npixel_7_7\n\n\n\n\ncount\n1797.0\n1797.000000\n1797.000000\n1797.000000\n1797.000000\n1797.000000\n1797.000000\n1797.000000\n1797.000000\n1797.000000\n...\n1797.000000\n1797.000000\n1797.000000\n1797.000000\n1797.000000\n1797.000000\n1797.000000\n1797.000000\n1797.000000\n1797.000000\n\n\nmean\n0.0\n0.303840\n5.204786\n11.835838\n11.848080\n5.781859\n1.362270\n0.129661\n0.005565\n1.993879\n...\n3.725097\n0.206455\n0.000556\n0.279354\n5.557596\n12.089037\n11.809126\n6.764051\n2.067891\n0.364496\n\n\nstd\n0.0\n0.907192\n4.754826\n4.248842\n4.287388\n5.666418\n3.325775\n1.037383\n0.094222\n3.196160\n...\n4.919406\n0.984401\n0.023590\n0.934302\n5.103019\n4.374694\n4.933947\n5.900623\n4.090548\n1.860122\n\n\nmin\n0.0\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.0\n0.000000\n1.000000\n10.000000\n10.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n11.000000\n10.000000\n0.000000\n0.000000\n0.000000\n\n\n50%\n0.0\n0.000000\n4.000000\n13.000000\n13.000000\n4.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n0.000000\n0.000000\n0.000000\n4.000000\n13.000000\n14.000000\n6.000000\n0.000000\n0.000000\n\n\n75%\n0.0\n0.000000\n9.000000\n15.000000\n15.000000\n11.000000\n0.000000\n0.000000\n0.000000\n3.000000\n...\n7.000000\n0.000000\n0.000000\n0.000000\n10.000000\n16.000000\n16.000000\n12.000000\n2.000000\n0.000000\n\n\nmax\n0.0\n8.000000\n16.000000\n16.000000\n16.000000\n16.000000\n16.000000\n15.000000\n2.000000\n16.000000\n...\n16.000000\n13.000000\n1.000000\n9.000000\n16.000000\n16.000000\n16.000000\n16.000000\n16.000000\n16.000000\n\n\n\n\n8 rows × 64 columns\n\n\n\n\nX = df\ny = dataset.target\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nX_scaled\n\narray([[ 0.        , -0.33501649, -0.04308102, ..., -1.14664746,\n        -0.5056698 , -0.19600752],\n       [ 0.        , -0.33501649, -1.09493684, ...,  0.54856067,\n        -0.5056698 , -0.19600752],\n       [ 0.        , -0.33501649, -1.09493684, ...,  1.56568555,\n         1.6951369 , -0.19600752],\n       ...,\n       [ 0.        , -0.33501649, -0.88456568, ..., -0.12952258,\n        -0.5056698 , -0.19600752],\n       [ 0.        , -0.33501649, -0.67419451, ...,  0.8876023 ,\n        -0.5056698 , -0.19600752],\n       [ 0.        , -0.33501649,  1.00877481, ...,  0.8876023 ,\n        -0.26113572, -0.19600752]])\n\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=30)\n\n\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\nmodel.score(X_test, y_test)\n\n0.9722222222222222\n\n\n\nUse PCA to reduce dimensions\n\n\nX\n\n\n\n\n\n\n\n\npixel_0_0\npixel_0_1\npixel_0_2\npixel_0_3\npixel_0_4\npixel_0_5\npixel_0_6\npixel_0_7\npixel_1_0\npixel_1_1\n...\npixel_6_6\npixel_6_7\npixel_7_0\npixel_7_1\npixel_7_2\npixel_7_3\npixel_7_4\npixel_7_5\npixel_7_6\npixel_7_7\n\n\n\n\n0\n0.0\n0.0\n5.0\n13.0\n9.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n6.0\n13.0\n10.0\n0.0\n0.0\n0.0\n\n\n1\n0.0\n0.0\n0.0\n12.0\n13.0\n5.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n11.0\n16.0\n10.0\n0.0\n0.0\n\n\n2\n0.0\n0.0\n0.0\n4.0\n15.0\n12.0\n0.0\n0.0\n0.0\n0.0\n...\n5.0\n0.0\n0.0\n0.0\n0.0\n3.0\n11.0\n16.0\n9.0\n0.0\n\n\n3\n0.0\n0.0\n7.0\n15.0\n13.0\n1.0\n0.0\n0.0\n0.0\n8.0\n...\n9.0\n0.0\n0.0\n0.0\n7.0\n13.0\n13.0\n9.0\n0.0\n0.0\n\n\n4\n0.0\n0.0\n0.0\n1.0\n11.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n2.0\n16.0\n4.0\n0.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1792\n0.0\n0.0\n4.0\n10.0\n13.0\n6.0\n0.0\n0.0\n0.0\n1.0\n...\n4.0\n0.0\n0.0\n0.0\n2.0\n14.0\n15.0\n9.0\n0.0\n0.0\n\n\n1793\n0.0\n0.0\n6.0\n16.0\n13.0\n11.0\n1.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n0.0\n0.0\n6.0\n16.0\n14.0\n6.0\n0.0\n0.0\n\n\n1794\n0.0\n0.0\n1.0\n11.0\n15.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n2.0\n9.0\n13.0\n6.0\n0.0\n0.0\n\n\n1795\n0.0\n0.0\n2.0\n10.0\n7.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n2.0\n0.0\n0.0\n0.0\n5.0\n12.0\n16.0\n12.0\n0.0\n0.0\n\n\n1796\n0.0\n0.0\n10.0\n14.0\n8.0\n1.0\n0.0\n0.0\n0.0\n2.0\n...\n8.0\n0.0\n0.0\n1.0\n8.0\n12.0\n14.0\n12.0\n1.0\n0.0\n\n\n\n\n1797 rows × 64 columns\n\n\n\n\nUse components such that 95% of variance is retained\n\n\nfrom sklearn.decomposition import PCA\n\npca = PCA(0.95)\nX_pca = pca.fit_transform(X)\nX_pca.shape\n\n(1797, 29)\n\n\n\npca.explained_variance_ratio_\n\narray([0.14890594, 0.13618771, 0.11794594, 0.08409979, 0.05782415,\n       0.0491691 , 0.04315987, 0.03661373, 0.03353248, 0.03078806,\n       0.02372341, 0.02272697, 0.01821863, 0.01773855, 0.01467101,\n       0.01409716, 0.01318589, 0.01248138, 0.01017718, 0.00905617,\n       0.00889538, 0.00797123, 0.00767493, 0.00722904, 0.00695889,\n       0.00596081, 0.00575615, 0.00515158, 0.0048954 ])\n\n\n\npca.n_components_\n\n29\n\n\nPCA created 29 components out of 64 original columns\n\nX_pca\n\narray([[ -1.25946645,  21.27488348,  -9.46305462, ...,   3.67072108,\n         -0.9436689 ,  -1.13250195],\n       [  7.9576113 , -20.76869896,   4.43950604, ...,   2.18261819,\n         -0.51022719,   2.31354911],\n       [  6.99192297,  -9.95598641,   2.95855808, ...,   4.22882114,\n          2.1576573 ,   0.8379578 ],\n       ...,\n       [ 10.8012837 ,  -6.96025223,   5.59955453, ...,  -3.56866194,\n          1.82444444,   3.53885886],\n       [ -4.87210009,  12.42395362, -10.17086635, ...,   3.25330054,\n          0.95484174,  -0.93895602],\n       [ -0.34438963,   6.36554919,  10.77370849, ...,  -3.01636722,\n          1.29752723,   2.58810313]])\n\n\n\nX_train_pca, X_test_pca, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=30)\n\n\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(X_train_pca, y_train)\nmodel.score(X_test_pca, y_test)\n\n0.9694444444444444\n\n\nLet’s now select only two components\n\npca = PCA(n_components=16)\nX_pca = pca.fit_transform(X)\nX_pca.shape\n\n(1797, 16)\n\n\n\nX_pca\n\narray([[ -1.2594598 ,  21.27487967,  -9.46303989, ...,  -3.31223438,\n          6.02583852,   2.68741732],\n       [  7.95761297, -20.7686941 ,   4.43950445, ...,  -6.50507014,\n         -2.27310577,  -2.14622234],\n       [  6.99193119,  -9.95600512,   2.95858793, ...,   6.83484372,\n         -1.65925951,  -4.25884236],\n       ...,\n       [ 10.80128998,  -6.96027365,   5.5995876 , ...,   1.24767177,\n          2.62633682,  -5.79460334],\n       [ -4.87210168,  12.42396132, -10.17087749, ...,  -1.21624842,\n         10.77743555,   2.38010911],\n       [ -0.34438951,   6.36554076,  10.77371718, ...,   7.0540168 ,\n          0.72142473,   0.27006973]])\n\n\n\npca.explained_variance_ratio_\n\narray([0.14890594, 0.13618771, 0.11794594, 0.08409979, 0.05782415,\n       0.0491691 , 0.04315987, 0.03661373, 0.03353248, 0.03078806,\n       0.02372336, 0.02272696, 0.01821856, 0.01773823, 0.01466903,\n       0.01409675])\n\n\nYou can see that both combined retains 0.14+0.13=0.27 or 27% of important feature information\n\nX_train_pca, X_test_pca, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=30)\n\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(X_train_pca, y_train)\nmodel.score(X_test_pca, y_test)\n\n0.9472222222222222\n\n\nWe get less accuancy (~60%) as using only 2 components did not retain much of the feature information. However in real life you will find many cases where using 2 or few PCA components can still give you a pretty good accuracy\n\n\n\n Back to top"
  },
  {
    "objectID": "svm.html",
    "href": "svm.html",
    "title": "Support Vector Regression",
    "section": "",
    "text": "import pandas as pd\nfrom sklearn.datasets import load_iris\niris = load_iris()\n\n\n\niris.feature_names\n\n['sepal length (cm)',\n 'sepal width (cm)',\n 'petal length (cm)',\n 'petal width (cm)']\n\n\n\niris.target_names\n\narray(['setosa', 'versicolor', 'virginica'], dtype='&lt;U10')\n\n\n\ndf = pd.DataFrame(iris.data,columns=iris.feature_names)\ndf.head()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n4.9\n3.0\n1.4\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\n\n\n3\n4.6\n3.1\n1.5\n0.2\n\n\n4\n5.0\n3.6\n1.4\n0.2\n\n\n\n\n\n\n\n\ndf['target'] = iris.target\ndf.head()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n0\n\n\n1\n4.9\n3.0\n1.4\n0.2\n0\n\n\n2\n4.7\n3.2\n1.3\n0.2\n0\n\n\n3\n4.6\n3.1\n1.5\n0.2\n0\n\n\n4\n5.0\n3.6\n1.4\n0.2\n0\n\n\n\n\n\n\n\n\ndf[df.target==1].head()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\n50\n7.0\n3.2\n4.7\n1.4\n1\n\n\n51\n6.4\n3.2\n4.5\n1.5\n1\n\n\n52\n6.9\n3.1\n4.9\n1.5\n1\n\n\n53\n5.5\n2.3\n4.0\n1.3\n1\n\n\n54\n6.5\n2.8\n4.6\n1.5\n1\n\n\n\n\n\n\n\n\ndf[df.target==2].head()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\n100\n6.3\n3.3\n6.0\n2.5\n2\n\n\n101\n5.8\n2.7\n5.1\n1.9\n2\n\n\n102\n7.1\n3.0\n5.9\n2.1\n2\n\n\n103\n6.3\n2.9\n5.6\n1.8\n2\n\n\n104\n6.5\n3.0\n5.8\n2.2\n2\n\n\n\n\n\n\n\n\ndf['flower_name'] =df.target.apply(lambda x: iris.target_names[x])\ndf.head()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\nflower_name\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n0\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\n0\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\n0\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\n0\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\n0\nsetosa\n\n\n\n\n\n\n\n\ndf[45:55]\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\nflower_name\n\n\n\n\n45\n4.8\n3.0\n1.4\n0.3\n0\nsetosa\n\n\n46\n5.1\n3.8\n1.6\n0.2\n0\nsetosa\n\n\n47\n4.6\n3.2\n1.4\n0.2\n0\nsetosa\n\n\n48\n5.3\n3.7\n1.5\n0.2\n0\nsetosa\n\n\n49\n5.0\n3.3\n1.4\n0.2\n0\nsetosa\n\n\n50\n7.0\n3.2\n4.7\n1.4\n1\nversicolor\n\n\n51\n6.4\n3.2\n4.5\n1.5\n1\nversicolor\n\n\n52\n6.9\n3.1\n4.9\n1.5\n1\nversicolor\n\n\n53\n5.5\n2.3\n4.0\n1.3\n1\nversicolor\n\n\n54\n6.5\n2.8\n4.6\n1.5\n1\nversicolor\n\n\n\n\n\n\n\n\ndf0 = df[:50]\ndf1 = df[50:100]\ndf2 = df[100:]\n\n\nimport matplotlib.pyplot as plt\n\nSepal length vs Sepal Width (Setosa vs Versicolor)\n\nplt.xlabel('Sepal Length')\nplt.ylabel('Sepal Width')\nplt.scatter(df0['sepal length (cm)'], df0['sepal width (cm)'],color=\"green\",marker='+')\nplt.scatter(df1['sepal length (cm)'], df1['sepal width (cm)'],color=\"blue\",marker='.')\n\n&lt;matplotlib.collections.PathCollection&gt;\n\n\n\n\n\nPetal length vs Pepal Width (Setosa vs Versicolor)\n\nplt.xlabel('Petal Length')\nplt.ylabel('Petal Width')\nplt.scatter(df0['petal length (cm)'], df0['petal width (cm)'],color=\"green\",marker='+')\nplt.scatter(df1['petal length (cm)'], df1['petal width (cm)'],color=\"blue\",marker='.')\n\n&lt;matplotlib.collections.PathCollection&gt;\n\n\n\n\n\nTrain Using Support Vector Machine (SVM)\n\nfrom sklearn.model_selection import train_test_split\n\n\nX = df.drop(['target','flower_name'], axis='columns')\ny = df.target\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n\nlen(X_train)\n\n120\n\n\n\nlen(X_test)\n\n30\n\n\n\nfrom sklearn.svm import SVC\nmodel = SVC()\n\n\nmodel.fit(X_train, y_train)\n\nSVC()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVCSVC()\n\n\n\nmodel.score(X_test, y_test)\n\n1.0\n\n\n\nmodel.predict([[4.8,3.0,1.5,0.3]])\n\n/home/benedict/mambaforge/envs/cfast/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but SVC was fitted with feature names\n  warnings.warn(\n\n\narray([0])\n\n\nTune parameters\n1. Regularization (C)\n\nmodel_C = SVC(C=1)\nmodel_C.fit(X_train, y_train)\nmodel_C.score(X_test, y_test)\n\n1.0\n\n\n\nmodel_C = SVC(C=10)\nmodel_C.fit(X_train, y_train)\nmodel_C.score(X_test, y_test)\n\n0.9666666666666667\n\n\n2. Gamma\n\nmodel_g = SVC(gamma=10)\nmodel_g.fit(X_train, y_train)\nmodel_g.score(X_test, y_test)\n\n0.9666666666666667\n\n\n3. Kernel\n\nmodel_linear_kernal = SVC(kernel='linear')\nmodel_linear_kernal.fit(X_train, y_train)\n\nSVC(kernel='linear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVCSVC(kernel='linear')\n\n\n\nmodel_linear_kernal.score(X_test, y_test)\n\n0.9666666666666667\n\n\nExercise\nTrain SVM classifier using sklearn digits dataset (i.e. from sklearn.datasets import load_digits) and then,\n\nMeasure accuracy of your model using different kernels such as rbf and linear.\nTune your model further using regularization and gamma parameters and try to come up with highest accurancy score\nUse 80% of samples as training data size\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ML",
    "section": "",
    "text": "This file will become your README and also the index of your documentation."
  },
  {
    "objectID": "index.html#step-for-git-push",
    "href": "index.html#step-for-git-push",
    "title": "ML",
    "section": "Step for git push",
    "text": "Step for git push\n\nnbdev_prepare\n\nnbdev_prepare\n\nGit stuff\n\ngit add .\ngit commit -m \"update\"\ngit push"
  },
  {
    "objectID": "index.html#after-changing-dependencies",
    "href": "index.html#after-changing-dependencies",
    "title": "ML",
    "section": "After changing dependencies",
    "text": "After changing dependencies\npip install ML\npip install -e '.[dev]'"
  },
  {
    "objectID": "loss_or_cost_funtions.html",
    "href": "loss_or_cost_funtions.html",
    "title": "Loss or Cost funtions",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "crossentropy.html",
    "href": "crossentropy.html",
    "title": "Cross Entropy",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "generative.html",
    "href": "generative.html",
    "title": "Generative Models",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "bayesian.html",
    "href": "bayesian.html",
    "title": "The Bayesian Approach and Gaussian Processes",
    "section": "",
    "text": "\\(P(queen/diamond) = \\dfrac{P(diamond/queen) * P(queen)}{P(diamond)}\\)\n$P(queen/diamond) = $\n\\(P(diamond/queen) = 1/4\\)\n\\(P(queen) = 1/13\\)\n\\(P(diamond) = 1/4\\)\n\\(P(queen/diamond) = \\dfrac{1/4 * 1/13}{1/4} = 1/13\\)\n\nimport pandas as pd\n\n\ndf = pd.read_csv(\"Data/titanic.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nPassengerId\nName\nPclass\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nSurvived\n\n\n\n\n0\n1\nBraund, Mr. Owen Harris\n3\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n0\n\n\n1\n2\nCumings, Mrs. John Bradley (Florence Briggs Th...\n1\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n1\n\n\n2\n3\nHeikkinen, Miss. Laina\n3\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n1\n\n\n3\n4\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n1\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n1\n\n\n4\n5\nAllen, Mr. William Henry\n3\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n0\n\n\n\n\n\n\n\n\ndf.drop(['PassengerId','Name','SibSp','Parch','Ticket','Cabin','Embarked'],axis='columns',inplace=True)\ndf.head()\n\n\n\n\n\n\n\n\nPclass\nSex\nAge\nFare\nSurvived\n\n\n\n\n0\n3\nmale\n22.0\n7.2500\n0\n\n\n1\n1\nfemale\n38.0\n71.2833\n1\n\n\n2\n3\nfemale\n26.0\n7.9250\n1\n\n\n3\n1\nfemale\n35.0\n53.1000\n1\n\n\n4\n3\nmale\n35.0\n8.0500\n0\n\n\n\n\n\n\n\n\ninputs = df.drop('Survived',axis='columns')\ntarget = df.Survived\n\n\n#inputs.Sex = inputs.Sex.map({'male': 1, 'female': 2})\n\n\ndummies = pd.get_dummies(inputs.Sex)\ndummies.head(3)\n\n\n\n\n\n\n\n\nfemale\nmale\n\n\n\n\n0\n0\n1\n\n\n1\n1\n0\n\n\n2\n1\n0\n\n\n\n\n\n\n\n\ninputs = pd.concat([inputs,dummies],axis='columns')\ninputs.head(3)\n\n\n\n\n\n\n\n\nPclass\nSex\nAge\nFare\nfemale\nmale\n\n\n\n\n0\n3\nmale\n22.0\n7.2500\n0\n1\n\n\n1\n1\nfemale\n38.0\n71.2833\n1\n0\n\n\n2\n3\nfemale\n26.0\n7.9250\n1\n0\n\n\n\n\n\n\n\nI am dropping male column as well because of dummy variable trap theory. One column is enough to repressent male vs female\n\ninputs.drop(['Sex','male'],axis='columns',inplace=True)\ninputs.head(3)\n\n\n\n\n\n\n\n\nPclass\nAge\nFare\nfemale\n\n\n\n\n0\n3\n22.0\n7.2500\n0\n\n\n1\n1\n38.0\n71.2833\n1\n\n\n2\n3\n26.0\n7.9250\n1\n\n\n\n\n\n\n\n\ninputs.columns[inputs.isna().any()]\n\nIndex(['Age'], dtype='object')\n\n\n\ninputs.Age[:10]\n\n0    22.0\n1    38.0\n2    26.0\n3    35.0\n4    35.0\n5     NaN\n6    54.0\n7     2.0\n8    27.0\n9    14.0\nName: Age, dtype: float64\n\n\n\ninputs.Age = inputs.Age.fillna(inputs.Age.mean())\ninputs.head()\n\n\n\n\n\n\n\n\nPclass\nAge\nFare\nfemale\n\n\n\n\n0\n3\n22.0\n7.2500\n0\n\n\n1\n1\n38.0\n71.2833\n1\n\n\n2\n3\n26.0\n7.9250\n1\n\n\n3\n1\n35.0\n53.1000\n1\n\n\n4\n3\n35.0\n8.0500\n0\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(inputs,target,test_size=0.3)\n\n\nfrom sklearn.naive_bayes import GaussianNB\nmodel = GaussianNB()\n\n\nmodel.fit(X_train,y_train)\n\nGaussianNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianNBGaussianNB()\n\n\n\nmodel.score(X_test,y_test)\n\n0.7910447761194029\n\n\n\nX_test[0:10]\n\n\n\n\n\n\n\n\nPclass\nAge\nFare\nfemale\n\n\n\n\n509\n3\n26.000000\n56.4958\n0\n\n\n325\n1\n36.000000\n135.6333\n1\n\n\n248\n1\n37.000000\n52.5542\n0\n\n\n391\n3\n21.000000\n7.7958\n0\n\n\n411\n3\n29.699118\n6.8583\n0\n\n\n688\n3\n18.000000\n7.7958\n0\n\n\n183\n2\n1.000000\n39.0000\n0\n\n\n14\n3\n14.000000\n7.8542\n1\n\n\n763\n1\n36.000000\n120.0000\n1\n\n\n383\n1\n35.000000\n52.0000\n1\n\n\n\n\n\n\n\n\ny_test[0:10]\n\n509    1\n325    1\n248    1\n391    1\n411    0\n688    0\n183    1\n14     0\n763    1\n383    1\nName: Survived, dtype: int64\n\n\n\nmodel.predict(X_test[0:10])\n\narray([0, 1, 0, 0, 0, 0, 0, 1, 1, 1])\n\n\n\nmodel.predict_proba(X_test[:10])\n\narray([[9.22826078e-01, 7.71739224e-02],\n       [1.90547332e-04, 9.99809453e-01],\n       [6.93224146e-01, 3.06775854e-01],\n       [9.59335969e-01, 4.06640310e-02],\n       [9.65380973e-01, 3.46190265e-02],\n       [9.56271850e-01, 4.37281504e-02],\n       [8.17245910e-01, 1.82754090e-01],\n       [3.83233278e-01, 6.16766722e-01],\n       [9.28107033e-04, 9.99071893e-01],\n       [6.70466692e-02, 9.32953331e-01]])\n\n\nCalculate the score using cross validation\n\nfrom sklearn.model_selection import cross_val_score\ncross_val_score(GaussianNB(),X_train, y_train, cv=5)\n\narray([0.784     , 0.728     , 0.744     , 0.75806452, 0.80645161])\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "pca_heart_disease_prediction_exercise_solution.html",
    "href": "pca_heart_disease_prediction_exercise_solution.html",
    "title": "PCA Machine Learning",
    "section": "",
    "text": "import pandas as pd\n\n# https://www.kaggle.com/fedesoriano/heart-failure-prediction\ndf = pd.read_csv(\"Data/heart.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nAge\nSex\nChestPainType\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nHeartDisease\n\n\n\n\n0\n40\nM\nATA\n140\n289\n0\nNormal\n172\nN\n0.0\nUp\n0\n\n\n1\n49\nF\nNAP\n160\n180\n0\nNormal\n156\nN\n1.0\nFlat\n1\n\n\n2\n37\nM\nATA\n130\n283\n0\nST\n98\nN\n0.0\nUp\n0\n\n\n3\n48\nF\nASY\n138\n214\n0\nNormal\n108\nY\n1.5\nFlat\n1\n\n\n4\n54\nM\nNAP\n150\n195\n0\nNormal\n122\nN\n0.0\nUp\n0\n\n\n\n\n\n\n\n\ndf.shape\n\n(918, 12)\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nAge\nRestingBP\nCholesterol\nFastingBS\nMaxHR\nOldpeak\nHeartDisease\n\n\n\n\ncount\n918.000000\n918.000000\n918.000000\n918.000000\n918.000000\n918.000000\n918.000000\n\n\nmean\n53.510893\n132.396514\n198.799564\n0.233115\n136.809368\n0.887364\n0.553377\n\n\nstd\n9.432617\n18.514154\n109.384145\n0.423046\n25.460334\n1.066570\n0.497414\n\n\nmin\n28.000000\n0.000000\n0.000000\n0.000000\n60.000000\n-2.600000\n0.000000\n\n\n25%\n47.000000\n120.000000\n173.250000\n0.000000\n120.000000\n0.000000\n0.000000\n\n\n50%\n54.000000\n130.000000\n223.000000\n0.000000\n138.000000\n0.600000\n1.000000\n\n\n75%\n60.000000\n140.000000\n267.000000\n0.000000\n156.000000\n1.500000\n1.000000\n\n\nmax\n77.000000\n200.000000\n603.000000\n1.000000\n202.000000\n6.200000\n1.000000\n\n\n\n\n\n\n\n\nTreat Outliers\n\n\ndf[df.Cholesterol&gt;(df.Cholesterol.mean()+3*df.Cholesterol.std())]\n\n\n\n\n\n\n\n\nAge\nSex\nChestPainType\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nHeartDisease\n\n\n\n\n76\n32\nM\nASY\n118\n529\n0\nNormal\n130\nN\n0.0\nFlat\n1\n\n\n149\n54\nM\nASY\n130\n603\n1\nNormal\n125\nY\n1.0\nFlat\n1\n\n\n616\n67\nF\nNAP\n115\n564\n0\nLVH\n160\nN\n1.6\nFlat\n0\n\n\n\n\n\n\n\n\ndf.shape\n\n(918, 12)\n\n\n\ndf1 = df[df.Cholesterol&lt;=(df.Cholesterol.mean()+3*df.Cholesterol.std())]\ndf1.shape\n\n(915, 12)\n\n\n\ndf[df.MaxHR&gt;(df.MaxHR.mean()+3*df.MaxHR.std())]\n\n\n\n\n\n\n\n\nAge\nSex\nChestPainType\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nHeartDisease\n\n\n\n\n\n\n\n\n\n\ndf[df.FastingBS&gt;(df.FastingBS.mean()+3*df.FastingBS.std())]\n\n\n\n\n\n\n\n\nAge\nSex\nChestPainType\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nHeartDisease\n\n\n\n\n\n\n\n\n\n\ndf[df.Oldpeak&gt;(df.Oldpeak.mean()+3*df.Oldpeak.std())]\n\n\n\n\n\n\n\n\nAge\nSex\nChestPainType\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nHeartDisease\n\n\n\n\n166\n50\nM\nASY\n140\n231\n0\nST\n140\nY\n5.0\nFlat\n1\n\n\n702\n59\nM\nTA\n178\n270\n0\nLVH\n145\nN\n4.2\nDown\n0\n\n\n771\n55\nM\nASY\n140\n217\n0\nNormal\n111\nY\n5.6\nDown\n1\n\n\n791\n51\nM\nASY\n140\n298\n0\nNormal\n122\nY\n4.2\nFlat\n1\n\n\n850\n62\nF\nASY\n160\n164\n0\nLVH\n145\nN\n6.2\nDown\n1\n\n\n900\n58\nM\nASY\n114\n318\n0\nST\n140\nN\n4.4\nDown\n1\n\n\n\n\n\n\n\n\ndf2 = df1[df1.Oldpeak&lt;=(df1.Oldpeak.mean()+3*df1.Oldpeak.std())]\ndf2.shape\n\n(909, 12)\n\n\n\ndf[df.RestingBP&gt;(df.RestingBP.mean()+3*df.RestingBP.std())]\n\n\n\n\n\n\n\n\nAge\nSex\nChestPainType\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nHeartDisease\n\n\n\n\n109\n39\nM\nATA\n190\n241\n0\nNormal\n106\nN\n0.0\nUp\n0\n\n\n241\n54\nM\nASY\n200\n198\n0\nNormal\n142\nY\n2.0\nFlat\n1\n\n\n365\n64\nF\nASY\n200\n0\n0\nNormal\n140\nY\n1.0\nFlat\n1\n\n\n399\n61\nM\nNAP\n200\n0\n1\nST\n70\nN\n0.0\nFlat\n1\n\n\n592\n61\nM\nASY\n190\n287\n1\nLVH\n150\nY\n2.0\nDown\n1\n\n\n732\n56\nF\nASY\n200\n288\n1\nLVH\n133\nY\n4.0\nDown\n1\n\n\n759\n54\nM\nATA\n192\n283\n0\nLVH\n195\nN\n0.0\nUp\n1\n\n\n\n\n\n\n\n\ndf3 = df2[df2.RestingBP&lt;=(df2.RestingBP.mean()+3*df2.RestingBP.std())]\ndf3.shape\n\n(902, 12)\n\n\n\ndf.ChestPainType.unique()\n\narray(['ATA', 'NAP', 'ASY', 'TA'], dtype=object)\n\n\n\ndf.RestingECG.unique()\n\narray(['Normal', 'ST', 'LVH'], dtype=object)\n\n\n\ndf.ExerciseAngina.unique()\n\narray(['N', 'Y'], dtype=object)\n\n\n\ndf.ST_Slope.unique()\n\narray(['Up', 'Flat', 'Down'], dtype=object)\n\n\n\ndf4 = df3.copy()\ndf4.ExerciseAngina.replace(\n    {\n        'N': 0,\n        'Y': 1\n    },\n    inplace=True)\n\ndf4.ST_Slope.replace(\n    {\n        'Down': 1,\n        'Flat': 2,\n        'Up': 3\n    },\n    inplace=True\n)\n\ndf4.RestingECG.replace(\n    {\n        'Normal': 1,\n        'ST': 2,\n        'LVH': 3\n    },\n    inplace=True)\n\ndf4.head()\n\n\n\n\n\n\n\n\nAge\nSex\nChestPainType\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nHeartDisease\n\n\n\n\n0\n40\nM\nATA\n140\n289\n0\n1\n172\n0\n0.0\n3\n0\n\n\n1\n49\nF\nNAP\n160\n180\n0\n1\n156\n0\n1.0\n2\n1\n\n\n2\n37\nM\nATA\n130\n283\n0\n2\n98\n0\n0.0\n3\n0\n\n\n3\n48\nF\nASY\n138\n214\n0\n1\n108\n1\n1.5\n2\n1\n\n\n4\n54\nM\nNAP\n150\n195\n0\n1\n122\n0\n0.0\n3\n0\n\n\n\n\n\n\n\n\ndf5 = pd.get_dummies(df4, drop_first=True)\ndf5.head()\n\n\n\n\n\n\n\n\nAge\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nHeartDisease\nSex_M\nChestPainType_ATA\nChestPainType_NAP\nChestPainType_TA\n\n\n\n\n0\n40\n140\n289\n0\n1\n172\n0\n0.0\n3\n0\n1\n1\n0\n0\n\n\n1\n49\n160\n180\n0\n1\n156\n0\n1.0\n2\n1\n0\n0\n1\n0\n\n\n2\n37\n130\n283\n0\n2\n98\n0\n0.0\n3\n0\n1\n1\n0\n0\n\n\n3\n48\n138\n214\n0\n1\n108\n1\n1.5\n2\n1\n0\n0\n0\n0\n\n\n4\n54\n150\n195\n0\n1\n122\n0\n0.0\n3\n0\n1\n0\n1\n0\n\n\n\n\n\n\n\n\nX = df5.drop(\"HeartDisease\",axis='columns')\ny = df5.HeartDisease\n\nX.head()\n\n\n\n\n\n\n\n\nAge\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nSex_M\nChestPainType_ATA\nChestPainType_NAP\nChestPainType_TA\n\n\n\n\n0\n40\n140\n289\n0\n1\n172\n0\n0.0\n3\n1\n1\n0\n0\n\n\n1\n49\n160\n180\n0\n1\n156\n0\n1.0\n2\n0\n0\n1\n0\n\n\n2\n37\n130\n283\n0\n2\n98\n0\n0.0\n3\n1\n1\n0\n0\n\n\n3\n48\n138\n214\n0\n1\n108\n1\n1.5\n2\n0\n0\n0\n0\n\n\n4\n54\n150\n195\n0\n1\n122\n0\n0.0\n3\n1\n0\n1\n0\n\n\n\n\n\n\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nX_scaled\n\narray([[-1.42896269,  0.46089071,  0.85238015, ...,  2.06757196,\n        -0.53547478, -0.22914788],\n       [-0.47545956,  1.5925728 , -0.16132855, ..., -0.4836591 ,\n         1.86750159, -0.22914788],\n       [-1.74679706, -0.10495034,  0.79657967, ...,  2.06757196,\n        -0.53547478, -0.22914788],\n       ...,\n       [ 0.37209878, -0.10495034, -0.61703246, ..., -0.4836591 ,\n        -0.53547478, -0.22914788],\n       [ 0.37209878, -0.10495034,  0.35947592, ...,  2.06757196,\n        -0.53547478, -0.22914788],\n       [-1.64085227,  0.3477225 , -0.20782894, ..., -0.4836591 ,\n         1.86750159, -0.22914788]])\n\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=30)\n\n\nX_train.shape\n\n(721, 13)\n\n\n\nX_test.shape\n\n(181, 13)\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel_rf = RandomForestClassifier()\nmodel_rf.fit(X_train, y_train)\nmodel_rf.score(X_test, y_test)\n\n0.8674033149171271\n\n\n\nUse PCA to reduce dimensions\n\n\nX\n\n\n\n\n\n\n\n\nAge\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nSex_M\nChestPainType_ATA\nChestPainType_NAP\nChestPainType_TA\n\n\n\n\n0\n40\n140\n289\n0\n1\n172\n0\n0.0\n3\n1\n1\n0\n0\n\n\n1\n49\n160\n180\n0\n1\n156\n0\n1.0\n2\n0\n0\n1\n0\n\n\n2\n37\n130\n283\n0\n2\n98\n0\n0.0\n3\n1\n1\n0\n0\n\n\n3\n48\n138\n214\n0\n1\n108\n1\n1.5\n2\n0\n0\n0\n0\n\n\n4\n54\n150\n195\n0\n1\n122\n0\n0.0\n3\n1\n0\n1\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n913\n45\n110\n264\n0\n1\n132\n0\n1.2\n2\n1\n0\n0\n1\n\n\n914\n68\n144\n193\n1\n1\n141\n0\n3.4\n2\n1\n0\n0\n0\n\n\n915\n57\n130\n131\n0\n1\n115\n1\n1.2\n2\n1\n0\n0\n0\n\n\n916\n57\n130\n236\n0\n3\n174\n0\n0.0\n2\n0\n1\n0\n0\n\n\n917\n38\n138\n175\n0\n1\n173\n0\n0.0\n3\n1\n0\n1\n0\n\n\n\n\n902 rows × 13 columns\n\n\n\n\nfrom sklearn.decomposition import PCA\n\npca = PCA(0.95)\nX_pca = pca.fit_transform(X)\nX_pca\n\narray([[ 93.82465373, -29.40099458],\n       [-15.58422331, -14.10909233],\n       [ 83.29606634,  38.6867453 ],\n       ...,\n       [-67.57318721,  17.61319354],\n       [ 40.70458237, -33.38750602],\n       [-19.91368346, -37.29085722]])\n\n\n\nX_train_pca, X_test_pca, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=30)\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel_rf = RandomForestClassifier()\nmodel_rf.fit(X_train_pca, y_train)\nmodel_rf.score(X_test_pca, y_test)\n\n0.7182320441988951\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "pca_heart_disease_prediction.html",
    "href": "pca_heart_disease_prediction.html",
    "title": "PCA Machine Learning",
    "section": "",
    "text": "import pandas as pd\n\n# https://www.kaggle.com/fedesoriano/heart-failure-prediction\ndf = pd.read_csv(\"Data/heart.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nAge\nSex\nChestPainType\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nHeartDisease\n\n\n\n\n0\n40\nM\nATA\n140\n289\n0\nNormal\n172\nN\n0.0\nUp\n0\n\n\n1\n49\nF\nNAP\n160\n180\n0\nNormal\n156\nN\n1.0\nFlat\n1\n\n\n2\n37\nM\nATA\n130\n283\n0\nST\n98\nN\n0.0\nUp\n0\n\n\n3\n48\nF\nASY\n138\n214\n0\nNormal\n108\nY\n1.5\nFlat\n1\n\n\n4\n54\nM\nNAP\n150\n195\n0\nNormal\n122\nN\n0.0\nUp\n0\n\n\n\n\n\n\n\n\ndf.shape\n\n(918, 12)\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nAge\nRestingBP\nCholesterol\nFastingBS\nMaxHR\nOldpeak\nHeartDisease\n\n\n\n\ncount\n918.000000\n918.000000\n918.000000\n918.000000\n918.000000\n918.000000\n918.000000\n\n\nmean\n53.510893\n132.396514\n198.799564\n0.233115\n136.809368\n0.887364\n0.553377\n\n\nstd\n9.432617\n18.514154\n109.384145\n0.423046\n25.460334\n1.066570\n0.497414\n\n\nmin\n28.000000\n0.000000\n0.000000\n0.000000\n60.000000\n-2.600000\n0.000000\n\n\n25%\n47.000000\n120.000000\n173.250000\n0.000000\n120.000000\n0.000000\n0.000000\n\n\n50%\n54.000000\n130.000000\n223.000000\n0.000000\n138.000000\n0.600000\n1.000000\n\n\n75%\n60.000000\n140.000000\n267.000000\n0.000000\n156.000000\n1.500000\n1.000000\n\n\nmax\n77.000000\n200.000000\n603.000000\n1.000000\n202.000000\n6.200000\n1.000000\n\n\n\n\n\n\n\n\nTreat Outliers\n\n\ndf[df.Cholesterol&gt;(df.Cholesterol.mean()+3*df.Cholesterol.std())]\n\n\n\n\n\n\n\n\nAge\nSex\nChestPainType\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nHeartDisease\n\n\n\n\n76\n32\nM\nASY\n118\n529\n0\nNormal\n130\nN\n0.0\nFlat\n1\n\n\n149\n54\nM\nASY\n130\n603\n1\nNormal\n125\nY\n1.0\nFlat\n1\n\n\n616\n67\nF\nNAP\n115\n564\n0\nLVH\n160\nN\n1.6\nFlat\n0\n\n\n\n\n\n\n\n\ndf.shape\n\n(918, 12)\n\n\n\ndf1 = df[df.Cholesterol&lt;=(df.Cholesterol.mean()+3*df.Cholesterol.std())]\ndf1.shape\n\n(915, 12)\n\n\n\ndf[df.MaxHR&gt;(df.MaxHR.mean()+3*df.MaxHR.std())]\n\n\n\n\n\n\n\n\nAge\nSex\nChestPainType\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nHeartDisease\n\n\n\n\n\n\n\n\n\n\ndf[df.FastingBS&gt;(df.FastingBS.mean()+3*df.FastingBS.std())]\n\n\n\n\n\n\n\n\nAge\nSex\nChestPainType\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nHeartDisease\n\n\n\n\n\n\n\n\n\n\ndf[df.Oldpeak&gt;(df.Oldpeak.mean()+3*df.Oldpeak.std())]\n\n\n\n\n\n\n\n\nAge\nSex\nChestPainType\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nHeartDisease\n\n\n\n\n166\n50\nM\nASY\n140\n231\n0\nST\n140\nY\n5.0\nFlat\n1\n\n\n702\n59\nM\nTA\n178\n270\n0\nLVH\n145\nN\n4.2\nDown\n0\n\n\n771\n55\nM\nASY\n140\n217\n0\nNormal\n111\nY\n5.6\nDown\n1\n\n\n791\n51\nM\nASY\n140\n298\n0\nNormal\n122\nY\n4.2\nFlat\n1\n\n\n850\n62\nF\nASY\n160\n164\n0\nLVH\n145\nN\n6.2\nDown\n1\n\n\n900\n58\nM\nASY\n114\n318\n0\nST\n140\nN\n4.4\nDown\n1\n\n\n\n\n\n\n\n\ndf2 = df1[df1.Oldpeak&lt;=(df1.Oldpeak.mean()+3*df1.Oldpeak.std())]\ndf2.shape\n\n(909, 12)\n\n\n\ndf[df.RestingBP&gt;(df.RestingBP.mean()+3*df.RestingBP.std())]\n\n\n\n\n\n\n\n\nAge\nSex\nChestPainType\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nHeartDisease\n\n\n\n\n109\n39\nM\nATA\n190\n241\n0\nNormal\n106\nN\n0.0\nUp\n0\n\n\n241\n54\nM\nASY\n200\n198\n0\nNormal\n142\nY\n2.0\nFlat\n1\n\n\n365\n64\nF\nASY\n200\n0\n0\nNormal\n140\nY\n1.0\nFlat\n1\n\n\n399\n61\nM\nNAP\n200\n0\n1\nST\n70\nN\n0.0\nFlat\n1\n\n\n592\n61\nM\nASY\n190\n287\n1\nLVH\n150\nY\n2.0\nDown\n1\n\n\n732\n56\nF\nASY\n200\n288\n1\nLVH\n133\nY\n4.0\nDown\n1\n\n\n759\n54\nM\nATA\n192\n283\n0\nLVH\n195\nN\n0.0\nUp\n1\n\n\n\n\n\n\n\n\ndf3 = df2[df2.RestingBP&lt;=(df2.RestingBP.mean()+3*df2.RestingBP.std())]\ndf3.shape\n\n(902, 12)\n\n\n\ndf.ChestPainType.unique()\n\narray(['ATA', 'NAP', 'ASY', 'TA'], dtype=object)\n\n\n\ndf.RestingECG.unique()\n\narray(['Normal', 'ST', 'LVH'], dtype=object)\n\n\n\ndf.ExerciseAngina.unique()\n\narray(['N', 'Y'], dtype=object)\n\n\n\ndf.ST_Slope.unique()\n\narray(['Up', 'Flat', 'Down'], dtype=object)\n\n\n\ndf4 = df3.copy()\ndf4.ExerciseAngina.replace(\n    {\n        'N': 0,\n        'Y': 1\n    },\n    inplace=True)\n\ndf4.ST_Slope.replace(\n    {\n        'Down': 1,\n        'Flat': 2,\n        'Up': 3\n    },\n    inplace=True\n)\n\ndf4.RestingECG.replace(\n    {\n        'Normal': 1,\n        'ST': 2,\n        'LVH': 3\n    },\n    inplace=True)\n\ndf4.head()\n\n\n\n\n\n\n\n\nAge\nSex\nChestPainType\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nHeartDisease\n\n\n\n\n0\n40\nM\nATA\n140\n289\n0\n1\n172\n0\n0.0\n3\n0\n\n\n1\n49\nF\nNAP\n160\n180\n0\n1\n156\n0\n1.0\n2\n1\n\n\n2\n37\nM\nATA\n130\n283\n0\n2\n98\n0\n0.0\n3\n0\n\n\n3\n48\nF\nASY\n138\n214\n0\n1\n108\n1\n1.5\n2\n1\n\n\n4\n54\nM\nNAP\n150\n195\n0\n1\n122\n0\n0.0\n3\n0\n\n\n\n\n\n\n\n\ndf5 = pd.get_dummies(df4, drop_first=True)\ndf5.head()\n\n\n\n\n\n\n\n\nAge\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nHeartDisease\nSex_M\nChestPainType_ATA\nChestPainType_NAP\nChestPainType_TA\n\n\n\n\n0\n40\n140\n289\n0\n1\n172\n0\n0.0\n3\n0\n1\n1\n0\n0\n\n\n1\n49\n160\n180\n0\n1\n156\n0\n1.0\n2\n1\n0\n0\n1\n0\n\n\n2\n37\n130\n283\n0\n2\n98\n0\n0.0\n3\n0\n1\n1\n0\n0\n\n\n3\n48\n138\n214\n0\n1\n108\n1\n1.5\n2\n1\n0\n0\n0\n0\n\n\n4\n54\n150\n195\n0\n1\n122\n0\n0.0\n3\n0\n1\n0\n1\n0\n\n\n\n\n\n\n\n\nX = df5.drop(\"HeartDisease\",axis='columns')\ny = df5.HeartDisease\n\nX.head()\n\n\n\n\n\n\n\n\nAge\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nSex_M\nChestPainType_ATA\nChestPainType_NAP\nChestPainType_TA\n\n\n\n\n0\n40\n140\n289\n0\n1\n172\n0\n0.0\n3\n1\n1\n0\n0\n\n\n1\n49\n160\n180\n0\n1\n156\n0\n1.0\n2\n0\n0\n1\n0\n\n\n2\n37\n130\n283\n0\n2\n98\n0\n0.0\n3\n1\n1\n0\n0\n\n\n3\n48\n138\n214\n0\n1\n108\n1\n1.5\n2\n0\n0\n0\n0\n\n\n4\n54\n150\n195\n0\n1\n122\n0\n0.0\n3\n1\n0\n1\n0\n\n\n\n\n\n\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nX_scaled\n\narray([[-1.42896269,  0.46089071,  0.85238015, ...,  2.06757196,\n        -0.53547478, -0.22914788],\n       [-0.47545956,  1.5925728 , -0.16132855, ..., -0.4836591 ,\n         1.86750159, -0.22914788],\n       [-1.74679706, -0.10495034,  0.79657967, ...,  2.06757196,\n        -0.53547478, -0.22914788],\n       ...,\n       [ 0.37209878, -0.10495034, -0.61703246, ..., -0.4836591 ,\n        -0.53547478, -0.22914788],\n       [ 0.37209878, -0.10495034,  0.35947592, ...,  2.06757196,\n        -0.53547478, -0.22914788],\n       [-1.64085227,  0.3477225 , -0.20782894, ..., -0.4836591 ,\n         1.86750159, -0.22914788]])\n\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=30)\n\n\nX_train.shape\n\n(721, 13)\n\n\n\nX_test.shape\n\n(181, 13)\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel_rf = RandomForestClassifier()\nmodel_rf.fit(X_train, y_train)\nmodel_rf.score(X_test, y_test)\n\n0.8674033149171271\n\n\n\nUse PCA to reduce dimensions\n\n\nX\n\n\n\n\n\n\n\n\nAge\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nSex_M\nChestPainType_ATA\nChestPainType_NAP\nChestPainType_TA\n\n\n\n\n0\n40\n140\n289\n0\n1\n172\n0\n0.0\n3\n1\n1\n0\n0\n\n\n1\n49\n160\n180\n0\n1\n156\n0\n1.0\n2\n0\n0\n1\n0\n\n\n2\n37\n130\n283\n0\n2\n98\n0\n0.0\n3\n1\n1\n0\n0\n\n\n3\n48\n138\n214\n0\n1\n108\n1\n1.5\n2\n0\n0\n0\n0\n\n\n4\n54\n150\n195\n0\n1\n122\n0\n0.0\n3\n1\n0\n1\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n913\n45\n110\n264\n0\n1\n132\n0\n1.2\n2\n1\n0\n0\n1\n\n\n914\n68\n144\n193\n1\n1\n141\n0\n3.4\n2\n1\n0\n0\n0\n\n\n915\n57\n130\n131\n0\n1\n115\n1\n1.2\n2\n1\n0\n0\n0\n\n\n916\n57\n130\n236\n0\n3\n174\n0\n0.0\n2\n0\n1\n0\n0\n\n\n917\n38\n138\n175\n0\n1\n173\n0\n0.0\n3\n1\n0\n1\n0\n\n\n\n\n902 rows × 13 columns\n\n\n\n\nfrom sklearn.decomposition import PCA\n\npca = PCA(0.95)\nX_pca = pca.fit_transform(X)\nX_pca\n\narray([[ 93.82465373, -29.40099458],\n       [-15.58422331, -14.10909233],\n       [ 83.29606634,  38.6867453 ],\n       ...,\n       [-67.57318721,  17.61319354],\n       [ 40.70458237, -33.38750602],\n       [-19.91368346, -37.29085722]])\n\n\n\nX_train_pca, X_test_pca, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=30)\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel_rf = RandomForestClassifier()\nmodel_rf.fit(X_train_pca, y_train)\nmodel_rf.score(X_test_pca, y_test)\n\n0.7182320441988951\n\n\n\n\n\n Back to top"
  }
]