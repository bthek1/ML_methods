[
  {
    "objectID": "k_mean_clustering.html",
    "href": "k_mean_clustering.html",
    "title": "K Mean Clustering",
    "section": "",
    "text": "K-means clustering aims to partition a dataset into KK distinct, non-overlapping clusters. Each cluster is represented by its centroid, and the goal is to minimize the sum of the squared distances between each point and its corresponding centroid.\n\n\n\nThe objective of K-means is to minimize the inertia, also known as the within-cluster sum of squares (WCSS):\nInertia=∑k=1K∑x∈Ck∥x−μk∥2Inertia=∑k=1K​∑x∈Ck​​∥x−μk​∥2\nwhere: - KK is the number of clusters. - CkCk​ is the set of points in cluster kk. - μkμk​ is the centroid of cluster kk. - ∥x−μk∥∥x−μk​∥ is the Euclidean distance between point xx and centroid μkμk​.",
    "crumbs": [
      "Blog",
      "K Mean Clustering"
    ]
  },
  {
    "objectID": "k_mean_clustering.html#introduction-to-k-means-clustering",
    "href": "k_mean_clustering.html#introduction-to-k-means-clustering",
    "title": "K Mean Clustering",
    "section": "",
    "text": "K-means clustering aims to partition a dataset into KK distinct, non-overlapping clusters. Each cluster is represented by its centroid, and the goal is to minimize the sum of the squared distances between each point and its corresponding centroid.\n\n\n\nThe objective of K-means is to minimize the inertia, also known as the within-cluster sum of squares (WCSS):\nInertia=∑k=1K∑x∈Ck∥x−μk∥2Inertia=∑k=1K​∑x∈Ck​​∥x−μk​∥2\nwhere: - KK is the number of clusters. - CkCk​ is the set of points in cluster kk. - μkμk​ is the centroid of cluster kk. - ∥x−μk∥∥x−μk​∥ is the Euclidean distance between point xx and centroid μkμk​.",
    "crumbs": [
      "Blog",
      "K Mean Clustering"
    ]
  },
  {
    "objectID": "k_mean_clustering.html#algorithm-steps",
    "href": "k_mean_clustering.html#algorithm-steps",
    "title": "K Mean Clustering",
    "section": "Algorithm Steps",
    "text": "Algorithm Steps\nK-means clustering involves the following iterative process: - Initialization: Randomly select KK initial centroids. - Assignment Step: Assign each data point to the nearest centroid, forming KK clusters. - Update Step: Recalculate the centroids of each cluster based on the current assignments. - Convergence Check: Repeat the assignment and update steps until the centroids no longer change significantly or a maximum number of iterations is reached.",
    "crumbs": [
      "Blog",
      "K Mean Clustering"
    ]
  },
  {
    "objectID": "k_mean_clustering.html#choosing-the-number-of-clusters-k",
    "href": "k_mean_clustering.html#choosing-the-number-of-clusters-k",
    "title": "K Mean Clustering",
    "section": "Choosing the Number of Clusters (K)",
    "text": "Choosing the Number of Clusters (K)\nDetermining the appropriate number of clusters is a critical decision. Some common methods include: - Elbow Method: Plot the inertia for different values of KK. The “elbow” point, where the rate of decrease sharply slows down, indicates a suitable number of clusters. - Silhouette Score: Measures the quality of the clustering by evaluating how similar a point is to its own cluster compared to other clusters. A higher silhouette score indicates better-defined clusters.",
    "crumbs": [
      "Blog",
      "K Mean Clustering"
    ]
  },
  {
    "objectID": "k_mean_clustering.html#advantages-and-disadvantages",
    "href": "k_mean_clustering.html#advantages-and-disadvantages",
    "title": "K Mean Clustering",
    "section": "Advantages and Disadvantages",
    "text": "Advantages and Disadvantages\nAdvantages: - Simplicity: Easy to understand and implement. - Scalability: Efficient for large datasets, especially when using the Lloyd’s algorithm.\nDisadvantages: - Fixed Number of Clusters: The number of clusters KK must be specified in advance. - Sensitivity to Initialization: The final clusters can vary based on the initial selection of centroids. - Not Suitable for Non-Globular Clusters: K-means assumes that clusters are spherical and of similar size, which may not always be the case. - Sensitive to Outliers: Outliers can distort the cluster centroids significantly.",
    "crumbs": [
      "Blog",
      "K Mean Clustering"
    ]
  },
  {
    "objectID": "k_mean_clustering.html#example-with-python",
    "href": "k_mean_clustering.html#example-with-python",
    "title": "K Mean Clustering",
    "section": "Example with Python",
    "text": "Example with Python\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\n\n# Generating synthetic data\nnp.random.seed(42)\nX = np.vstack([\n    np.random.normal(loc=[2, 2], scale=0.5, size=(100, 2)),\n    np.random.normal(loc=[-2, -2], scale=0.5, size=(100, 2)),\n    np.random.normal(loc=[2, -2], scale=0.5, size=(100, 2))\n])\n\n# Applying K-means clustering\nkmeans = KMeans(n_clusters=3, random_state=42)\nkmeans.fit(X)\n\n# Plotting the results\nplt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap='viridis')\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red', marker='X')\nplt.title('K-means Clustering')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.show()\n\n\n\n\n\n\n\n\nPractical Considerations\n\nData Preprocessing: Standardizing the data can improve the performance of K-means, especially when features have different units or scales.\nAlgorithm Variants: Variants like MiniBatchKMeans can be used for large datasets as they are more memory-efficient.\nEvaluation: Use metrics like the inertia, silhouette score, or cluster purity to evaluate the quality of the clusters.",
    "crumbs": [
      "Blog",
      "K Mean Clustering"
    ]
  },
  {
    "objectID": "k_mean_clustering.html#extensions-and-alternatives",
    "href": "k_mean_clustering.html#extensions-and-alternatives",
    "title": "K Mean Clustering",
    "section": "Extensions and Alternatives",
    "text": "Extensions and Alternatives\n\nK-means++: A variant that improves the initialization of centroids, leading to better results.\nHierarchical Clustering: Does not require the number of clusters to be specified in advance.\nDBSCAN: Can find clusters of arbitrary shape and is robust to outliers.\nGaussian Mixture Models (GMM): A probabilistic model that can represent clusters with elliptical shapes.",
    "crumbs": [
      "Blog",
      "K Mean Clustering"
    ]
  },
  {
    "objectID": "decisiontree.html",
    "href": "decisiontree.html",
    "title": "Decision Tree",
    "section": "",
    "text": "Simple to understand, interpret, visualize.\nDecision trees implicitly performvariable screening or feature selection.\nCan handle both numerical and categorical data. Can also handle multi-output problems.\nDecision trees require relatively little effort from users for data preparation.\nNonlinear relationships between parameters do not affect tree performance.",
    "crumbs": [
      "Blog",
      "Decision Tree"
    ]
  },
  {
    "objectID": "decisiontree.html#advantages-of-cart",
    "href": "decisiontree.html#advantages-of-cart",
    "title": "Decision Tree",
    "section": "",
    "text": "Simple to understand, interpret, visualize.\nDecision trees implicitly performvariable screening or feature selection.\nCan handle both numerical and categorical data. Can also handle multi-output problems.\nDecision trees require relatively little effort from users for data preparation.\nNonlinear relationships between parameters do not affect tree performance.",
    "crumbs": [
      "Blog",
      "Decision Tree"
    ]
  },
  {
    "objectID": "decisiontree.html#disadvantages-of-cart",
    "href": "decisiontree.html#disadvantages-of-cart",
    "title": "Decision Tree",
    "section": "Disadvantages of CART",
    "text": "Disadvantages of CART\n\nDecision-tree learners can create over-complex trees that do not generalize the data well. This is called overfitting.\nDecision trees can be unstable because small variations in the data might result in a completely different tree being generated. This is called variance, which needs to be lowered by methods like bagging and boosting.\nGreedy algorithms cannot guarantee to return the globally optimal decision tree. This can be mitigated by training multiple trees, where the features and samples are randomly sampled with replacement.\nDecision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the data set prior to fitting with the decision tree.\n\n\nimport pandas as pd\nfrom fastai.vision.all import *\n\n\npath = Path('Data')\nname = 'salaries.csv'\n\n\ndf = pd.read_csv(path/name)\ndf.head()\n\n\n\n\n\n\n\n\ncompany\njob\ndegree\nsalary_more_then_100k\n\n\n\n\n0\ngoogle\nsales executive\nbachelors\n0\n\n\n1\ngoogle\nsales executive\nmasters\n0\n\n\n2\ngoogle\nbusiness manager\nbachelors\n1\n\n\n3\ngoogle\nbusiness manager\nmasters\n1\n\n\n4\ngoogle\ncomputer programmer\nbachelors\n0\n\n\n\n\n\n\n\n\ninputs = df.drop('salary_more_then_100k',axis='columns')\n\n\ntarget = df['salary_more_then_100k']\n\n\nfrom sklearn.preprocessing import LabelEncoder\n\n\nle_company = LabelEncoder()\nle_job = LabelEncoder()\nle_degree = LabelEncoder()\n\n\ninputs['company_n'] = le_company.fit_transform(inputs['company'])\ninputs['job_n'] = le_job.fit_transform(inputs['job'])\ninputs['degree_n'] = le_degree.fit_transform(inputs['degree'])\n\n\ninputs\n\n\n\n\n\n\n\n\ncompany\njob\ndegree\ncompany_n\njob_n\ndegree_n\n\n\n\n\n0\ngoogle\nsales executive\nbachelors\n2\n2\n0\n\n\n1\ngoogle\nsales executive\nmasters\n2\n2\n1\n\n\n2\ngoogle\nbusiness manager\nbachelors\n2\n0\n0\n\n\n3\ngoogle\nbusiness manager\nmasters\n2\n0\n1\n\n\n4\ngoogle\ncomputer programmer\nbachelors\n2\n1\n0\n\n\n5\ngoogle\ncomputer programmer\nmasters\n2\n1\n1\n\n\n6\nabc pharma\nsales executive\nmasters\n0\n2\n1\n\n\n7\nabc pharma\ncomputer programmer\nbachelors\n0\n1\n0\n\n\n8\nabc pharma\nbusiness manager\nbachelors\n0\n0\n0\n\n\n9\nabc pharma\nbusiness manager\nmasters\n0\n0\n1\n\n\n10\nfacebook\nsales executive\nbachelors\n1\n2\n0\n\n\n11\nfacebook\nsales executive\nmasters\n1\n2\n1\n\n\n12\nfacebook\nbusiness manager\nbachelors\n1\n0\n0\n\n\n13\nfacebook\nbusiness manager\nmasters\n1\n0\n1\n\n\n14\nfacebook\ncomputer programmer\nbachelors\n1\n1\n0\n\n\n15\nfacebook\ncomputer programmer\nmasters\n1\n1\n1\n\n\n\n\n\n\n\n\ninputs_n = inputs.drop(['company','job','degree'],axis='columns')\n\n\ninputs_n\n\n\n\n\n\n\n\n\ncompany_n\njob_n\ndegree_n\n\n\n\n\n0\n2\n2\n0\n\n\n1\n2\n2\n1\n\n\n2\n2\n0\n0\n\n\n3\n2\n0\n1\n\n\n4\n2\n1\n0\n\n\n5\n2\n1\n1\n\n\n6\n0\n2\n1\n\n\n7\n0\n1\n0\n\n\n8\n0\n0\n0\n\n\n9\n0\n0\n1\n\n\n10\n1\n2\n0\n\n\n11\n1\n2\n1\n\n\n12\n1\n0\n0\n\n\n13\n1\n0\n1\n\n\n14\n1\n1\n0\n\n\n15\n1\n1\n1\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\nnp.random.seed(42)\nn_points = len(inputs_n)\nx = np.random.rand(n_points)\ny = np.random.rand(n_points)\nz = np.random.rand(n_points)\n\n\ninputs_n['company_n'] += (x-0.5)/10\ninputs_n['job_n'] += (y-0.5)/10\ninputs_n['degree_n'] += (z-0.5)/10\n\n\n# Create a 3D scatter plot\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\nax.set_xlabel('degree_n')\nax.set_ylabel('job_n')\nax.set_zlabel('company_n')\n\nscatter = ax.scatter(inputs_n['degree_n'],\n           inputs_n['job_n'],\n           inputs_n['company_n'],\n           c=target,\n           cmap='viridis',\n           marker='+')\n\n# Adding a color bar to show the mapping of colors to values in 'color_column'\ncbar = fig.colorbar(scatter, ax=ax)\ncbar.set_label('Color Column')\n\n\n\n\n\n\n\n\n\ntarget\n\n0     0\n1     0\n2     1\n3     1\n4     0\n5     1\n6     0\n7     0\n8     0\n9     1\n10    1\n11    1\n12    1\n13    1\n14    1\n15    1\nName: salary_more_then_100k, dtype: int64\n\n\n\nfrom sklearn import tree\n\n\nmodel = tree.DecisionTreeClassifier()\nmodel.fit(inputs_n, target)\n\nDecisionTreeClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifierDecisionTreeClassifier()\n\n\n\nmodel.score(inputs_n,target)\n\n1.0\n\n\n\nfrom sklearn.tree import export_graphviz\n\n\nFEATURE_NAMES = ['company_n', 'job_n', 'degree_n']\nexport_graphviz(model, './Data/salary.dot', feature_names = FEATURE_NAMES)\n\n\n!dot -Tpng ./Data/salary.dot -o ./Data/salary.png\n\n\nimport matplotlib.pyplot as plt\nimport cv2 as cv\n\n\nimg = cv.imread('./Data/salary.png')\nplt.figure(figsize = (20, 20))\nplt.imshow(img)",
    "crumbs": [
      "Blog",
      "Decision Tree"
    ]
  },
  {
    "objectID": "decisiontree.html#predict",
    "href": "decisiontree.html#predict",
    "title": "Decision Tree",
    "section": "Predict",
    "text": "Predict\n\nmodel.predict([[2,1,0]])\n\n/home/ben/mambaforge/envs/cfast/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n  warnings.warn(\n\n\narray([0])\n\n\n\nmodel.predict([[2,1,1]])\n\n/home/ben/mambaforge/envs/cfast/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n  warnings.warn(\n\n\narray([0])",
    "crumbs": [
      "Blog",
      "Decision Tree"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ML Methods and Techniques",
    "section": "",
    "text": "Documentation of all the popular ML models and techniques\n\n\n\n Back to top",
    "crumbs": [
      "Blog",
      "[ML Methods and Techniques](https://bthek1.github.io/ML_methods/)"
    ]
  },
  {
    "objectID": "06_L1_and_L2 Regularization.html",
    "href": "06_L1_and_L2 Regularization.html",
    "title": "L1 and L2 Regularization",
    "section": "",
    "text": "L1 and L2 Regularization\n# import libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n# Suppress Warnings for clean notebook\nimport warnings\nwarnings.filterwarnings('ignore')\nWe are going to use Melbourne House Price Dataset where we’ll predict House Predictions based on various features. #### The Dataset Link is https://www.kaggle.com/anthonypino/melbourne-housing-market\n# read dataset\ndataset_og = pd.read_csv('./Data/Melbourne_housing_FULL.csv')\ndataset_og.head()\n\n\n\n\n\n\n\n\nSuburb\nAddress\nRooms\nType\nPrice\nMethod\nSellerG\nDate\nDistance\nPostcode\n...\nBathroom\nCar\nLandsize\nBuildingArea\nYearBuilt\nCouncilArea\nLattitude\nLongtitude\nRegionname\nPropertycount\n\n\n\n\n0\nAbbotsford\n68 Studley St\n2\nh\nNaN\nSS\nJellis\n3/09/2016\n2.5\n3067.0\n...\n1.0\n1.0\n126.0\nNaN\nNaN\nYarra City Council\n-37.8014\n144.9958\nNorthern Metropolitan\n4019.0\n\n\n1\nAbbotsford\n85 Turner St\n2\nh\n1480000.0\nS\nBiggin\n3/12/2016\n2.5\n3067.0\n...\n1.0\n1.0\n202.0\nNaN\nNaN\nYarra City Council\n-37.7996\n144.9984\nNorthern Metropolitan\n4019.0\n\n\n2\nAbbotsford\n25 Bloomburg St\n2\nh\n1035000.0\nS\nBiggin\n4/02/2016\n2.5\n3067.0\n...\n1.0\n0.0\n156.0\n79.0\n1900.0\nYarra City Council\n-37.8079\n144.9934\nNorthern Metropolitan\n4019.0\n\n\n3\nAbbotsford\n18/659 Victoria St\n3\nu\nNaN\nVB\nRounds\n4/02/2016\n2.5\n3067.0\n...\n2.0\n1.0\n0.0\nNaN\nNaN\nYarra City Council\n-37.8114\n145.0116\nNorthern Metropolitan\n4019.0\n\n\n4\nAbbotsford\n5 Charles St\n3\nh\n1465000.0\nSP\nBiggin\n4/03/2017\n2.5\n3067.0\n...\n2.0\n0.0\n134.0\n150.0\n1900.0\nYarra City Council\n-37.8093\n144.9944\nNorthern Metropolitan\n4019.0\n\n\n\n\n5 rows × 21 columns\ndataset_og.nunique()\n\nSuburb             351\nAddress          34009\nRooms               12\nType                 3\nPrice             2871\nMethod               9\nSellerG            388\nDate                78\nDistance           215\nPostcode           211\nBedroom2            15\nBathroom            11\nCar                 15\nLandsize          1684\nBuildingArea       740\nYearBuilt          160\nCouncilArea         33\nLattitude        13402\nLongtitude       14524\nRegionname           8\nPropertycount      342\ndtype: int64\n# let's use limited columns which makes more sense for serving our purpose\ncols_to_use = ['Suburb', 'Rooms', 'Type', 'Method', 'SellerG', 'Regionname', 'Propertycount', \n               'Distance', 'CouncilArea', 'Bedroom2', 'Bathroom', 'Car', 'Landsize', 'BuildingArea', 'Price']\ndataset = dataset_og[cols_to_use]\ndataset.head()\n\n\n\n\n\n\n\n\nSuburb\nRooms\nType\nMethod\nSellerG\nRegionname\nPropertycount\nDistance\nCouncilArea\nBedroom2\nBathroom\nCar\nLandsize\nBuildingArea\nPrice\n\n\n\n\n0\nAbbotsford\n2\nh\nSS\nJellis\nNorthern Metropolitan\n4019.0\n2.5\nYarra City Council\n2.0\n1.0\n1.0\n126.0\nNaN\nNaN\n\n\n1\nAbbotsford\n2\nh\nS\nBiggin\nNorthern Metropolitan\n4019.0\n2.5\nYarra City Council\n2.0\n1.0\n1.0\n202.0\nNaN\n1480000.0\n\n\n2\nAbbotsford\n2\nh\nS\nBiggin\nNorthern Metropolitan\n4019.0\n2.5\nYarra City Council\n2.0\n1.0\n0.0\n156.0\n79.0\n1035000.0\n\n\n3\nAbbotsford\n3\nu\nVB\nRounds\nNorthern Metropolitan\n4019.0\n2.5\nYarra City Council\n3.0\n2.0\n1.0\n0.0\nNaN\nNaN\n\n\n4\nAbbotsford\n3\nh\nSP\nBiggin\nNorthern Metropolitan\n4019.0\n2.5\nYarra City Council\n3.0\n2.0\n0.0\n134.0\n150.0\n1465000.0\ndataset.shape\n\n(34857, 15)",
    "crumbs": [
      "Blog",
      "L1 and L2 Regularization"
    ]
  },
  {
    "objectID": "06_L1_and_L2 Regularization.html#normal-regression-is-clearly-overfitting-the-data-lets-try-other-models",
    "href": "06_L1_and_L2 Regularization.html#normal-regression-is-clearly-overfitting-the-data-lets-try-other-models",
    "title": "L1 and L2 Regularization",
    "section": "Normal Regression is clearly overfitting the data, let’s try other models",
    "text": "Normal Regression is clearly overfitting the data, let’s try other models\n\nUsing Lasso (L1 Regularized) Regression Model\n\nfrom sklearn.linear_model import Lasso\nlasso_reg = Lasso(alpha=50, max_iter=100, tol=0.1)\nlasso_reg.fit(train_X, train_y)\n\nLasso(alpha=50, max_iter=100, tol=0.1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LassoLasso(alpha=50, max_iter=100, tol=0.1)\n\n\n\nlasso_reg.score(test_X, test_y)\n\n0.6636111369404488\n\n\n\nlasso_reg.score(train_X, train_y)\n\n0.6766985624766824\n\n\n\nUsing Ridge (L2 Regularized) Regression Model\n\nfrom sklearn.linear_model import Ridge\nridge_reg= Ridge(alpha=50, max_iter=100, tol=0.1)\nridge_reg.fit(train_X, train_y)\n\nRidge(alpha=50, max_iter=100, tol=0.1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RidgeRidge(alpha=50, max_iter=100, tol=0.1)\n\n\n\nridge_reg.score(test_X, test_y)\n\n0.6670848945194958\n\n\n\nridge_reg.score(train_X, train_y)\n\n0.6622376739684328\n\n\nWe see that Lasso and Ridge Regularizations prove to be beneficial when our Simple Linear Regression Model overfits. These results may not be that contrast but significant in most cases.Also that L1 & L2 Regularizations are used in Neural Networks too",
    "crumbs": [
      "Blog",
      "L1 and L2 Regularization"
    ]
  },
  {
    "objectID": "pca_tut.html",
    "href": "pca_tut.html",
    "title": "PCA Tutorial",
    "section": "",
    "text": "import pandas as pd\n\n# https://www.kaggle.com/fedesoriano/heart-failure-prediction\ndf = pd.read_csv(\"Data/heart.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nAge\nSex\nChestPainType\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nHeartDisease\n\n\n\n\n0\n40\nM\nATA\n140\n289\n0\nNormal\n172\nN\n0.0\nUp\n0\n\n\n1\n49\nF\nNAP\n160\n180\n0\nNormal\n156\nN\n1.0\nFlat\n1\n\n\n2\n37\nM\nATA\n130\n283\n0\nST\n98\nN\n0.0\nUp\n0\n\n\n3\n48\nF\nASY\n138\n214\n0\nNormal\n108\nY\n1.5\nFlat\n1\n\n\n4\n54\nM\nNAP\n150\n195\n0\nNormal\n122\nN\n0.0\nUp\n0\n\n\n\n\n\n\n\n\ndf.shape\n\n(918, 12)\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nAge\nRestingBP\nCholesterol\nFastingBS\nMaxHR\nOldpeak\nHeartDisease\n\n\n\n\ncount\n918.000000\n918.000000\n918.000000\n918.000000\n918.000000\n918.000000\n918.000000\n\n\nmean\n53.510893\n132.396514\n198.799564\n0.233115\n136.809368\n0.887364\n0.553377\n\n\nstd\n9.432617\n18.514154\n109.384145\n0.423046\n25.460334\n1.066570\n0.497414\n\n\nmin\n28.000000\n0.000000\n0.000000\n0.000000\n60.000000\n-2.600000\n0.000000\n\n\n25%\n47.000000\n120.000000\n173.250000\n0.000000\n120.000000\n0.000000\n0.000000\n\n\n50%\n54.000000\n130.000000\n223.000000\n0.000000\n138.000000\n0.600000\n1.000000\n\n\n75%\n60.000000\n140.000000\n267.000000\n0.000000\n156.000000\n1.500000\n1.000000\n\n\nmax\n77.000000\n200.000000\n603.000000\n1.000000\n202.000000\n6.200000\n1.000000\n\n\n\n\n\n\n\n\nTreat Outliers\n\n\ndf[df.Cholesterol&gt;(df.Cholesterol.mean()+3*df.Cholesterol.std())]\n\n\n\n\n\n\n\n\nAge\nSex\nChestPainType\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nHeartDisease\n\n\n\n\n76\n32\nM\nASY\n118\n529\n0\nNormal\n130\nN\n0.0\nFlat\n1\n\n\n149\n54\nM\nASY\n130\n603\n1\nNormal\n125\nY\n1.0\nFlat\n1\n\n\n616\n67\nF\nNAP\n115\n564\n0\nLVH\n160\nN\n1.6\nFlat\n0\n\n\n\n\n\n\n\n\ndf.shape\n\n(918, 12)\n\n\n\ndf1 = df[df.Cholesterol&lt;=(df.Cholesterol.mean()+3*df.Cholesterol.std())]\ndf1.shape\n\n(915, 12)\n\n\n\ndf[df.MaxHR&gt;(df.MaxHR.mean()+3*df.MaxHR.std())]\n\n\n\n\n\n\n\n\nAge\nSex\nChestPainType\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nHeartDisease\n\n\n\n\n\n\n\n\n\n\ndf[df.FastingBS&gt;(df.FastingBS.mean()+3*df.FastingBS.std())]\n\n\n\n\n\n\n\n\nAge\nSex\nChestPainType\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nHeartDisease\n\n\n\n\n\n\n\n\n\n\ndf[df.Oldpeak&gt;(df.Oldpeak.mean()+3*df.Oldpeak.std())]\n\n\n\n\n\n\n\n\nAge\nSex\nChestPainType\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nHeartDisease\n\n\n\n\n166\n50\nM\nASY\n140\n231\n0\nST\n140\nY\n5.0\nFlat\n1\n\n\n702\n59\nM\nTA\n178\n270\n0\nLVH\n145\nN\n4.2\nDown\n0\n\n\n771\n55\nM\nASY\n140\n217\n0\nNormal\n111\nY\n5.6\nDown\n1\n\n\n791\n51\nM\nASY\n140\n298\n0\nNormal\n122\nY\n4.2\nFlat\n1\n\n\n850\n62\nF\nASY\n160\n164\n0\nLVH\n145\nN\n6.2\nDown\n1\n\n\n900\n58\nM\nASY\n114\n318\n0\nST\n140\nN\n4.4\nDown\n1\n\n\n\n\n\n\n\n\ndf2 = df1[df1.Oldpeak&lt;=(df1.Oldpeak.mean()+3*df1.Oldpeak.std())]\ndf2.shape\n\n(909, 12)\n\n\n\ndf[df.RestingBP&gt;(df.RestingBP.mean()+3*df.RestingBP.std())]\n\n\n\n\n\n\n\n\nAge\nSex\nChestPainType\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nHeartDisease\n\n\n\n\n109\n39\nM\nATA\n190\n241\n0\nNormal\n106\nN\n0.0\nUp\n0\n\n\n241\n54\nM\nASY\n200\n198\n0\nNormal\n142\nY\n2.0\nFlat\n1\n\n\n365\n64\nF\nASY\n200\n0\n0\nNormal\n140\nY\n1.0\nFlat\n1\n\n\n399\n61\nM\nNAP\n200\n0\n1\nST\n70\nN\n0.0\nFlat\n1\n\n\n592\n61\nM\nASY\n190\n287\n1\nLVH\n150\nY\n2.0\nDown\n1\n\n\n732\n56\nF\nASY\n200\n288\n1\nLVH\n133\nY\n4.0\nDown\n1\n\n\n759\n54\nM\nATA\n192\n283\n0\nLVH\n195\nN\n0.0\nUp\n1\n\n\n\n\n\n\n\n\ndf3 = df2[df2.RestingBP&lt;=(df2.RestingBP.mean()+3*df2.RestingBP.std())]\ndf3.shape\n\n(902, 12)\n\n\n\ndf.ChestPainType.unique()\n\narray(['ATA', 'NAP', 'ASY', 'TA'], dtype=object)\n\n\n\ndf.RestingECG.unique()\n\narray(['Normal', 'ST', 'LVH'], dtype=object)\n\n\n\ndf.ExerciseAngina.unique()\n\narray(['N', 'Y'], dtype=object)\n\n\n\ndf.ST_Slope.unique()\n\narray(['Up', 'Flat', 'Down'], dtype=object)\n\n\n\ndf4 = df3.copy()\ndf4.ExerciseAngina.replace(\n    {\n        'N': 0,\n        'Y': 1\n    },\n    inplace=True)\n\ndf4.ST_Slope.replace(\n    {\n        'Down': 1,\n        'Flat': 2,\n        'Up': 3\n    },\n    inplace=True\n)\n\ndf4.RestingECG.replace(\n    {\n        'Normal': 1,\n        'ST': 2,\n        'LVH': 3\n    },\n    inplace=True)\n\ndf4.head()\n\n\n\n\n\n\n\n\nAge\nSex\nChestPainType\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nHeartDisease\n\n\n\n\n0\n40\nM\nATA\n140\n289\n0\n1\n172\n0\n0.0\n3\n0\n\n\n1\n49\nF\nNAP\n160\n180\n0\n1\n156\n0\n1.0\n2\n1\n\n\n2\n37\nM\nATA\n130\n283\n0\n2\n98\n0\n0.0\n3\n0\n\n\n3\n48\nF\nASY\n138\n214\n0\n1\n108\n1\n1.5\n2\n1\n\n\n4\n54\nM\nNAP\n150\n195\n0\n1\n122\n0\n0.0\n3\n0\n\n\n\n\n\n\n\n\ndf5 = pd.get_dummies(df4, drop_first=True)\ndf5.head()\n\n\n\n\n\n\n\n\nAge\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nHeartDisease\nSex_M\nChestPainType_ATA\nChestPainType_NAP\nChestPainType_TA\n\n\n\n\n0\n40\n140\n289\n0\n1\n172\n0\n0.0\n3\n0\n1\n1\n0\n0\n\n\n1\n49\n160\n180\n0\n1\n156\n0\n1.0\n2\n1\n0\n0\n1\n0\n\n\n2\n37\n130\n283\n0\n2\n98\n0\n0.0\n3\n0\n1\n1\n0\n0\n\n\n3\n48\n138\n214\n0\n1\n108\n1\n1.5\n2\n1\n0\n0\n0\n0\n\n\n4\n54\n150\n195\n0\n1\n122\n0\n0.0\n3\n0\n1\n0\n1\n0\n\n\n\n\n\n\n\n\nX = df5.drop(\"HeartDisease\",axis='columns')\ny = df5.HeartDisease\n\nX.head()\n\n\n\n\n\n\n\n\nAge\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nSex_M\nChestPainType_ATA\nChestPainType_NAP\nChestPainType_TA\n\n\n\n\n0\n40\n140\n289\n0\n1\n172\n0\n0.0\n3\n1\n1\n0\n0\n\n\n1\n49\n160\n180\n0\n1\n156\n0\n1.0\n2\n0\n0\n1\n0\n\n\n2\n37\n130\n283\n0\n2\n98\n0\n0.0\n3\n1\n1\n0\n0\n\n\n3\n48\n138\n214\n0\n1\n108\n1\n1.5\n2\n0\n0\n0\n0\n\n\n4\n54\n150\n195\n0\n1\n122\n0\n0.0\n3\n1\n0\n1\n0\n\n\n\n\n\n\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nX_scaled\n\narray([[-1.42896269,  0.46089071,  0.85238015, ...,  2.06757196,\n        -0.53547478, -0.22914788],\n       [-0.47545956,  1.5925728 , -0.16132855, ..., -0.4836591 ,\n         1.86750159, -0.22914788],\n       [-1.74679706, -0.10495034,  0.79657967, ...,  2.06757196,\n        -0.53547478, -0.22914788],\n       ...,\n       [ 0.37209878, -0.10495034, -0.61703246, ..., -0.4836591 ,\n        -0.53547478, -0.22914788],\n       [ 0.37209878, -0.10495034,  0.35947592, ...,  2.06757196,\n        -0.53547478, -0.22914788],\n       [-1.64085227,  0.3477225 , -0.20782894, ..., -0.4836591 ,\n         1.86750159, -0.22914788]])\n\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=30)\n\n\nX_train.shape\n\n(721, 13)\n\n\n\nX_test.shape\n\n(181, 13)\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel_rf = RandomForestClassifier()\nmodel_rf.fit(X_train, y_train)\nmodel_rf.score(X_test, y_test)\n\n0.8674033149171271\n\n\n\nUse PCA to reduce dimensions\n\n\nX\n\n\n\n\n\n\n\n\nAge\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nSex_M\nChestPainType_ATA\nChestPainType_NAP\nChestPainType_TA\n\n\n\n\n0\n40\n140\n289\n0\n1\n172\n0\n0.0\n3\n1\n1\n0\n0\n\n\n1\n49\n160\n180\n0\n1\n156\n0\n1.0\n2\n0\n0\n1\n0\n\n\n2\n37\n130\n283\n0\n2\n98\n0\n0.0\n3\n1\n1\n0\n0\n\n\n3\n48\n138\n214\n0\n1\n108\n1\n1.5\n2\n0\n0\n0\n0\n\n\n4\n54\n150\n195\n0\n1\n122\n0\n0.0\n3\n1\n0\n1\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n913\n45\n110\n264\n0\n1\n132\n0\n1.2\n2\n1\n0\n0\n1\n\n\n914\n68\n144\n193\n1\n1\n141\n0\n3.4\n2\n1\n0\n0\n0\n\n\n915\n57\n130\n131\n0\n1\n115\n1\n1.2\n2\n1\n0\n0\n0\n\n\n916\n57\n130\n236\n0\n3\n174\n0\n0.0\n2\n0\n1\n0\n0\n\n\n917\n38\n138\n175\n0\n1\n173\n0\n0.0\n3\n1\n0\n1\n0\n\n\n\n\n902 rows × 13 columns\n\n\n\n\nfrom sklearn.decomposition import PCA\n\npca = PCA(0.95)\nX_pca = pca.fit_transform(X)\nX_pca\n\narray([[ 93.82465373, -29.40099458],\n       [-15.58422331, -14.10909233],\n       [ 83.29606634,  38.6867453 ],\n       ...,\n       [-67.57318721,  17.61319354],\n       [ 40.70458237, -33.38750602],\n       [-19.91368346, -37.29085722]])\n\n\n\nX_train_pca, X_test_pca, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=30)\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel_rf = RandomForestClassifier()\nmodel_rf.fit(X_train_pca, y_train)\nmodel_rf.score(X_test_pca, y_test)\n\n0.7182320441988951\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Blog",
      "PCA Tutorial"
    ]
  },
  {
    "objectID": "backpropagation.html",
    "href": "backpropagation.html",
    "title": "Back Propagation",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Blog",
      "Back Propagation"
    ]
  },
  {
    "objectID": "random_forest.html",
    "href": "random_forest.html",
    "title": "Random Forest",
    "section": "",
    "text": "Digits dataset from sklearn\n\nimport pandas as pd\nfrom sklearn.datasets import load_digits\ndigits = load_digits()\n\n\ndir(digits)\n\n['DESCR', 'data', 'feature_names', 'frame', 'images', 'target', 'target_names']\n\n\n\nimport matplotlib.pyplot as plt\n\n\nplt.gray() \nfor i in range(4):\n    plt.matshow(digits.images[i])\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf = pd.DataFrame(digits.data)\ndf.head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n\n\n\n\n0\n0.0\n0.0\n5.0\n13.0\n9.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n6.0\n13.0\n10.0\n0.0\n0.0\n0.0\n\n\n1\n0.0\n0.0\n0.0\n12.0\n13.0\n5.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n11.0\n16.0\n10.0\n0.0\n0.0\n\n\n2\n0.0\n0.0\n0.0\n4.0\n15.0\n12.0\n0.0\n0.0\n0.0\n0.0\n...\n5.0\n0.0\n0.0\n0.0\n0.0\n3.0\n11.0\n16.0\n9.0\n0.0\n\n\n3\n0.0\n0.0\n7.0\n15.0\n13.0\n1.0\n0.0\n0.0\n0.0\n8.0\n...\n9.0\n0.0\n0.0\n0.0\n7.0\n13.0\n13.0\n9.0\n0.0\n0.0\n\n\n4\n0.0\n0.0\n0.0\n1.0\n11.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n2.0\n16.0\n4.0\n0.0\n0.0\n\n\n\n\n5 rows × 64 columns\n\n\n\n\ndf['target'] = digits.target\n\n\ndf[0:12]\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n55\n56\n57\n58\n59\n60\n61\n62\n63\ntarget\n\n\n\n\n0\n0.0\n0.0\n5.0\n13.0\n9.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n6.0\n13.0\n10.0\n0.0\n0.0\n0.0\n0\n\n\n1\n0.0\n0.0\n0.0\n12.0\n13.0\n5.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n11.0\n16.0\n10.0\n0.0\n0.0\n1\n\n\n2\n0.0\n0.0\n0.0\n4.0\n15.0\n12.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n3.0\n11.0\n16.0\n9.0\n0.0\n2\n\n\n3\n0.0\n0.0\n7.0\n15.0\n13.0\n1.0\n0.0\n0.0\n0.0\n8.0\n...\n0.0\n0.0\n0.0\n7.0\n13.0\n13.0\n9.0\n0.0\n0.0\n3\n\n\n4\n0.0\n0.0\n0.0\n1.0\n11.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n2.0\n16.0\n4.0\n0.0\n0.0\n4\n\n\n5\n0.0\n0.0\n12.0\n10.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n9.0\n16.0\n16.0\n10.0\n0.0\n0.0\n5\n\n\n6\n0.0\n0.0\n0.0\n12.0\n13.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n1.0\n9.0\n15.0\n11.0\n3.0\n0.0\n6\n\n\n7\n0.0\n0.0\n7.0\n8.0\n13.0\n16.0\n15.0\n1.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n13.0\n5.0\n0.0\n0.0\n0.0\n0.0\n7\n\n\n8\n0.0\n0.0\n9.0\n14.0\n8.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n11.0\n16.0\n15.0\n11.0\n1.0\n0.0\n8\n\n\n9\n0.0\n0.0\n11.0\n12.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2.0\n...\n0.0\n0.0\n0.0\n9.0\n12.0\n13.0\n3.0\n0.0\n0.0\n9\n\n\n10\n0.0\n0.0\n1.0\n9.0\n15.0\n11.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n1.0\n10.0\n13.0\n3.0\n0.0\n0.0\n0\n\n\n11\n0.0\n0.0\n0.0\n0.0\n14.0\n13.0\n1.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n1.0\n13.0\n16.0\n1.0\n0.0\n1\n\n\n\n\n12 rows × 65 columns\n\n\n\nTrain and the model and prediction\n\nX = df.drop('target',axis='columns')\ny = df.target\n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier(n_estimators=20)\nmodel.fit(X_train, y_train)\n\nRandomForestClassifier(n_estimators=20)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(n_estimators=20)\n\n\n\n?RandomForestClassifier\n\n\nInit signature:\nRandomForestClassifier(\n    n_estimators=100,\n    *,\n    criterion='gini',\n    max_depth=None,\n    min_samples_split=2,\n    min_samples_leaf=1,\n    min_weight_fraction_leaf=0.0,\n    max_features='sqrt',\n    max_leaf_nodes=None,\n    min_impurity_decrease=0.0,\n    bootstrap=True,\n    oob_score=False,\n    n_jobs=None,\n    random_state=None,\n    verbose=0,\n    warm_start=False,\n    class_weight=None,\n    ccp_alpha=0.0,\n    max_samples=None,\n)\nDocstring:     \nA random forest classifier.\nA random forest is a meta estimator that fits a number of decision tree\nclassifiers on various sub-samples of the dataset and uses averaging to\nimprove the predictive accuracy and control over-fitting.\nThe sub-sample size is controlled with the `max_samples` parameter if\n`bootstrap=True` (default), otherwise the whole dataset is used to build\neach tree.\nFor a comparison between tree-based ensemble models see the example\n:ref:`sphx_glr_auto_examples_ensemble_plot_forest_hist_grad_boosting_comparison.py`.\nRead more in the :ref:`User Guide &lt;forest&gt;`.\nParameters\n----------\nn_estimators : int, default=100\n    The number of trees in the forest.\n    .. versionchanged:: 0.22\n       The default value of ``n_estimators`` changed from 10 to 100\n       in 0.22.\ncriterion : {\"gini\", \"entropy\", \"log_loss\"}, default=\"gini\"\n    The function to measure the quality of a split. Supported criteria are\n    \"gini\" for the Gini impurity and \"log_loss\" and \"entropy\" both for the\n    Shannon information gain, see :ref:`tree_mathematical_formulation`.\n    Note: This parameter is tree-specific.\nmax_depth : int, default=None\n    The maximum depth of the tree. If None, then nodes are expanded until\n    all leaves are pure or until all leaves contain less than\n    min_samples_split samples.\nmin_samples_split : int or float, default=2\n    The minimum number of samples required to split an internal node:\n    - If int, then consider `min_samples_split` as the minimum number.\n    - If float, then `min_samples_split` is a fraction and\n      `ceil(min_samples_split * n_samples)` are the minimum\n      number of samples for each split.\n    .. versionchanged:: 0.18\n       Added float values for fractions.\nmin_samples_leaf : int or float, default=1\n    The minimum number of samples required to be at a leaf node.\n    A split point at any depth will only be considered if it leaves at\n    least ``min_samples_leaf`` training samples in each of the left and\n    right branches.  This may have the effect of smoothing the model,\n    especially in regression.\n    - If int, then consider `min_samples_leaf` as the minimum number.\n    - If float, then `min_samples_leaf` is a fraction and\n      `ceil(min_samples_leaf * n_samples)` are the minimum\n      number of samples for each node.\n    .. versionchanged:: 0.18\n       Added float values for fractions.\nmin_weight_fraction_leaf : float, default=0.0\n    The minimum weighted fraction of the sum total of weights (of all\n    the input samples) required to be at a leaf node. Samples have\n    equal weight when sample_weight is not provided.\nmax_features : {\"sqrt\", \"log2\", None}, int or float, default=\"sqrt\"\n    The number of features to consider when looking for the best split:\n    - If int, then consider `max_features` features at each split.\n    - If float, then `max_features` is a fraction and\n      `max(1, int(max_features * n_features_in_))` features are considered at each\n      split.\n    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n    - If \"log2\", then `max_features=log2(n_features)`.\n    - If None, then `max_features=n_features`.\n    .. versionchanged:: 1.1\n        The default of `max_features` changed from `\"auto\"` to `\"sqrt\"`.\n    Note: the search for a split does not stop until at least one\n    valid partition of the node samples is found, even if it requires to\n    effectively inspect more than ``max_features`` features.\nmax_leaf_nodes : int, default=None\n    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n    Best nodes are defined as relative reduction in impurity.\n    If None then unlimited number of leaf nodes.\nmin_impurity_decrease : float, default=0.0\n    A node will be split if this split induces a decrease of the impurity\n    greater than or equal to this value.\n    The weighted impurity decrease equation is the following::\n        N_t / N * (impurity - N_t_R / N_t * right_impurity\n                            - N_t_L / N_t * left_impurity)\n    where ``N`` is the total number of samples, ``N_t`` is the number of\n    samples at the current node, ``N_t_L`` is the number of samples in the\n    left child, and ``N_t_R`` is the number of samples in the right child.\n    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n    if ``sample_weight`` is passed.\n    .. versionadded:: 0.19\nbootstrap : bool, default=True\n    Whether bootstrap samples are used when building trees. If False, the\n    whole dataset is used to build each tree.\noob_score : bool or callable, default=False\n    Whether to use out-of-bag samples to estimate the generalization score.\n    By default, :func:`~sklearn.metrics.accuracy_score` is used.\n    Provide a callable with signature `metric(y_true, y_pred)` to use a\n    custom metric. Only available if `bootstrap=True`.\nn_jobs : int, default=None\n    The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n    :meth:`decision_path` and :meth:`apply` are all parallelized over the\n    trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n    context. ``-1`` means using all processors. See :term:`Glossary\n    &lt;n_jobs&gt;` for more details.\nrandom_state : int, RandomState instance or None, default=None\n    Controls both the randomness of the bootstrapping of the samples used\n    when building trees (if ``bootstrap=True``) and the sampling of the\n    features to consider when looking for the best split at each node\n    (if ``max_features &lt; n_features``).\n    See :term:`Glossary &lt;random_state&gt;` for details.\nverbose : int, default=0\n    Controls the verbosity when fitting and predicting.\nwarm_start : bool, default=False\n    When set to ``True``, reuse the solution of the previous call to fit\n    and add more estimators to the ensemble, otherwise, just fit a whole\n    new forest. See :term:`Glossary &lt;warm_start&gt;` and\n    :ref:`gradient_boosting_warm_start` for details.\nclass_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts,             default=None\n    Weights associated with classes in the form ``{class_label: weight}``.\n    If not given, all classes are supposed to have weight one. For\n    multi-output problems, a list of dicts can be provided in the same\n    order as the columns of y.\n    Note that for multioutput (including multilabel) weights should be\n    defined for each class of every column in its own dict. For example,\n    for four-class multilabel classification weights should be\n    [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n    [{1:1}, {2:5}, {3:1}, {4:1}].\n    The \"balanced\" mode uses the values of y to automatically adjust\n    weights inversely proportional to class frequencies in the input data\n    as ``n_samples / (n_classes * np.bincount(y))``\n    The \"balanced_subsample\" mode is the same as \"balanced\" except that\n    weights are computed based on the bootstrap sample for every tree\n    grown.\n    For multi-output, the weights of each column of y will be multiplied.\n    Note that these weights will be multiplied with sample_weight (passed\n    through the fit method) if sample_weight is specified.\nccp_alpha : non-negative float, default=0.0\n    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n    subtree with the largest cost complexity that is smaller than\n    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n    :ref:`minimal_cost_complexity_pruning` for details.\n    .. versionadded:: 0.22\nmax_samples : int or float, default=None\n    If bootstrap is True, the number of samples to draw from X\n    to train each base estimator.\n    - If None (default), then draw `X.shape[0]` samples.\n    - If int, then draw `max_samples` samples.\n    - If float, then draw `max(round(n_samples * max_samples), 1)` samples. Thus,\n      `max_samples` should be in the interval `(0.0, 1.0]`.\n    .. versionadded:: 0.22\nAttributes\n----------\nestimator_ : :class:`~sklearn.tree.DecisionTreeClassifier`\n    The child estimator template used to create the collection of fitted\n    sub-estimators.\n    .. versionadded:: 1.2\n       `base_estimator_` was renamed to `estimator_`.\nbase_estimator_ : DecisionTreeClassifier\n    The child estimator template used to create the collection of fitted\n    sub-estimators.\n    .. deprecated:: 1.2\n        `base_estimator_` is deprecated and will be removed in 1.4.\n        Use `estimator_` instead.\nestimators_ : list of DecisionTreeClassifier\n    The collection of fitted sub-estimators.\nclasses_ : ndarray of shape (n_classes,) or a list of such arrays\n    The classes labels (single output problem), or a list of arrays of\n    class labels (multi-output problem).\nn_classes_ : int or list\n    The number of classes (single output problem), or a list containing the\n    number of classes for each output (multi-output problem).\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n    .. versionadded:: 0.24\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n    .. versionadded:: 1.0\nn_outputs_ : int\n    The number of outputs when ``fit`` is performed.\nfeature_importances_ : ndarray of shape (n_features,)\n    The impurity-based feature importances.\n    The higher, the more important the feature.\n    The importance of a feature is computed as the (normalized)\n    total reduction of the criterion brought by that feature.  It is also\n    known as the Gini importance.\n    Warning: impurity-based feature importances can be misleading for\n    high cardinality features (many unique values). See\n    :func:`sklearn.inspection.permutation_importance` as an alternative.\noob_score_ : float\n    Score of the training dataset obtained using an out-of-bag estimate.\n    This attribute exists only when ``oob_score`` is True.\noob_decision_function_ : ndarray of shape (n_samples, n_classes) or             (n_samples, n_classes, n_outputs)\n    Decision function computed with out-of-bag estimate on the training\n    set. If n_estimators is small it might be possible that a data point\n    was never left out during the bootstrap. In this case,\n    `oob_decision_function_` might contain NaN. This attribute exists\n    only when ``oob_score`` is True.\nSee Also\n--------\nsklearn.tree.DecisionTreeClassifier : A decision tree classifier.\nsklearn.ensemble.ExtraTreesClassifier : Ensemble of extremely randomized\n    tree classifiers.\nsklearn.ensemble.HistGradientBoostingClassifier : A Histogram-based Gradient\n    Boosting Classification Tree, very fast for big datasets (n_samples &gt;=\n    10_000).\nNotes\n-----\nThe default values for the parameters controlling the size of the trees\n(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\nunpruned trees which can potentially be very large on some data sets. To\nreduce memory consumption, the complexity and size of the trees should be\ncontrolled by setting those parameter values.\nThe features are always randomly permuted at each split. Therefore,\nthe best found split may vary, even with the same training data,\n``max_features=n_features`` and ``bootstrap=False``, if the improvement\nof the criterion is identical for several splits enumerated during the\nsearch of the best split. To obtain a deterministic behaviour during\nfitting, ``random_state`` has to be fixed.\nReferences\n----------\n.. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\nExamples\n--------\n&gt;&gt;&gt; from sklearn.ensemble import RandomForestClassifier\n&gt;&gt;&gt; from sklearn.datasets import make_classification\n&gt;&gt;&gt; X, y = make_classification(n_samples=1000, n_features=4,\n...                            n_informative=2, n_redundant=0,\n...                            random_state=0, shuffle=False)\n&gt;&gt;&gt; clf = RandomForestClassifier(max_depth=2, random_state=0)\n&gt;&gt;&gt; clf.fit(X, y)\nRandomForestClassifier(...)\n&gt;&gt;&gt; print(clf.predict([[0, 0, 0, 0]]))\n[1]\nFile:           ~/mambaforge/envs/cfast/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\nType:           ABCMeta\nSubclasses:     \n\n\n\n\nmodel.score(X_test, y_test)\n\n0.9638888888888889\n\n\n\ny_predicted = model.predict(X_test)\n\nConfusion Matrix\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_predicted)\ncm\n\narray([[37,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n       [ 0, 33,  0,  0,  1,  0,  0,  1,  0,  1],\n       [ 1,  0, 32,  2,  0,  0,  0,  0,  1,  0],\n       [ 0,  0,  0, 35,  0,  0,  0,  0,  0,  0],\n       [ 0,  0,  0,  0, 29,  0,  0,  1,  0,  0],\n       [ 0,  0,  0,  0,  0, 40,  0,  0,  1,  0],\n       [ 0,  0,  0,  0,  0,  0, 33,  0,  0,  0],\n       [ 0,  0,  0,  0,  0,  0,  0, 37,  0,  0],\n       [ 0,  0,  1,  0,  0,  0,  0,  1, 35,  0],\n       [ 0,  0,  0,  1,  0,  0,  0,  0,  1, 36]])\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sn\nplt.figure(figsize=(10,7))\nsn.heatmap(cm, annot=True)\nplt.xlabel('Predicted')\nplt.ylabel('Truth')\n\nText(95.72222222222221, 0.5, 'Truth')\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Blog",
      "Random Forest"
    ]
  },
  {
    "objectID": "gaussian_mixture_models.html",
    "href": "gaussian_mixture_models.html",
    "title": "Gaussian Mixture Models",
    "section": "",
    "text": "A Gaussian (or normal) distribution is characterized by its mean (()) and covariance (()). In a (d)-dimensional space, the probability density function of a Gaussian distribution is:\n\\(p(x \\mid \\mu, \\Sigma) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma|^{1/2}} \\exp \\left( -\\frac{1}{2} (x - \\mu)^T \\Sigma^{-1} (x - \\mu) \\right)\\)",
    "crumbs": [
      "Blog",
      "Gaussian Mixture Models"
    ]
  },
  {
    "objectID": "gaussian_mixture_models.html#gaussian-distribution",
    "href": "gaussian_mixture_models.html#gaussian-distribution",
    "title": "Gaussian Mixture Models",
    "section": "",
    "text": "A Gaussian (or normal) distribution is characterized by its mean (()) and covariance (()). In a (d)-dimensional space, the probability density function of a Gaussian distribution is:\n\\(p(x \\mid \\mu, \\Sigma) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma|^{1/2}} \\exp \\left( -\\frac{1}{2} (x - \\mu)^T \\Sigma^{-1} (x - \\mu) \\right)\\)",
    "crumbs": [
      "Blog",
      "Gaussian Mixture Models"
    ]
  },
  {
    "objectID": "gaussian_mixture_models.html#mixture-models",
    "href": "gaussian_mixture_models.html#mixture-models",
    "title": "Gaussian Mixture Models",
    "section": "Mixture Models",
    "text": "Mixture Models\n\nA mixture model represents the presence of subpopulations within an overall population without requiring that an observed dataset identifies the subpopulation to which an individual observation belongs. A Gaussian Mixture Model (GMM) assumes that the data is generated from a mixture of several Gaussian distributions with unknown parameters.",
    "crumbs": [
      "Blog",
      "Gaussian Mixture Models"
    ]
  },
  {
    "objectID": "gaussian_mixture_models.html#model-definition",
    "href": "gaussian_mixture_models.html#model-definition",
    "title": "Gaussian Mixture Models",
    "section": "Model Definition",
    "text": "Model Definition\nA GMM is defined by the following components:\n\n\\(K\\): The number of Gaussian components.\n\\(\\pi_k\\): The mixing coefficient for component \\(k\\) with \\(\\sum_{k=1}^{K} \\pi_k = 1\\) and \\(\\pi_k \\geq 0\\).\n\\(\\mu_k\\): The mean vector of the (k)-th Gaussian component.\n\\(\\Sigma_k\\): The covariance matrix of the (k)-th Gaussian component.\n\nThe probability density function of a GMM is given by:\n$ p(x) = _{k=1}^{K} _k , (x _k, _k) $\nwhere \\(\\mathcal{N}(x \\mid \\mu_k, \\Sigma_k)\\) is the Gaussian density for component (k).",
    "crumbs": [
      "Blog",
      "Gaussian Mixture Models"
    ]
  },
  {
    "objectID": "gaussian_mixture_models.html#expectation-maximization-em-algorithm",
    "href": "gaussian_mixture_models.html#expectation-maximization-em-algorithm",
    "title": "Gaussian Mixture Models",
    "section": "Expectation-Maximization (EM) Algorithm",
    "text": "Expectation-Maximization (EM) Algorithm\nThe EM algorithm is used to find the parameters of the GMM that best fit the data. It consists of two main steps:\n\nExpectation (E) Step: Calculate the responsibility that each Gaussian component takes for each data point.\nMaximization (M) Step: Update the parameters (π, μ, Σ) to maximize the likelihood of the observed data given these responsibilities.",
    "crumbs": [
      "Blog",
      "Gaussian Mixture Models"
    ]
  },
  {
    "objectID": "gaussian_mixture_models.html#applications",
    "href": "gaussian_mixture_models.html#applications",
    "title": "Gaussian Mixture Models",
    "section": "Applications",
    "text": "Applications\nGMMs are widely used in:\n\nClustering: Identifying groups in data.\nAnomaly Detection: Detecting outliers as points with low probability under the model.\nDensity Estimation: Estimating the underlying distribution of data.",
    "crumbs": [
      "Blog",
      "Gaussian Mixture Models"
    ]
  },
  {
    "objectID": "gaussian_mixture_models.html#implementation",
    "href": "gaussian_mixture_models.html#implementation",
    "title": "Gaussian Mixture Models",
    "section": "Implementation",
    "text": "Implementation\n\nimport numpy as np\nfrom sklearn.mixture import GaussianMixture\nimport matplotlib.pyplot as plt\n\n# Generating sample data\nnp.random.seed(0)\nn_samples = 500\n\n# Generate random samples from two different Gaussian distributions\nX = np.vstack([np.random.multivariate_normal(mean=[-1, -1], cov=[[1, 0.5], [0.5, 1]], size=n_samples),\n               np.random.multivariate_normal(mean=[5, 5], cov=[[1, -0.5], [-0.5, 1]], size=n_samples)])\n\n# Fit the GMM model\ngmm = GaussianMixture(n_components=5, covariance_type='full')\ngmm.fit(X)\n\n# Predict the labels for the data points\nlabels = gmm.predict(X)\n\n# Plotting the results\nplt.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis')\nplt.title('Gaussian Mixture Model Clustering')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.show()",
    "crumbs": [
      "Blog",
      "Gaussian Mixture Models"
    ]
  },
  {
    "objectID": "gaussian_mixture_models.html#key-considerations",
    "href": "gaussian_mixture_models.html#key-considerations",
    "title": "Gaussian Mixture Models",
    "section": "Key Considerations",
    "text": "Key Considerations\n\nNumber of Components (K): Choosing the right number of components is crucial and can be done using methods like the Bayesian Information Criterion (BIC) or the Akaike Information Criterion (AIC).\nInitialization: The EM algorithm is sensitive to initialization. Using multiple initializations and choosing the best model can help.\nCovariance Type: GMMs allow different covariance structures (‘full’, ‘tied’, ‘diag’, ‘spherical’). The choice depends on the nature of the data.",
    "crumbs": [
      "Blog",
      "Gaussian Mixture Models"
    ]
  },
  {
    "objectID": "gaussian_mixture_models.html#advantages-and-disadvantages",
    "href": "gaussian_mixture_models.html#advantages-and-disadvantages",
    "title": "Gaussian Mixture Models",
    "section": "Advantages and Disadvantages",
    "text": "Advantages and Disadvantages\nAdvantages: - Flexibility: GMMs can model any continuous data distribution, given enough components. - Probabilistic Model: Provides a probabilistic framework that can be used for soft clustering.\nDisadvantages: - Computational Complexity: EM algorithm can be computationally expensive, especially for large datasets. - Local Optima: The EM algorithm may converge to a local optimum, making the initialization important.",
    "crumbs": [
      "Blog",
      "Gaussian Mixture Models"
    ]
  },
  {
    "objectID": "ensemble.html",
    "href": "ensemble.html",
    "title": "Ensemble Methods",
    "section": "",
    "text": "import pandas as pd\n\ndf = pd.read_csv(\"Data/diabetes.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nPregnancies\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nOutcome\n\n\n\n\n0\n6\n148\n72\n35\n0\n33.6\n0.627\n50\n1\n\n\n1\n1\n85\n66\n29\n0\n26.6\n0.351\n31\n0\n\n\n2\n8\n183\n64\n0\n0\n23.3\n0.672\n32\n1\n\n\n3\n1\n89\n66\n23\n94\n28.1\n0.167\n21\n0\n\n\n4\n0\n137\n40\n35\n168\n43.1\n2.288\n33\n1\n\n\n\n\n\n\n\n\ndf.isnull().sum()\n\nPregnancies                 0\nGlucose                     0\nBloodPressure               0\nSkinThickness               0\nInsulin                     0\nBMI                         0\nDiabetesPedigreeFunction    0\nAge                         0\nOutcome                     0\ndtype: int64\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nPregnancies\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nOutcome\n\n\n\n\ncount\n768.000000\n768.000000\n768.000000\n768.000000\n768.000000\n768.000000\n768.000000\n768.000000\n768.000000\n\n\nmean\n3.845052\n120.894531\n69.105469\n20.536458\n79.799479\n31.992578\n0.471876\n33.240885\n0.348958\n\n\nstd\n3.369578\n31.972618\n19.355807\n15.952218\n115.244002\n7.884160\n0.331329\n11.760232\n0.476951\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.078000\n21.000000\n0.000000\n\n\n25%\n1.000000\n99.000000\n62.000000\n0.000000\n0.000000\n27.300000\n0.243750\n24.000000\n0.000000\n\n\n50%\n3.000000\n117.000000\n72.000000\n23.000000\n30.500000\n32.000000\n0.372500\n29.000000\n0.000000\n\n\n75%\n6.000000\n140.250000\n80.000000\n32.000000\n127.250000\n36.600000\n0.626250\n41.000000\n1.000000\n\n\nmax\n17.000000\n199.000000\n122.000000\n99.000000\n846.000000\n67.100000\n2.420000\n81.000000\n1.000000\n\n\n\n\n\n\n\n\ndf.Outcome.value_counts()\n\n0    500\n1    268\nName: Outcome, dtype: int64\n\n\nThere is slight imbalance in our dataset but since it is not major we will not worry about it!\n\n\nX = df.drop(\"Outcome\",axis=\"columns\")\ny = df.Outcome\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nX_scaled[:3]\n\narray([[ 0.63994726,  0.84832379,  0.14964075,  0.90726993, -0.69289057,\n         0.20401277,  0.46849198,  1.4259954 ],\n       [-0.84488505, -1.12339636, -0.16054575,  0.53090156, -0.69289057,\n        -0.68442195, -0.36506078, -0.19067191],\n       [ 1.23388019,  1.94372388, -0.26394125, -1.28821221, -0.69289057,\n        -1.10325546,  0.60439732, -0.10558415]])\n\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, stratify=y, random_state=10)\n\n\nX_train.shape\n\n(576, 8)\n\n\n\nX_test.shape\n\n(192, 8)\n\n\n\ny_train.value_counts()\n\n0    375\n1    201\nName: Outcome, dtype: int64\n\n\n\n201/375\n\n0.536\n\n\n\ny_test.value_counts()\n\n0    125\n1     67\nName: Outcome, dtype: int64\n\n\n\n67/125\n\n0.536\n\n\n\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\n\nscores = cross_val_score(DecisionTreeClassifier(), X, y, cv=5)\nscores\n\narray([0.68831169, 0.68181818, 0.68831169, 0.78431373, 0.73202614])\n\n\n\nscores.mean()\n\n0.7149562855445208\n\n\n\n\nfrom sklearn.ensemble import BaggingClassifier\n\nbag_model = BaggingClassifier(\n   estimator=DecisionTreeClassifier(), \n    n_estimators=100, \n    max_samples=0.8, \n    oob_score=True,\n    random_state=0\n)\nbag_model.fit(X_train, y_train)\nbag_model.oob_score_\n\n0.7534722222222222\n\n\n\nbag_model.score(X_test, y_test)\n\n0.7760416666666666\n\n\n\nbag_model = BaggingClassifier(\n    estimator=DecisionTreeClassifier(), \n    n_estimators=100, \n    max_samples=0.8, \n    oob_score=True,\n    random_state=0\n)\nscores = cross_val_score(bag_model, X, y, cv=5)\nscores\n\narray([0.75324675, 0.72727273, 0.74675325, 0.82352941, 0.73856209])\n\n\n\nscores.mean()\n\n0.7578728461081402\n\n\nWe can see some improvement in test score with bagging classifier as compared to a standalone classifier\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nscores = cross_val_score(RandomForestClassifier(n_estimators=100), X, y, cv=5)\nscores.mean()\n\n0.772192513368984",
    "crumbs": [
      "Blog",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "ensemble.html#bagging",
    "href": "ensemble.html#bagging",
    "title": "Ensemble Methods",
    "section": "",
    "text": "import pandas as pd\n\ndf = pd.read_csv(\"Data/diabetes.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nPregnancies\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nOutcome\n\n\n\n\n0\n6\n148\n72\n35\n0\n33.6\n0.627\n50\n1\n\n\n1\n1\n85\n66\n29\n0\n26.6\n0.351\n31\n0\n\n\n2\n8\n183\n64\n0\n0\n23.3\n0.672\n32\n1\n\n\n3\n1\n89\n66\n23\n94\n28.1\n0.167\n21\n0\n\n\n4\n0\n137\n40\n35\n168\n43.1\n2.288\n33\n1\n\n\n\n\n\n\n\n\ndf.isnull().sum()\n\nPregnancies                 0\nGlucose                     0\nBloodPressure               0\nSkinThickness               0\nInsulin                     0\nBMI                         0\nDiabetesPedigreeFunction    0\nAge                         0\nOutcome                     0\ndtype: int64\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nPregnancies\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nOutcome\n\n\n\n\ncount\n768.000000\n768.000000\n768.000000\n768.000000\n768.000000\n768.000000\n768.000000\n768.000000\n768.000000\n\n\nmean\n3.845052\n120.894531\n69.105469\n20.536458\n79.799479\n31.992578\n0.471876\n33.240885\n0.348958\n\n\nstd\n3.369578\n31.972618\n19.355807\n15.952218\n115.244002\n7.884160\n0.331329\n11.760232\n0.476951\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.078000\n21.000000\n0.000000\n\n\n25%\n1.000000\n99.000000\n62.000000\n0.000000\n0.000000\n27.300000\n0.243750\n24.000000\n0.000000\n\n\n50%\n3.000000\n117.000000\n72.000000\n23.000000\n30.500000\n32.000000\n0.372500\n29.000000\n0.000000\n\n\n75%\n6.000000\n140.250000\n80.000000\n32.000000\n127.250000\n36.600000\n0.626250\n41.000000\n1.000000\n\n\nmax\n17.000000\n199.000000\n122.000000\n99.000000\n846.000000\n67.100000\n2.420000\n81.000000\n1.000000\n\n\n\n\n\n\n\n\ndf.Outcome.value_counts()\n\n0    500\n1    268\nName: Outcome, dtype: int64\n\n\nThere is slight imbalance in our dataset but since it is not major we will not worry about it!\n\n\nX = df.drop(\"Outcome\",axis=\"columns\")\ny = df.Outcome\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nX_scaled[:3]\n\narray([[ 0.63994726,  0.84832379,  0.14964075,  0.90726993, -0.69289057,\n         0.20401277,  0.46849198,  1.4259954 ],\n       [-0.84488505, -1.12339636, -0.16054575,  0.53090156, -0.69289057,\n        -0.68442195, -0.36506078, -0.19067191],\n       [ 1.23388019,  1.94372388, -0.26394125, -1.28821221, -0.69289057,\n        -1.10325546,  0.60439732, -0.10558415]])\n\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, stratify=y, random_state=10)\n\n\nX_train.shape\n\n(576, 8)\n\n\n\nX_test.shape\n\n(192, 8)\n\n\n\ny_train.value_counts()\n\n0    375\n1    201\nName: Outcome, dtype: int64\n\n\n\n201/375\n\n0.536\n\n\n\ny_test.value_counts()\n\n0    125\n1     67\nName: Outcome, dtype: int64\n\n\n\n67/125\n\n0.536\n\n\n\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\n\nscores = cross_val_score(DecisionTreeClassifier(), X, y, cv=5)\nscores\n\narray([0.68831169, 0.68181818, 0.68831169, 0.78431373, 0.73202614])\n\n\n\nscores.mean()\n\n0.7149562855445208\n\n\n\n\nfrom sklearn.ensemble import BaggingClassifier\n\nbag_model = BaggingClassifier(\n   estimator=DecisionTreeClassifier(), \n    n_estimators=100, \n    max_samples=0.8, \n    oob_score=True,\n    random_state=0\n)\nbag_model.fit(X_train, y_train)\nbag_model.oob_score_\n\n0.7534722222222222\n\n\n\nbag_model.score(X_test, y_test)\n\n0.7760416666666666\n\n\n\nbag_model = BaggingClassifier(\n    estimator=DecisionTreeClassifier(), \n    n_estimators=100, \n    max_samples=0.8, \n    oob_score=True,\n    random_state=0\n)\nscores = cross_val_score(bag_model, X, y, cv=5)\nscores\n\narray([0.75324675, 0.72727273, 0.74675325, 0.82352941, 0.73856209])\n\n\n\nscores.mean()\n\n0.7578728461081402\n\n\nWe can see some improvement in test score with bagging classifier as compared to a standalone classifier\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nscores = cross_val_score(RandomForestClassifier(n_estimators=100), X, y, cv=5)\nscores.mean()\n\n0.772192513368984",
    "crumbs": [
      "Blog",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "ensemble.html#boosting",
    "href": "ensemble.html#boosting",
    "title": "Ensemble Methods",
    "section": "Boosting",
    "text": "Boosting\n\nGradient Boosting – It is a boosting technique that builds a final model from the sum of several weak learning algorithms that were trained on the same dataset. It operates on the idea of stagewise addition. The first weak learner in the gradient boosting algorithm will not be trained on the dataset; instead, it will simply return the mean of the relevant column. The residual for the first weak learner algorithm’s output will then be calculated and used as the output column or target column for the next weak learning algorithm that will be trained. The second weak learner will be trained using the same methodology, and the residuals will be computed and utilized as an output column once more for the third weak learner, and so on until we achieve zero residuals. The dataset for gradient boosting must be in the form of numerical or categorical data, and the loss function used to generate the residuals must be differential at all times.\nXGBoost – In addition to the gradient boosting technique, XGBoost is another boosting machine learning approach. The full name of the XGBoost algorithm is the eXtreme Gradient Boosting algorithm, which is an extreme variation of the previous gradient boosting technique. The key distinction between XGBoost and GradientBoosting is that XGBoost applies a regularisation approach. It is a regularised version of the current gradient-boosting technique. Because of this, XGBoost outperforms a standard gradient boosting method, which explains why it is also faster than that. Additionally, it works better when the dataset contains both numerical and categorical variables.\nAdaboost – AdaBoost is a boosting algorithm that also works on the principle of the stagewise addition method where multiple weak learners are used for getting strong learners. The value of the alpha parameter, in this case, will be indirectly proportional to the error of the weak learner, Unlike Gradient Boosting in XGBoost, the alpha parameter calculated is related to the errors of the weak learner, here the value of the alpha parameter will be indirectly proportional to the error of the weak learner.\nCatBoost – The growth of decision trees inside CatBoost is the primary distinction that sets it apart from and improves upon competitors. The decision trees that are created in CatBoost are symmetric. As there is a unique sort of approach for handling categorical datasets, CatBoost works very well on categorical datasets compared to any other algorithm in the field of machine learning. The categorical features in CatBoost are encoded based on the output columns. As a result, the output column’s weight will be taken into account while training or encoding the categorical features, increasing its accuracy on categorical datasets.\n\n\n# for classification\nfrom sklearn.ensemble import GradientBoostingClassifier\nmodel = GradientBoostingClassifier(n_estimators=100)\nmodel.fit(X_train, y_train)\n\nGradientBoostingClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GradientBoostingClassifierGradientBoostingClassifier()\n\n\n\nmodel.score(X_test, y_test)\n\n0.796875\n\n\n\nfrom sklearn.ensemble import AdaBoostClassifier\n\n\nclf = AdaBoostClassifier(\n    n_estimators=100,\n    random_state=0,\n    algorithm='SAMME')\n\n\nclf.fit(X_train, y_train)\n\nAdaBoostClassifier(algorithm='SAMME', n_estimators=100, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.AdaBoostClassifierAdaBoostClassifier(algorithm='SAMME', n_estimators=100, random_state=0)\n\n\n\nclf.score(X_test, y_test)\n\n0.796875",
    "crumbs": [
      "Blog",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "linearregression.html",
    "href": "linearregression.html",
    "title": "Linear Regression",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\npath = Path('Data/homeprices.csv')\ndf = pd.read_csv(path)\ndf\n\n\n\n\n\n\n\n\narea\nbedrooms\nage\nprice\n\n\n\n\n0\n2600\n3.0\n20\n550000\n\n\n1\n3000\n4.0\n15\n565000\n\n\n2\n3200\nNaN\n18\n610000\n\n\n3\n3600\n3.0\n30\n595000\n\n\n4\n4000\n5.0\n8\n760000\n\n\n5\n4100\n6.0\n8\n810000\nplt.xlabel('area')\nplt.ylabel('price')\nplt.scatter(df.area,df.price,color='red',marker='+')\n\nplt.show()\nnew_df = df.drop('price',axis='columns')\nnew_df = new_df.drop('bedrooms',axis='columns')\nnew_df = new_df.drop('age',axis='columns')\nnew_df\n\n\n\n\n\n\n\n\narea\n\n\n\n\n0\n2600\n\n\n1\n3000\n\n\n2\n3200\n\n\n3\n3600\n\n\n4\n4000\n\n\n5\n4100\nprice = df.price\nprice\n\n0    550000\n1    565000\n2    610000\n3    595000\n4    760000\n5    810000\nName: price, dtype: int64\nfrom sklearn import linear_model\n# Create linear regression object\nreg = linear_model.LinearRegression()\nreg.fit(new_df,price)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\nreg.predict([[3300]])\n\n/home/ben/mambaforge/envs/cfast/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n  warnings.warn(\n\n\narray([628813.88621022])\nreg.coef_\n\narray([167.30954677])\nreg.intercept_\n\n76692.3818707813\nY = m * X + b (m is coefficient and b is intercept)\n5000*reg.coef_ + reg.intercept_\n\narray([913240.11571842])\nreg.predict([[5000]])\n\n/home/ben/mambaforge/envs/cfast/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n  warnings.warn(\n\n\narray([913240.11571842])",
    "crumbs": [
      "Blog",
      "Linear Regression"
    ]
  },
  {
    "objectID": "linearregression.html#generate-csv-file-with-list-of-home-price-predictions",
    "href": "linearregression.html#generate-csv-file-with-list-of-home-price-predictions",
    "title": "Linear Regression",
    "section": "Generate CSV file with list of home price predictions",
    "text": "Generate CSV file with list of home price predictions",
    "crumbs": [
      "Blog",
      "Linear Regression"
    ]
  },
  {
    "objectID": "train_test_split.html",
    "href": "train_test_split.html",
    "title": "Training And Testing",
    "section": "",
    "text": "We have a dataset containing prices of used BMW cars. We are going to analyze this dataset and build a prediction function that can predict a price by taking mileage and age of the car as input. We will use sklearn train_test_split method to split training and testing dataset\n\n\nimport pandas as pd\ndf = pd.read_csv(\"Data/carprices.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nMileage\nAge(yrs)\nSell Price($)\n\n\n\n\n0\n69000\n6\n18000\n\n\n1\n35000\n3\n34000\n\n\n2\n57000\n5\n26100\n\n\n3\n22500\n2\n40000\n\n\n4\n46000\n4\n31500\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\nCar Mileage Vs Sell Price ($)\n\nplt.scatter(df['Mileage'],df['Sell Price($)'])\n\n\n\n\n\n\n\n\nCar Age Vs Sell Price ($)\n\nplt.scatter(df['Age(yrs)'],df['Sell Price($)'])\n\n\n\n\n\n\n\n\nLooking at above two scatter plots, using linear regression model makes sense as we can clearly see a linear relationship between our dependant (i.e. Sell Price) and independant variables (i.e. car age and car mileage)\n\nThe approach we are going to use here is to split available data in two sets\n\n&lt;ol&gt;\n    &lt;b&gt;\n    &lt;li&gt;Training: We will train our model on this dataset&lt;/li&gt;\n    &lt;li&gt;Testing: We will use this subset to make actual predictions using trained model&lt;/li&gt;\n    &lt;/b&gt;\n &lt;/ol&gt;\n\nThe reason we don’t use same training set for testing is because our model has seen those samples before, using same samples for making predictions might give us wrong impression about accuracy of our model. It is like you ask same questions in exam paper as you tought the students in the class. \n\n\nX = df[['Mileage','Age(yrs)']]\n\n\ny = df['Sell Price($)']\n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)\n\n\nX_train\n\n\n\n\n\n\n\n\nMileage\nAge(yrs)\n\n\n\n\n4\n46000\n4\n\n\n13\n58780\n4\n\n\n5\n59000\n5\n\n\n7\n72000\n6\n\n\n12\n59000\n5\n\n\n11\n79000\n7\n\n\n3\n22500\n2\n\n\n9\n67000\n6\n\n\n0\n69000\n6\n\n\n8\n91000\n8\n\n\n14\n82450\n7\n\n\n2\n57000\n5\n\n\n6\n52000\n5\n\n\n15\n25400\n3\n\n\n\n\n\n\n\n\nX_test\n\n\n\n\n\n\n\n\nMileage\nAge(yrs)\n\n\n\n\n1\n35000\n3\n\n\n10\n83000\n7\n\n\n17\n69000\n5\n\n\n16\n28000\n2\n\n\n18\n87600\n8\n\n\n19\n52000\n5\n\n\n\n\n\n\n\n\ny_train\n\n4     31500\n13    27500\n5     26750\n7     19300\n12    26000\n11    19500\n3     40000\n9     22000\n0     18000\n8     12000\n14    19400\n2     26100\n6     32000\n15    35000\nName: Sell Price($), dtype: int64\n\n\n\ny_test\n\n1     34000\n10    18700\n17    19700\n16    35500\n18    12800\n19    28200\nName: Sell Price($), dtype: int64\n\n\nLets run linear regression model now\n\nfrom sklearn.linear_model import LinearRegression\nclf = LinearRegression()\nclf.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nX_test\n\n\n\n\n\n\n\n\nMileage\nAge(yrs)\n\n\n\n\n1\n35000\n3\n\n\n10\n83000\n7\n\n\n17\n69000\n5\n\n\n16\n28000\n2\n\n\n18\n87600\n8\n\n\n19\n52000\n5\n\n\n\n\n\n\n\n\nclf.predict(X_test)\n\narray([34944.44210911, 16905.69832696, 23351.64782019, 38167.41685573,\n       14300.34495583, 27726.46589653])\n\n\n\ny_results = pd.DataFrame(clf.predict(X_test))\ny_results\n\n\n\n\n\n\n\n\n0\n\n\n\n\n0\n34944.442109\n\n\n1\n16905.698327\n\n\n2\n23351.647820\n\n\n3\n38167.416856\n\n\n4\n14300.344956\n\n\n5\n27726.465897\n\n\n\n\n\n\n\n\ny_res = y_results[0].sort_values(ascending=True).reset_index()\ny_res.drop('index', axis = 1, inplace=True)\ny_res\n\n\n\n\n\n\n\n\n0\n\n\n\n\n0\n14300.344956\n\n\n1\n16905.698327\n\n\n2\n23351.647820\n\n\n3\n27726.465897\n\n\n4\n34944.442109\n\n\n5\n38167.416856\n\n\n\n\n\n\n\n\ny_test\n\n1     34000\n10    18700\n17    19700\n16    35500\n18    12800\n19    28200\nName: Sell Price($), dtype: int64\n\n\n\ny_t = y_test.sort_values(ascending=True).reset_index()\ny_t\n\n\n\n\n\n\n\n\nindex\nSell Price($)\n\n\n\n\n0\n18\n12800\n\n\n1\n10\n18700\n\n\n2\n17\n19700\n\n\n3\n19\n28200\n\n\n4\n1\n34000\n\n\n5\n16\n35500\n\n\n\n\n\n\n\n\ny_t.drop('index', axis = 1, inplace=True)\ny_t\n\n\n\n\n\n\n\n\nSell Price($)\n\n\n\n\n0\n12800\n\n\n1\n18700\n\n\n2\n19700\n\n\n3\n28200\n\n\n4\n34000\n\n\n5\n35500\n\n\n\n\n\n\n\n\nplt.plot(y_t)\nplt.plot(y_res)\n\n\n\n\n\n\n\n\n\nclf.score(X_test, y_test)\n\n0.9353054216597069\n\n\nrandom_state argument\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=10)\nX_test\n\n\n\n\n\n\n\n\nMileage\nAge(yrs)\n\n\n\n\n7\n72000\n6\n\n\n10\n83000\n7\n\n\n5\n59000\n5\n\n\n6\n52000\n5\n\n\n3\n22500\n2\n\n\n18\n87600\n8\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Blog",
      "Training And Testing"
    ]
  },
  {
    "objectID": "svm.html",
    "href": "svm.html",
    "title": "Support Vector Regression",
    "section": "",
    "text": "import pandas as pd\nfrom sklearn.datasets import load_iris\niris = load_iris()\n\n\niris.feature_names\n\n['sepal length (cm)',\n 'sepal width (cm)',\n 'petal length (cm)',\n 'petal width (cm)']\n\n\n\niris.target_names\n\narray(['setosa', 'versicolor', 'virginica'], dtype='&lt;U10')\n\n\n\ndf = pd.DataFrame(iris.data,columns=iris.feature_names)\ndf.head()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n4.9\n3.0\n1.4\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\n\n\n3\n4.6\n3.1\n1.5\n0.2\n\n\n4\n5.0\n3.6\n1.4\n0.2\n\n\n\n\n\n\n\n\ndf['target'] = iris.target\ndf.head()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n0\n\n\n1\n4.9\n3.0\n1.4\n0.2\n0\n\n\n2\n4.7\n3.2\n1.3\n0.2\n0\n\n\n3\n4.6\n3.1\n1.5\n0.2\n0\n\n\n4\n5.0\n3.6\n1.4\n0.2\n0\n\n\n\n\n\n\n\n\ndf[df.target==1].head()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\n50\n7.0\n3.2\n4.7\n1.4\n1\n\n\n51\n6.4\n3.2\n4.5\n1.5\n1\n\n\n52\n6.9\n3.1\n4.9\n1.5\n1\n\n\n53\n5.5\n2.3\n4.0\n1.3\n1\n\n\n54\n6.5\n2.8\n4.6\n1.5\n1\n\n\n\n\n\n\n\n\ndf[df.target==2].head()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\n100\n6.3\n3.3\n6.0\n2.5\n2\n\n\n101\n5.8\n2.7\n5.1\n1.9\n2\n\n\n102\n7.1\n3.0\n5.9\n2.1\n2\n\n\n103\n6.3\n2.9\n5.6\n1.8\n2\n\n\n104\n6.5\n3.0\n5.8\n2.2\n2\n\n\n\n\n\n\n\n\ndf['flower_name'] =df.target.apply(lambda x: iris.target_names[x])\ndf.head()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\nflower_name\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n0\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\n0\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\n0\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\n0\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\n0\nsetosa\n\n\n\n\n\n\n\n\ndf[45:55]\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\nflower_name\n\n\n\n\n45\n4.8\n3.0\n1.4\n0.3\n0\nsetosa\n\n\n46\n5.1\n3.8\n1.6\n0.2\n0\nsetosa\n\n\n47\n4.6\n3.2\n1.4\n0.2\n0\nsetosa\n\n\n48\n5.3\n3.7\n1.5\n0.2\n0\nsetosa\n\n\n49\n5.0\n3.3\n1.4\n0.2\n0\nsetosa\n\n\n50\n7.0\n3.2\n4.7\n1.4\n1\nversicolor\n\n\n51\n6.4\n3.2\n4.5\n1.5\n1\nversicolor\n\n\n52\n6.9\n3.1\n4.9\n1.5\n1\nversicolor\n\n\n53\n5.5\n2.3\n4.0\n1.3\n1\nversicolor\n\n\n54\n6.5\n2.8\n4.6\n1.5\n1\nversicolor\n\n\n\n\n\n\n\n\ndf0 = df[:50]\ndf1 = df[50:100]\ndf2 = df[100:]\n\n\nimport matplotlib.pyplot as plt\n\nSepal length vs Sepal Width (Setosa vs Versicolor)\n\nplt.xlabel('Sepal Length')\nplt.ylabel('Sepal Width')\nplt.scatter(df0['sepal length (cm)'], df0['sepal width (cm)'],color=\"green\",marker='+')\nplt.scatter(df1['sepal length (cm)'], df1['sepal width (cm)'],color=\"blue\",marker='.')\n\n\n\n\n\n\n\n\nPetal length vs Pepal Width (Setosa vs Versicolor)\n\nplt.xlabel('Petal Length')\nplt.ylabel('Petal Width')\nplt.scatter(df0['petal length (cm)'], df0['petal width (cm)'],color=\"green\",marker='+')\nplt.scatter(df1['petal length (cm)'], df1['petal width (cm)'],color=\"blue\",marker='.')\n\n\n\n\n\n\n\n\nTrain Using Support Vector Machine (SVM)\n\nfrom sklearn.model_selection import train_test_split\n\n\nX = df.drop(['target','flower_name'], axis='columns')\ny = df.target\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n\nlen(X_train)\n\n120\n\n\n\nlen(X_test)\n\n30\n\n\n\nfrom sklearn.svm import SVC\nmodel = SVC()\n\n\nmodel.fit(X_train, y_train)\n\nSVC()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVCSVC()\n\n\n\nmodel.score(X_test, y_test)\n\n1.0\n\n\n\nmodel.predict([[4.8,3.0,1.5,0.3]])\n\n/home/benedict/mambaforge/envs/cfast/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but SVC was fitted with feature names\n  warnings.warn(\n\n\narray([0])\n\n\nTune parameters\n1. Regularization (C)\n\nmodel_C = SVC(C=1)\nmodel_C.fit(X_train, y_train)\nmodel_C.score(X_test, y_test)\n\n1.0\n\n\n\nmodel_C = SVC(C=10)\nmodel_C.fit(X_train, y_train)\nmodel_C.score(X_test, y_test)\n\n0.9666666666666667\n\n\n2. Gamma\n\nmodel_g = SVC(gamma=10)\nmodel_g.fit(X_train, y_train)\nmodel_g.score(X_test, y_test)\n\n0.9666666666666667\n\n\n3. Kernel\n\nmodel_linear_kernal = SVC(kernel='linear')\nmodel_linear_kernal.fit(X_train, y_train)\n\nSVC(kernel='linear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVCSVC(kernel='linear')\n\n\n\nmodel_linear_kernal.score(X_test, y_test)\n\n0.9666666666666667\n\n\nExercise\nTrain SVM classifier using sklearn digits dataset (i.e. from sklearn.datasets import load_digits) and then,\n\nMeasure accuracy of your model using different kernels such as rbf and linear.\nTune your model further using regularization and gamma parameters and try to come up with highest accurancy score\nUse 80% of samples as training data size\n\n\n\n\n Back to top",
    "crumbs": [
      "Blog",
      "Support Vector Regression"
    ]
  },
  {
    "objectID": "bagging_diabetes_prediction.html",
    "href": "bagging_diabetes_prediction.html",
    "title": "Ensemble Learning: Bagging Tutorial",
    "section": "",
    "text": "We will use pima indian diabetes dataset to predict if a person has a diabetes or not based on certain features such as blood pressure, skin thickness, age etc. We will train a standalone model first and then use bagging ensemble technique to check how it can improve the performance of the model\ndataset credit: https://www.kaggle.com/gargmanas/pima-indians-diabetes\n\nimport pandas as pd\n\ndf = pd.read_csv(\"diabetes.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nPregnancies\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nOutcome\n\n\n\n\n0\n6\n148\n72\n35\n0\n33.6\n0.627\n50\n1\n\n\n1\n1\n85\n66\n29\n0\n26.6\n0.351\n31\n0\n\n\n2\n8\n183\n64\n0\n0\n23.3\n0.672\n32\n1\n\n\n3\n1\n89\n66\n23\n94\n28.1\n0.167\n21\n0\n\n\n4\n0\n137\n40\n35\n168\n43.1\n2.288\n33\n1\n\n\n\n\n\n\n\n\ndf.isnull().sum()\n\nPregnancies                 0\nGlucose                     0\nBloodPressure               0\nSkinThickness               0\nInsulin                     0\nBMI                         0\nDiabetesPedigreeFunction    0\nAge                         0\nOutcome                     0\ndtype: int64\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nPregnancies\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nOutcome\n\n\n\n\ncount\n768.000000\n768.000000\n768.000000\n768.000000\n768.000000\n768.000000\n768.000000\n768.000000\n768.000000\n\n\nmean\n3.845052\n120.894531\n69.105469\n20.536458\n79.799479\n31.992578\n0.471876\n33.240885\n0.348958\n\n\nstd\n3.369578\n31.972618\n19.355807\n15.952218\n115.244002\n7.884160\n0.331329\n11.760232\n0.476951\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.078000\n21.000000\n0.000000\n\n\n25%\n1.000000\n99.000000\n62.000000\n0.000000\n0.000000\n27.300000\n0.243750\n24.000000\n0.000000\n\n\n50%\n3.000000\n117.000000\n72.000000\n23.000000\n30.500000\n32.000000\n0.372500\n29.000000\n0.000000\n\n\n75%\n6.000000\n140.250000\n80.000000\n32.000000\n127.250000\n36.600000\n0.626250\n41.000000\n1.000000\n\n\nmax\n17.000000\n199.000000\n122.000000\n99.000000\n846.000000\n67.100000\n2.420000\n81.000000\n1.000000\n\n\n\n\n\n\n\n\ndf.Outcome.value_counts()\n\n0    500\n1    268\nName: Outcome, dtype: int64\n\n\nThere is slight imbalance in our dataset but since it is not major we will not worry about it!\n\nTrain test split\n\n\nX = df.drop(\"Outcome\",axis=\"columns\")\ny = df.Outcome\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nX_scaled[:3]\n\narray([[ 0.63994726,  0.84832379,  0.14964075,  0.90726993, -0.69289057,\n         0.20401277,  0.46849198,  1.4259954 ],\n       [-0.84488505, -1.12339636, -0.16054575,  0.53090156, -0.69289057,\n        -0.68442195, -0.36506078, -0.19067191],\n       [ 1.23388019,  1.94372388, -0.26394125, -1.28821221, -0.69289057,\n        -1.10325546,  0.60439732, -0.10558415]])\n\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, stratify=y, random_state=10)\n\n\nX_train.shape\n\n(576, 8)\n\n\n\nX_test.shape\n\n(192, 8)\n\n\n\ny_train.value_counts()\n\n0    375\n1    201\nName: Outcome, dtype: int64\n\n\n\n201/375\n\n0.536\n\n\n\ny_test.value_counts()\n\n0    125\n1     67\nName: Outcome, dtype: int64\n\n\n\n67/125\n\n0.536\n\n\n\nTrain using stand alone model\n\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\n\nscores = cross_val_score(DecisionTreeClassifier(), X, y, cv=5)\nscores\n\narray([0.68831169, 0.68181818, 0.69480519, 0.77777778, 0.71895425])\n\n\n\nscores.mean()\n\n0.7123334182157711\n\n\n\nTrain using Bagging\n\n\nfrom sklearn.ensemble import BaggingClassifier\n\nbag_model = BaggingClassifier(\n    base_estimator=DecisionTreeClassifier(), \n    n_estimators=100, \n    max_samples=0.8, \n    oob_score=True,\n    random_state=0\n)\nbag_model.fit(X_train, y_train)\nbag_model.oob_score_\n\n0.7534722222222222\n\n\n\nbag_model.score(X_test, y_test)\n\n0.7760416666666666\n\n\n\nbag_model = BaggingClassifier(\n    base_estimator=DecisionTreeClassifier(), \n    n_estimators=100, \n    max_samples=0.8, \n    oob_score=True,\n    random_state=0\n)\nscores = cross_val_score(bag_model, X, y, cv=5)\nscores\n\narray([0.75324675, 0.72727273, 0.74675325, 0.82352941, 0.73856209])\n\n\n\nscores.mean()\n\n0.7578728461081402\n\n\nWe can see some improvement in test score with bagging classifier as compared to a standalone classifier\n\nTrain using Random Forest\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nscores = cross_val_score(RandomForestClassifier(n_estimators=50), X, y, cv=5)\nscores.mean()\n\n0.7617689500042442\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Blog",
      "Ensemble Learning: Bagging Tutorial"
    ]
  },
  {
    "objectID": "bayesian.html",
    "href": "bayesian.html",
    "title": "The Naive Bayes Approach",
    "section": "",
    "text": "\\(P(queen/diamond) = \\dfrac{P(diamond/queen) * P(queen)}{P(diamond)}\\)\n$P(queen/diamond) = $\n\\(P(diamond/queen) = 1/4\\)\n\\(P(queen) = 1/13\\)\n\\(P(diamond) = 1/4\\)\n\\(P(queen/diamond) = \\dfrac{1/4 * 1/13}{1/4} = 1/13\\)\n\nimport pandas as pd\n\n\ndf = pd.read_csv(\"Data/titanic.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nPassengerId\nName\nPclass\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nSurvived\n\n\n\n\n0\n1\nBraund, Mr. Owen Harris\n3\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n0\n\n\n1\n2\nCumings, Mrs. John Bradley (Florence Briggs Th...\n1\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n1\n\n\n2\n3\nHeikkinen, Miss. Laina\n3\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n1\n\n\n3\n4\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n1\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n1\n\n\n4\n5\nAllen, Mr. William Henry\n3\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n0\n\n\n\n\n\n\n\n\ndf.drop(['PassengerId','Name','SibSp','Parch','Ticket','Cabin','Embarked'],axis='columns',inplace=True)\ndf.head()\n\n\n\n\n\n\n\n\nPclass\nSex\nAge\nFare\nSurvived\n\n\n\n\n0\n3\nmale\n22.0\n7.2500\n0\n\n\n1\n1\nfemale\n38.0\n71.2833\n1\n\n\n2\n3\nfemale\n26.0\n7.9250\n1\n\n\n3\n1\nfemale\n35.0\n53.1000\n1\n\n\n4\n3\nmale\n35.0\n8.0500\n0\n\n\n\n\n\n\n\n\ninputs = df.drop('Survived',axis='columns')\ntarget = df.Survived\n\n\n#inputs.Sex = inputs.Sex.map({'male': 1, 'female': 2})\n\n\ndummies = pd.get_dummies(inputs.Sex)\ndummies.head(3)\n\n\n\n\n\n\n\n\nfemale\nmale\n\n\n\n\n0\n0\n1\n\n\n1\n1\n0\n\n\n2\n1\n0\n\n\n\n\n\n\n\n\ninputs = pd.concat([inputs,dummies],axis='columns')\ninputs.head(3)\n\n\n\n\n\n\n\n\nPclass\nSex\nAge\nFare\nfemale\nmale\n\n\n\n\n0\n3\nmale\n22.0\n7.2500\n0\n1\n\n\n1\n1\nfemale\n38.0\n71.2833\n1\n0\n\n\n2\n3\nfemale\n26.0\n7.9250\n1\n0\n\n\n\n\n\n\n\nI am dropping male column as well because of dummy variable trap theory. One column is enough to repressent male vs female\n\ninputs.drop(['Sex','male'],axis='columns',inplace=True)\ninputs.head(3)\n\n\n\n\n\n\n\n\nPclass\nAge\nFare\nfemale\n\n\n\n\n0\n3\n22.0\n7.2500\n0\n\n\n1\n1\n38.0\n71.2833\n1\n\n\n2\n3\n26.0\n7.9250\n1\n\n\n\n\n\n\n\n\ninputs.columns[inputs.isna().any()]\n\nIndex(['Age'], dtype='object')\n\n\n\ninputs.Age[:10]\n\n0    22.0\n1    38.0\n2    26.0\n3    35.0\n4    35.0\n5     NaN\n6    54.0\n7     2.0\n8    27.0\n9    14.0\nName: Age, dtype: float64\n\n\n\ninputs.Age = inputs.Age.fillna(inputs.Age.mean())\ninputs.head()\n\n\n\n\n\n\n\n\nPclass\nAge\nFare\nfemale\n\n\n\n\n0\n3\n22.0\n7.2500\n0\n\n\n1\n1\n38.0\n71.2833\n1\n\n\n2\n3\n26.0\n7.9250\n1\n\n\n3\n1\n35.0\n53.1000\n1\n\n\n4\n3\n35.0\n8.0500\n0\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(inputs,target,test_size=0.3)\n\n\nfrom sklearn.naive_bayes import GaussianNB\nmodel = GaussianNB()\n\n\nmodel.fit(X_train,y_train)\n\nGaussianNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianNBGaussianNB()\n\n\n\nmodel.score(X_test,y_test)\n\n0.7910447761194029\n\n\n\nX_test[0:10]\n\n\n\n\n\n\n\n\nPclass\nAge\nFare\nfemale\n\n\n\n\n509\n3\n26.000000\n56.4958\n0\n\n\n325\n1\n36.000000\n135.6333\n1\n\n\n248\n1\n37.000000\n52.5542\n0\n\n\n391\n3\n21.000000\n7.7958\n0\n\n\n411\n3\n29.699118\n6.8583\n0\n\n\n688\n3\n18.000000\n7.7958\n0\n\n\n183\n2\n1.000000\n39.0000\n0\n\n\n14\n3\n14.000000\n7.8542\n1\n\n\n763\n1\n36.000000\n120.0000\n1\n\n\n383\n1\n35.000000\n52.0000\n1\n\n\n\n\n\n\n\n\ny_test[0:10]\n\n509    1\n325    1\n248    1\n391    1\n411    0\n688    0\n183    1\n14     0\n763    1\n383    1\nName: Survived, dtype: int64\n\n\n\nmodel.predict(X_test[0:10])\n\narray([0, 1, 0, 0, 0, 0, 0, 1, 1, 1])\n\n\n\nmodel.predict_proba(X_test[:10])\n\narray([[9.22826078e-01, 7.71739224e-02],\n       [1.90547332e-04, 9.99809453e-01],\n       [6.93224146e-01, 3.06775854e-01],\n       [9.59335969e-01, 4.06640310e-02],\n       [9.65380973e-01, 3.46190265e-02],\n       [9.56271850e-01, 4.37281504e-02],\n       [8.17245910e-01, 1.82754090e-01],\n       [3.83233278e-01, 6.16766722e-01],\n       [9.28107033e-04, 9.99071893e-01],\n       [6.70466692e-02, 9.32953331e-01]])\n\n\nCalculate the score using cross validation\n\nfrom sklearn.model_selection import cross_val_score\ncross_val_score(GaussianNB(),X_train, y_train, cv=5)\n\narray([0.784     , 0.728     , 0.744     , 0.75806452, 0.80645161])\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Blog",
      "The Naive Bayes Approach"
    ]
  },
  {
    "objectID": "kernalsridgeregression.html",
    "href": "kernalsridgeregression.html",
    "title": "Kernels Ridge Regression",
    "section": "",
    "text": "Ridge Regression is a type of linear regression that includes a regularization term to prevent overfitting.\n\nCost(w)=∑i=1n​(yi​−yi​^​)2+λ∑j=1p​wj2​\nwhere: - yiyi​ is the true value, - yiyi​​ is the predicted value, - wjwj​ are the model coefficients, - λλ is the regularization parameter.",
    "crumbs": [
      "Blog",
      "Kernels Ridge Regression"
    ]
  },
  {
    "objectID": "kernalsridgeregression.html#basics-of-ridge-regression",
    "href": "kernalsridgeregression.html#basics-of-ridge-regression",
    "title": "Kernels Ridge Regression",
    "section": "",
    "text": "Ridge Regression is a type of linear regression that includes a regularization term to prevent overfitting.\n\nCost(w)=∑i=1n​(yi​−yi​^​)2+λ∑j=1p​wj2​\nwhere: - yiyi​ is the true value, - yiyi​​ is the predicted value, - wjwj​ are the model coefficients, - λλ is the regularization parameter.",
    "crumbs": [
      "Blog",
      "Kernels Ridge Regression"
    ]
  },
  {
    "objectID": "kernalsridgeregression.html#kernels",
    "href": "kernalsridgeregression.html#kernels",
    "title": "Kernels Ridge Regression",
    "section": "Kernels",
    "text": "Kernels\n\nKernel methods involve using a kernel function to implicitly map the input features into a higher-dimensional space where a linear relationship might exist. This allows the model to capture non-linear relationships in the original feature space.\n\nCommon kernel functions include:\n\nLinear Kernel: k(x,x′)=x⋅x′k(x,x′)=x⋅x′\nPolynomial Kernel: k(x,x′)=(x⋅x′+1)dk(x,x′)=(x⋅x′+1)d\nGaussian (RBF) Kernel: k(x,x′)=exp⁡(−γ∥x−x′∥2)k(x,x′)=exp(−γ∥x−x′∥2)\n\nAdvantages of KRR\n\nNon-linearity: It can model complex, non-linear relationships.\nFlexibility: Different kernels can be used to capture various data patterns.\nRegularization: The regularization term helps prevent overfitting.\n\nDisadvantages of KRR - Computational Cost: Computing the kernel matrix can be computationally expensive, especially for large datasets. - Parameter Tuning: Choosing the right kernel and regularization parameter λλ can be challenging and often requires cross-validation.",
    "crumbs": [
      "Blog",
      "Kernels Ridge Regression"
    ]
  },
  {
    "objectID": "kernalsridgeregression.html#implementation",
    "href": "kernalsridgeregression.html#implementation",
    "title": "Kernels Ridge Regression",
    "section": "Implementation",
    "text": "Implementation\n\n# Example data\nX = np.array([[1, 2], [2, 3], [3, 4]])\ny = np.array([1, 2, 3])\n\n\n\n# Make predictions\npredictions = krr.predict(X)\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\n\n\npath = Path('Data/homeprices.csv')\ndf = pd.read_csv(path)\ndf\n\n\n\n\n\n\n\n\narea\nbedrooms\nage\nprice\n\n\n\n\n0\n2600\n3.0\n20\n550000\n\n\n1\n3000\n4.0\n15\n565000\n\n\n2\n3200\nNaN\n18\n610000\n\n\n3\n3600\n3.0\n30\n595000\n\n\n4\n4000\n5.0\n8\n760000\n\n\n5\n4100\n6.0\n8\n810000\n\n\n\n\n\n\n\n\nplt.xlabel('area')\nplt.ylabel('price')\nplt.scatter(df.area,df.price,color='red',marker='+')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nnew_df = df.drop('price',axis='columns')\nnew_df = new_df.drop('bedrooms',axis='columns')\nnew_df = new_df.drop('age',axis='columns')\nnew_df\n\n\n\n\n\n\n\n\narea\n\n\n\n\n0\n2600\n\n\n1\n3000\n\n\n2\n3200\n\n\n3\n3600\n\n\n4\n4000\n\n\n5\n4100\n\n\n\n\n\n\n\n\nprice = df.price\nprice\n\n0    550000\n1    565000\n2    610000\n3    595000\n4    760000\n5    810000\nName: price, dtype: int64\n\n\n\nfrom sklearn.kernel_ridge import KernelRidge\n\n\n# Define the model\nkrr = KernelRidge(alpha=.1, kernel='rbf')\n\n# Fit the model\nkrr.fit(new_df, price)\n\nKernelRidge(alpha=0.1, kernel='rbf')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KernelRidge?Documentation for KernelRidgeiFittedKernelRidge(alpha=0.1, kernel='rbf') \n\n\n\nkrr.predict([[3300]])\n\n/home/ben/miniconda3/envs/pfast/lib/python3.12/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but KernelRidge was fitted with feature names\n  warnings.warn(\n\n\narray([0.])\n\n\n\nkrr.predict([[5000]])\n\n/home/ben/miniconda3/envs/pfast/lib/python3.12/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but KernelRidge was fitted with feature names\n  warnings.warn(\n\n\narray([0.])\n\n\n\nnumbers_list = list(range(2000, 4000, 100))\n\n\n# Create a DataFrame using the pandas constructor and a dictionary\ndata = {'area': numbers_list}\narea_df = pd.DataFrame(data)\narea_df\n\n\n\n\n\n\n\n\narea\n\n\n\n\n0\n2000\n\n\n1\n2100\n\n\n2\n2200\n\n\n3\n2300\n\n\n4\n2400\n\n\n5\n2500\n\n\n6\n2600\n\n\n7\n2700\n\n\n8\n2800\n\n\n9\n2900\n\n\n10\n3000\n\n\n11\n3100\n\n\n12\n3200\n\n\n13\n3300\n\n\n14\n3400\n\n\n15\n3500\n\n\n16\n3600\n\n\n17\n3700\n\n\n18\n3800\n\n\n19\n3900\n\n\n\n\n\n\n\n\np = krr.predict(area_df)\np\n\narray([     0.        ,      0.        ,      0.        ,      0.        ,\n            0.        ,      0.        , 500000.        ,      0.        ,\n            0.        ,      0.        , 513636.36363636,      0.        ,\n       554545.45454545,      0.        ,      0.        ,      0.        ,\n       540909.09090909,      0.        ,      0.        ,      0.        ])\n\n\n\nplt.plot(area_df, p)",
    "crumbs": [
      "Blog",
      "Kernels Ridge Regression"
    ]
  },
  {
    "objectID": "generative.html",
    "href": "generative.html",
    "title": "Generative Models",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Blog",
      "Generative Models"
    ]
  },
  {
    "objectID": "naive_bayes_tut.html",
    "href": "naive_bayes_tut.html",
    "title": "The Naive Bayes Tutorial",
    "section": "",
    "text": "import pandas as pd\n\n\ndf = pd.read_csv(\"Data/spam.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nCategory\nMessage\n\n\n\n\n0\nham\nGo until jurong point, crazy.. Available only ...\n\n\n1\nham\nOk lar... Joking wif u oni...\n\n\n2\nspam\nFree entry in 2 a wkly comp to win FA Cup fina...\n\n\n3\nham\nU dun say so early hor... U c already then say...\n\n\n4\nham\nNah I don't think he goes to usf, he lives aro...\n\n\n\n\n\n\n\n\ndf.groupby('Category').describe()\n\n\n\n\n\n\n\n\nMessage\n\n\n\ncount\nunique\ntop\nfreq\n\n\nCategory\n\n\n\n\n\n\n\n\nham\n4825\n4516\nSorry, I'll call later\n30\n\n\nspam\n747\n641\nPlease call our customer service representativ...\n4\n\n\n\n\n\n\n\n\ndf['spam']=df['Category'].apply(lambda x: 1 if x=='spam' else 0)\ndf.head()\n\n\n\n\n\n\n\n\nCategory\nMessage\nspam\n\n\n\n\n0\nham\nGo until jurong point, crazy.. Available only ...\n0\n\n\n1\nham\nOk lar... Joking wif u oni...\n0\n\n\n2\nspam\nFree entry in 2 a wkly comp to win FA Cup fina...\n1\n\n\n3\nham\nU dun say so early hor... U c already then say...\n0\n\n\n4\nham\nNah I don't think he goes to usf, he lives aro...\n0\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df.Message,df.spam)\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nv = CountVectorizer()\nX_train_count = v.fit_transform(X_train.values)\nX_train_count.toarray()[:2]\n\narray([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]])\n\n\n\nfrom sklearn.naive_bayes import MultinomialNB\nmodel = MultinomialNB()\nmodel.fit(X_train_count,y_train)\n\nMultinomialNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MultinomialNBMultinomialNB()\n\n\n\nemails = [\n    'Hey mohan, can we get together to watch footbal game tomorrow?',\n    'Upto 20% discount on parking, exclusive offer just for you. Dont miss this reward!'\n]\nemails_count = v.transform(emails)\nmodel.predict(emails_count)\n\narray([0, 1])\n\n\n\nX_test_count = v.transform(X_test)\nmodel.score(X_test_count, y_test)\n\n0.9863603732950467\n\n\nSklearn Pipeline\n\nfrom sklearn.pipeline import Pipeline\nclf = Pipeline([\n    ('vectorizer', CountVectorizer()),\n    ('nb', MultinomialNB())\n])\n\n\nclf.fit(X_train, y_train)\n\nPipeline(steps=[('vectorizer', CountVectorizer()), ('nb', MultinomialNB())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('vectorizer', CountVectorizer()), ('nb', MultinomialNB())])CountVectorizerCountVectorizer()MultinomialNBMultinomialNB()\n\n\n\nclf.score(X_test,y_test)\n\n0.9863603732950467\n\n\n\nclf.predict(emails)\n\narray([0, 1])\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Blog",
      "The Naive Bayes Tutorial"
    ]
  },
  {
    "objectID": "logisticregression.html",
    "href": "logisticregression.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "Binary Classification\nMulticlass Classification",
    "crumbs": [
      "Blog",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "logisticregression.html#classification-types",
    "href": "logisticregression.html#classification-types",
    "title": "Logistic Regression",
    "section": "",
    "text": "Binary Classification\nMulticlass Classification",
    "crumbs": [
      "Blog",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "logisticregression.html#binary-classification",
    "href": "logisticregression.html#binary-classification",
    "title": "Logistic Regression",
    "section": "Binary Classification",
    "text": "Binary Classification\n\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom pathlib import Path\nimport numpy as np\n\n\npath = Path('Data/insurance_data.csv')\ndf = pd.read_csv(path)\ndf.head()\n\n\n\n\n\n\n\n\nage\nbought_insurance\n\n\n\n\n0\n22\n0\n\n\n1\n25\n0\n\n\n2\n47\n1\n\n\n3\n52\n0\n\n\n4\n46\n1\n\n\n\n\n\n\n\n\\(\\text{sigmoid}(z) = \\dfrac{1}{1+e^-z}\\) where e = Euler’s number ~ 2.71828\n\nplt.scatter(df.age,df.bought_insurance,marker='+',color='red')\n\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\n\nX_train, X_test, y_train, y_test = train_test_split(df[['age']],df.bought_insurance,train_size=0.8)\n\n\nX_test\n\n\n\n\n\n\n\n\nage\n\n\n\n\n12\n27\n\n\n25\n54\n\n\n7\n60\n\n\n2\n47\n\n\n10\n18\n\n\n9\n61\n\n\n\n\n\n\n\n\nmodel = LogisticRegression()\n\n\nmodel.fit(X_train, y_train)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\nX_test\n\n\n\n\n\n\n\n\nage\n\n\n\n\n12\n27\n\n\n25\n54\n\n\n7\n60\n\n\n2\n47\n\n\n10\n18\n\n\n9\n61\n\n\n\n\n\n\n\n\ny_predicted = model.predict(X_test)\ny_predicted\n\narray([0, 1, 1, 1, 0, 1])\n\n\n\ny_probability = model.predict_proba(X_test)\n\n\nmodel.score(X_test,y_test)\n\n1.0\n\n\n\narray1 = np.array(X_test)\n\n# Stack the arrays horizontally\ncombined_array = np.hstack((array1, y_probability[:,1].reshape(-1, 1)))\n\nprint(\"Combined Array:\")\nprint(combined_array)\n\nCombined Array:\n[[27.          0.18888707]\n [54.          0.84143616]\n [60.          0.91401439]\n [47.          0.70233749]\n [18.          0.07590529]\n [61.          0.92268871]]\n\n\n\nsorted_array = np.sort(combined_array, 0)\nsorted_array[:,0]\n\narray([18., 27., 47., 54., 60., 61.])\n\n\n\nplt.scatter(df.age,df.bought_insurance,marker='+',color='blue')\nplt.scatter(X_test,y_test,marker='+',color='red')\nplt.plot(sorted_array[:,0],sorted_array[:,1],marker='+',color='green')\nplt.scatter(X_test,y_predicted,marker='*',color='green')",
    "crumbs": [
      "Blog",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "logisticregression.html#multiclass-regression",
    "href": "logisticregression.html#multiclass-regression",
    "title": "Logistic Regression",
    "section": "Multiclass Regression",
    "text": "Multiclass Regression\n\nfrom sklearn.datasets import load_digits\nimport matplotlib.pyplot as plt\n\n\ndigits = load_digits()\n\n\nplt.gray() \nfor i in range(2):\n    plt.matshow(digits.images[i])\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndir(digits)\n\n['DESCR', 'data', 'feature_names', 'frame', 'images', 'target', 'target_names']\n\n\n\ndigits.target[:]\n\narray([0, 1, 2, ..., 8, 9, 8])\n\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\n\nmodel = LogisticRegression()\n\n\nX_train, X_test, y_train, y_test = train_test_split(digits.data,digits.target, test_size=0.2)\n\n\nmodel.fit(X_train, y_train)\n\n/home/ben/mambaforge/envs/cfast/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\nmodel.score(X_test, y_test)\n\n0.9722222222222222\n\n\n\nmodel.predict(digits.data[0:5])\n\narray([0, 1, 2, 3, 4])\n\n\n\ny_predicted = model.predict(X_test)\n\narray([5, 4, 0, 2, 9, 5, 3, 2, 0, 4, 1, 3, 5, 1, 5, 3, 6, 3, 5, 2, 3, 2,\n       0, 8, 1, 9, 6, 7, 0, 8, 9, 4, 5, 7, 2, 4, 4, 4, 8, 3, 7, 8, 3, 6,\n       4, 9, 2, 4, 6, 3, 5, 1, 6, 0, 7, 9, 4, 8, 8, 3, 8, 9, 5, 6, 4, 9,\n       8, 5, 2, 0, 7, 7, 6, 2, 5, 8, 9, 5, 7, 5, 5, 4, 4, 8, 9, 8, 9, 2,\n       1, 0, 7, 4, 8, 6, 3, 3, 3, 8, 1, 1, 5, 6, 7, 6, 1, 7, 2, 8, 1, 5,\n       3, 4, 4, 9, 5, 0, 7, 0, 6, 3, 2, 2, 4, 3, 4, 8, 6, 0, 8, 0, 3, 1,\n       4, 9, 0, 3, 2, 9, 9, 6, 7, 8, 4, 6, 8, 6, 9, 0, 4, 9, 7, 6, 8, 3,\n       9, 6, 0, 7, 1, 7, 2, 5, 2, 3, 3, 8, 0, 0, 9, 4, 4, 5, 9, 0, 8, 8,\n       7, 9, 9, 8, 3, 3, 8, 7, 0, 4, 6, 6, 1, 1, 9, 0, 3, 1, 3, 9, 2, 8,\n       3, 7, 4, 5, 5, 7, 2, 1, 9, 5, 5, 7, 9, 1, 9, 1, 7, 6, 5, 1, 6, 7,\n       5, 6, 7, 2, 9, 4, 9, 0, 8, 3, 3, 6, 0, 1, 3, 3, 9, 6, 1, 5, 1, 6,\n       6, 3, 1, 0, 1, 0, 2, 2, 1, 9, 7, 9, 1, 0, 9, 1, 3, 8, 1, 5, 0, 0,\n       8, 6, 1, 2, 6, 6, 9, 5, 3, 6, 3, 8, 9, 8, 6, 9, 7, 2, 8, 5, 9, 6,\n       9, 7, 3, 7, 4, 3, 2, 1, 5, 8, 0, 8, 6, 6, 7, 5, 6, 6, 4, 6, 3, 7,\n       2, 3, 6, 8, 5, 3, 1, 6, 8, 8, 9, 0, 8, 5, 6, 8, 0, 1, 2, 0, 0, 1,\n       9, 6, 7, 6, 3, 2, 0, 5, 5, 2, 7, 1, 6, 4, 6, 0, 2, 5, 2, 0, 7, 6,\n       9, 6, 1, 9, 1, 1, 0, 0])\n\n\n\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sn\n\n\ncm = confusion_matrix(y_test, y_predicted)\nplt.figure(figsize = (10,7))\nsn.heatmap(cm, annot=True)\nplt.xlabel('Predicted')\nplt.ylabel('Truth')\n\nText(95.72222222222221, 0.5, 'Truth')\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import classification_report\n\n\nreport = classification_report(y_test, y_predicted)\nprint(report)\n\n              precision    recall  f1-score   support\n\n           0       0.97      1.00      0.99        35\n           1       0.97      0.97      0.97        36\n           2       0.93      0.96      0.95        28\n           3       0.97      0.95      0.96        40\n           4       0.96      1.00      0.98        27\n           5       0.94      0.94      0.94        35\n           6       1.00      1.00      1.00        46\n           7       1.00      0.97      0.98        33\n           8       0.97      0.97      0.97        38\n           9       0.98      0.95      0.96        42\n\n    accuracy                           0.97       360\n   macro avg       0.97      0.97      0.97       360\nweighted avg       0.97      0.97      0.97       360",
    "crumbs": [
      "Blog",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "multi-linearregression.html",
    "href": "multi-linearregression.html",
    "title": "Multi-Linear Regression",
    "section": "",
    "text": "Below is the table containing home prices in monroe twp, NJ. Here price depends on area (square feet), bed rooms and age of the home (in years). Given these prices we have to predict prices of new homes based on area, bed rooms and age.\n\nGiven these home prices find out price of a home that has,\n3000 sqr ft area, 3 bedrooms, 40 year old\n2500 sqr ft area, 4 bedrooms, 5 year old\nWe will use regression with multiple variables here. Price can be calculated using following equation,\n\nHere area, bedrooms, age are called independant variables or features whereas price is a dependant variable\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn import linear_model\n\n\ndf = pd.read_csv('./Data/homeprices.csv')\ndf\n\n\n\n\n\n\n\n\narea\nbedrooms\nage\nprice\n\n\n\n\n0\n2600\n3.0\n20\n550000\n\n\n1\n3000\n4.0\n15\n565000\n\n\n2\n3200\nNaN\n18\n610000\n\n\n3\n3600\n3.0\n30\n595000\n\n\n4\n4000\n5.0\n8\n760000\n\n\n5\n4100\n6.0\n8\n810000\n\n\n\n\n\n\n\nData Preprocessing: Fill NA values with median value of a column\n\ndf.bedrooms.median()\n\n4.0\n\n\n\ndf.bedrooms = df.bedrooms.fillna(df.bedrooms.median())\ndf\n\n\n\n\n\n\n\n\narea\nbedrooms\nage\nprice\n\n\n\n\n0\n2600\n3.0\n20\n550000\n\n\n1\n3000\n4.0\n15\n565000\n\n\n2\n3200\n4.0\n18\n610000\n\n\n3\n3600\n3.0\n30\n595000\n\n\n4\n4000\n5.0\n8\n760000\n\n\n5\n4100\n6.0\n8\n810000\n\n\n\n\n\n\n\n\nreg = linear_model.LinearRegression()\nreg.fit(df.drop('price',axis='columns'),df.price)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nreg.coef_\n\narray([  112.06244194, 23388.88007794, -3231.71790863])\n\n\n\nreg.intercept_\n\n221323.00186540408\n\n\nFind price of home with 3000 sqr ft area, 3 bedrooms, 40 year old\n\nreg.predict([[3000, 3, 40]])\n\n/home/ben/mambaforge/envs/cfast/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n  warnings.warn(\n\n\narray([498408.25158031])\n\n\n\n112.06244194*3000 + 23388.88007794*3 + -3231.71790863*40 + 221323.00186540384\n\n498408.25157402386\n\n\nFind price of home with 2500 sqr ft area, 4 bedrooms, 5 year old\n\nreg.predict([[2500, 4, 5]])\n\n/home/ben/mambaforge/envs/cfast/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n  warnings.warn(\n\n\narray([578876.03748933])\n\n\n\nnp.set_printoptions(precision=2, suppress=True)\ncorrcoef = np.corrcoef(df, rowvar=False)\ncorrcoef\n\narray([[ 1.  ,  0.75, -0.45,  0.9 ],\n       [ 0.75,  1.  , -0.88,  0.92],\n       [-0.45, -0.88,  1.  , -0.73],\n       [ 0.9 ,  0.92, -0.73,  1.  ]])\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sn\n\n\ndf.columns\n\nIndex(['area', 'bedrooms', 'age', 'price'], dtype='object')\n\n\n\nplt.figure(figsize=(7,5))\nsn.heatmap(corrcoef, annot=True)\n\nplt.xticks(range(len(df.columns)), df.columns)\nplt.yticks(range(len(df.columns)), df.columns)\n# Move x-axis ticks and labels to the top\nplt.gca().xaxis.set_ticks_position('top')\n\n\nplt.show()",
    "crumbs": [
      "Blog",
      "Multi-Linear Regression"
    ]
  },
  {
    "objectID": "multi-linearregression.html#sample-problem-of-predicting-home-price-in-monroe-new-jersey-usa",
    "href": "multi-linearregression.html#sample-problem-of-predicting-home-price-in-monroe-new-jersey-usa",
    "title": "Multi-Linear Regression",
    "section": "",
    "text": "Below is the table containing home prices in monroe twp, NJ. Here price depends on area (square feet), bed rooms and age of the home (in years). Given these prices we have to predict prices of new homes based on area, bed rooms and age.\n\nGiven these home prices find out price of a home that has,\n3000 sqr ft area, 3 bedrooms, 40 year old\n2500 sqr ft area, 4 bedrooms, 5 year old\nWe will use regression with multiple variables here. Price can be calculated using following equation,\n\nHere area, bedrooms, age are called independant variables or features whereas price is a dependant variable\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn import linear_model\n\n\ndf = pd.read_csv('./Data/homeprices.csv')\ndf\n\n\n\n\n\n\n\n\narea\nbedrooms\nage\nprice\n\n\n\n\n0\n2600\n3.0\n20\n550000\n\n\n1\n3000\n4.0\n15\n565000\n\n\n2\n3200\nNaN\n18\n610000\n\n\n3\n3600\n3.0\n30\n595000\n\n\n4\n4000\n5.0\n8\n760000\n\n\n5\n4100\n6.0\n8\n810000\n\n\n\n\n\n\n\nData Preprocessing: Fill NA values with median value of a column\n\ndf.bedrooms.median()\n\n4.0\n\n\n\ndf.bedrooms = df.bedrooms.fillna(df.bedrooms.median())\ndf\n\n\n\n\n\n\n\n\narea\nbedrooms\nage\nprice\n\n\n\n\n0\n2600\n3.0\n20\n550000\n\n\n1\n3000\n4.0\n15\n565000\n\n\n2\n3200\n4.0\n18\n610000\n\n\n3\n3600\n3.0\n30\n595000\n\n\n4\n4000\n5.0\n8\n760000\n\n\n5\n4100\n6.0\n8\n810000\n\n\n\n\n\n\n\n\nreg = linear_model.LinearRegression()\nreg.fit(df.drop('price',axis='columns'),df.price)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nreg.coef_\n\narray([  112.06244194, 23388.88007794, -3231.71790863])\n\n\n\nreg.intercept_\n\n221323.00186540408\n\n\nFind price of home with 3000 sqr ft area, 3 bedrooms, 40 year old\n\nreg.predict([[3000, 3, 40]])\n\n/home/ben/mambaforge/envs/cfast/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n  warnings.warn(\n\n\narray([498408.25158031])\n\n\n\n112.06244194*3000 + 23388.88007794*3 + -3231.71790863*40 + 221323.00186540384\n\n498408.25157402386\n\n\nFind price of home with 2500 sqr ft area, 4 bedrooms, 5 year old\n\nreg.predict([[2500, 4, 5]])\n\n/home/ben/mambaforge/envs/cfast/lib/python3.11/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n  warnings.warn(\n\n\narray([578876.03748933])\n\n\n\nnp.set_printoptions(precision=2, suppress=True)\ncorrcoef = np.corrcoef(df, rowvar=False)\ncorrcoef\n\narray([[ 1.  ,  0.75, -0.45,  0.9 ],\n       [ 0.75,  1.  , -0.88,  0.92],\n       [-0.45, -0.88,  1.  , -0.73],\n       [ 0.9 ,  0.92, -0.73,  1.  ]])\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sn\n\n\ndf.columns\n\nIndex(['area', 'bedrooms', 'age', 'price'], dtype='object')\n\n\n\nplt.figure(figsize=(7,5))\nsn.heatmap(corrcoef, annot=True)\n\nplt.xticks(range(len(df.columns)), df.columns)\nplt.yticks(range(len(df.columns)), df.columns)\n# Move x-axis ticks and labels to the top\nplt.gca().xaxis.set_ticks_position('top')\n\n\nplt.show()",
    "crumbs": [
      "Blog",
      "Multi-Linear Regression"
    ]
  },
  {
    "objectID": "cuda.html",
    "href": "cuda.html",
    "title": "CUDA",
    "section": "",
    "text": "import torch, os, math\nimport torchvision as tv\nimport torchvision.transforms.functional as tvf\nfrom torchvision import io\nimport matplotlib.pyplot as plt\nfrom torch.utils.cpp_extension import load_inline\n\n\nimg = io.read_image('puppy.jpg')\nprint(img.shape)\nimg[:2,:3,:4]\n\ntorch.Size([3, 1330, 1920])\n\n\ntensor([[[225, 225, 225, 225],\n         [225, 225, 225, 225],\n         [225, 225, 225, 225]],\n\n        [[228, 228, 228, 228],\n         [228, 228, 228, 228],\n         [228, 228, 228, 228]]], dtype=torch.uint8)\n\n\n\ndef show_img(x, figsize=(4,3), **kwargs):\n    plt.figure(figsize=figsize)\n    plt.axis('off')\n    if len(x.shape)==3: x = x.permute(1,2,0)  # CHW -&gt; HWC\n    plt.imshow(x.cpu(), **kwargs)\n\n\nimg2 = tvf.resize(img, 150, antialias=True)\nch,h,w = img2.shape\nch,h,w,h*w\n\n(3, 150, 216, 32400)\n\n\n\nshow_img(img2)",
    "crumbs": [
      "Blog",
      "CUDA"
    ]
  },
  {
    "objectID": "cuda.html#setup",
    "href": "cuda.html#setup",
    "title": "CUDA",
    "section": "",
    "text": "import torch, os, math\nimport torchvision as tv\nimport torchvision.transforms.functional as tvf\nfrom torchvision import io\nimport matplotlib.pyplot as plt\nfrom torch.utils.cpp_extension import load_inline\n\n\nimg = io.read_image('puppy.jpg')\nprint(img.shape)\nimg[:2,:3,:4]\n\ntorch.Size([3, 1330, 1920])\n\n\ntensor([[[225, 225, 225, 225],\n         [225, 225, 225, 225],\n         [225, 225, 225, 225]],\n\n        [[228, 228, 228, 228],\n         [228, 228, 228, 228],\n         [228, 228, 228, 228]]], dtype=torch.uint8)\n\n\n\ndef show_img(x, figsize=(4,3), **kwargs):\n    plt.figure(figsize=figsize)\n    plt.axis('off')\n    if len(x.shape)==3: x = x.permute(1,2,0)  # CHW -&gt; HWC\n    plt.imshow(x.cpu(), **kwargs)\n\n\nimg2 = tvf.resize(img, 150, antialias=True)\nch,h,w = img2.shape\nch,h,w,h*w\n\n(3, 150, 216, 32400)\n\n\n\nshow_img(img2)",
    "crumbs": [
      "Blog",
      "CUDA"
    ]
  },
  {
    "objectID": "cuda.html#rgb---grey",
    "href": "cuda.html#rgb---grey",
    "title": "CUDA",
    "section": "RGB -> Grey",
    "text": "RGB -&gt; Grey\n\nBasic Python\n\ndef rgb2grey_py(x):\n    c,h,w = x.shape\n    n = h*w\n    x = x.flatten()\n    res = torch.empty(n, dtype=x.dtype, device=x.device)\n    for i in range(n): res[i] = 0.2989*x[i] + 0.5870*x[i+n] + 0.1140*x[i+2*n]\n    return res.view(h,w)\n\n\nimg_g = rgb2grey_py(img2)\n\nCPU times: user 1.59 s, sys: 27.1 ms, total: 1.61 s\nWall time: 1.11 s\n\n\n\nshow_img(img_g, cmap='gray')\n\n\n\n\n\n\n\n\n\n\nPython Kernel\n\ndef run_kernel(f, times, *args):\n    for i in range(times): f(i, *args)\n\nNB: A kernel can not return anything. It can only change contents of things passed to it.\n\ndef rgb2grey_k(i, x, out, n):\n    out[i] = 0.2989*x[i] + 0.5870*x[i+n] + 0.1140*x[i+2*n]\n\n\ndef rgb2grey_pyk(x):\n    c,h,w = x.shape\n    n = h*w\n    x = x.flatten()\n    res = torch.empty(n, dtype=x.dtype, device=x.device)\n    run_kernel(rgb2grey_k, h*w, x, res, n)\n    return res.view(h,w)\n\n\nimg_g = rgb2grey_pyk(img2)\n\nCPU times: user 1.06 s, sys: 0 ns, total: 1.06 s\nWall time: 1.06 s\n\n\n\nshow_img(img_g, cmap='gray')\n\n\n\n\n\n\n\n\n\n\nPython Block Kernel\n\nStreaming Multiprocessors (SMs): In NVIDIA GPUs, SMs are the fundamental units of execution. Each SM can execute multiple threads concurrently.\nThread Blocks: A thread block is a group of threads that can cooperate among themselves through shared memory and synchronization. All threads in a block are executed on the same SM. This means they can share resources such as shared memory and can synchronize their execution with each other.\nShared Memory: Shared memory is a small memory space on the GPU that is shared among the threads in a block. It is much faster than global memory (the main GPU memory), but it is also limited in size. Threads in the same block can use shared memory to share data with each other efficiently.\n\n\nThe RTX 3090, based on the Ampere architecture, has 82 SMs.\nEach SM in GA10x GPUs contain 128 CUDA Cores, four third-generation Tensor Cores, a 256 KB Register File, and 128 KB of L1/Shared Memory\nIn CUDA, all threads in a block have the potential to run concurrently. However, the actual concurrency depends on the number of CUDA cores per SM and the resources required by the threads.\n\n\n128*82\n\n10496\n\n\n\ndef blk_kernel(f, blocks, threads, *args):\n    for i in range(blocks):\n        for j in range(threads): f(i, j, threads, *args)\n\n\ndef rgb2grey_bk(blockidx, threadidx, blockdim, x, out, n):\n    i = blockidx*blockdim + threadidx\n    if i&lt;n: out[i] = 0.2989*x[i] + 0.5870*x[i+n] + 0.1140*x[i+2*n]\n\n\ndef rgb2grey_pybk(x):\n    c,h,w = x.shape\n    n = h*w\n    x = x.flatten()\n    res = torch.empty(n, dtype=x.dtype, device=x.device)\n    threads = 256\n    blocks = int(math.ceil(h*w/threads))\n    blk_kernel(rgb2grey_bk, blocks, threads, x, res, n)\n    return res.view(h,w)\n\n\nimg_g = rgb2grey_pybk(img2)\n\nCPU times: user 1.1 s, sys: 0 ns, total: 1.1 s\nWall time: 1.1 s\n\n\n\nshow_img(img_g, cmap='gray')\n\n\n\n\n\n\n\n\n\n\nCUDA Setup\n\nos.environ['CUDA_LAUNCH_BLOCKING']='1'\n\n\nos.environ['CUDA_HOME']='/usr/local/cuda'\n\n\ndef load_cuda(cuda_src, cpp_src, funcs, opt=False, verbose=False):\n    return load_inline(cuda_sources=[cuda_src], cpp_sources=[cpp_src], functions=funcs,\n                       extra_cuda_cflags=[\"-O2\"] if opt else [], verbose=verbose, name=\"inline_ext\")\n\n\ncuda_begin = r'''\n#include &lt;torch/extension.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;c10/cuda/CUDAException.h&gt;\n\n#define CHECK_CUDA(x) TORCH_CHECK(x.device().is_cuda(), #x \" must be a CUDA tensor\")\n#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x \" must be contiguous\")\n#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n\ninline unsigned int cdiv(unsigned int a, unsigned int b) { return (a + b - 1) / b;}\n'''\n\n\n\nCUDA kernel\n\n2^31 max blocks for dim 0, 2^16 max for dims 1 & 2\n1024 max threads per block (use a multiple of 32)\n\n\n\ncuda_src = cuda_begin + r'''\n__global__ void rgb_to_grayscale_kernel(unsigned char* x, unsigned char* out, int n) {\n    int i = blockIdx.x*blockDim.x + threadIdx.x;\n    if (i&lt;n) out[i] = 0.2989*x[i] + 0.5870*x[i+n] + 0.1140*x[i+2*n];\n}\n\ntorch::Tensor rgb_to_grayscale(torch::Tensor input) {\n    CHECK_INPUT(input);\n    int h = input.size(1);\n    int w = input.size(2);\n    printf(\"h*w: %d*%d\\n\", h, w);\n    auto output = torch::empty({h,w}, input.options());\n    int threads = 256;\n    rgb_to_grayscale_kernel&lt;&lt;&lt;cdiv(w*h,threads), threads&gt;&gt;&gt;(\n        input.data_ptr&lt;unsigned char&gt;(), output.data_ptr&lt;unsigned char&gt;(), w*h);\n    C10_CUDA_KERNEL_LAUNCH_CHECK();\n    return output;\n}'''\n\n\ncpp_src = \"torch::Tensor rgb_to_grayscale(torch::Tensor input);\"\n\n\nmodule = load_cuda(cuda_src, cpp_src, ['rgb_to_grayscale'], verbose=True)\n\nUsing /home/ben/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\nNo modifications detected for re-loaded extension module inline_ext, skipping build step...\nLoading extension module inline_ext...\n\n\n\n---------------------------------------------------------------------------\nImportError                               Traceback (most recent call last)\nCell In[33], line 1\n----&gt; 1 module = load_cuda(cuda_src, cpp_src, ['rgb_to_grayscale'], verbose=True)\n\nCell In[29], line 2, in load_cuda(cuda_src, cpp_src, funcs, opt, verbose)\n      1 def load_cuda(cuda_src, cpp_src, funcs, opt=False, verbose=False):\n----&gt; 2     return load_inline(cuda_sources=[cuda_src], cpp_sources=[cpp_src], functions=funcs,\n      3                        extra_cuda_cflags=[\"-O2\"] if opt else [], verbose=verbose, name=\"inline_ext\")\n\nFile ~/mambaforge/envs/cfast/lib/python3.11/site-packages/torch/utils/cpp_extension.py:1434, in load_inline(name, cpp_sources, cuda_sources, functions, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_python_module, with_pytorch_error_handling, keep_intermediates)\n   1430         cuda_source_file.write('\\n'.join(cuda_sources))\n   1432     sources.append(cuda_source_path)\n-&gt; 1434 return _jit_compile(\n   1435     name,\n   1436     sources,\n   1437     extra_cflags,\n   1438     extra_cuda_cflags,\n   1439     extra_ldflags,\n   1440     extra_include_paths,\n   1441     build_directory,\n   1442     verbose,\n   1443     with_cuda,\n   1444     is_python_module,\n   1445     is_standalone=False,\n   1446     keep_intermediates=keep_intermediates)\n\nFile ~/mambaforge/envs/cfast/lib/python3.11/site-packages/torch/utils/cpp_extension.py:1535, in _jit_compile(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_python_module, is_standalone, keep_intermediates)\n   1532 if is_standalone:\n   1533     return _get_exec_path(name, build_directory)\n-&gt; 1535 return _import_module_from_library(name, build_directory, is_python_module)\n\nFile ~/mambaforge/envs/cfast/lib/python3.11/site-packages/torch/utils/cpp_extension.py:1929, in _import_module_from_library(module_name, path, is_python_module)\n   1927 spec = importlib.util.spec_from_file_location(module_name, filepath)\n   1928 assert spec is not None\n-&gt; 1929 module = importlib.util.module_from_spec(spec)\n   1930 assert isinstance(spec.loader, importlib.abc.Loader)\n   1931 spec.loader.exec_module(module)\n\nFile &lt;frozen importlib._bootstrap&gt;:573, in module_from_spec(spec)\n\nFile &lt;frozen importlib._bootstrap_external&gt;:1233, in create_module(self, spec)\n\nFile &lt;frozen importlib._bootstrap&gt;:241, in _call_with_frames_removed(f, *args, **kwds)\n\nImportError: /home/ben/.cache/torch_extensions/py311_cu118/inline_ext/inline_ext.so: cannot open shared object file: No such file or directory\n\n\n\n\n[o for o in dir(module) if o[0]!='_']\n\n\nimgc = img.contiguous().cuda()\n\n\nres = module.rgb_to_grayscale(imgc).cpu()\nh,w = res.shape\nh,w,h*w\n\n\nshow_img(res, cmap='gray')",
    "crumbs": [
      "Blog",
      "CUDA"
    ]
  },
  {
    "objectID": "cuda.html#matmul",
    "href": "cuda.html#matmul",
    "title": "CUDA",
    "section": "Matmul",
    "text": "Matmul\n\nGet data\n\nimport gzip,pickle\nfrom urllib.request import urlretrieve\nfrom pathlib import Path\nfrom torch import tensor\n\n\nMNIST_URL='https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true'\npath_data = Path('data')\npath_data.mkdir(exist_ok=True)\npath_gz = path_data/'mnist.pkl.gz'\nif not path_gz.exists(): urlretrieve(MNIST_URL, path_gz)\n\n\nwith gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\nx_train,y_train,x_valid,y_valid = map(tensor, (x_train,y_train,x_valid,y_valid))\nx_train.shape,x_train.type()\n\n(torch.Size([50000, 784]), 'torch.FloatTensor')\n\n\n\nimgs = x_train.reshape((-1,28,28))\nimgs.shape\n\ntorch.Size([50000, 28, 28])\n\n\n\nshow_img(imgs[0], cmap='gray_r', figsize=(1,1))\n\n\n\n\n\n\n\n\n\ntorch.manual_seed(1)\nweights = torch.randn(784,10)\nweights\n\ntensor([[-1.5256, -0.7502, -0.6540,  ..., -1.6091, -0.7121,  0.3037],\n        [-0.7773, -0.2515, -0.2223,  ..., -1.1608,  0.6995,  0.1991],\n        [ 0.8657,  0.2444, -0.6629,  ..., -1.4465,  0.0612, -0.6177],\n        ...,\n        [ 0.5063,  0.4656, -0.2634,  ...,  0.6452,  0.4298, -1.2936],\n        [ 0.5171,  1.0315,  0.8120,  ..., -0.1046,  2.2588, -0.2793],\n        [-1.4899,  0.3898, -0.5454,  ..., -0.1923, -0.5076,  0.5439]])\n\n\n\n\nPython matmul\n\nm1 = x_valid[:5]\nm2 = weights\nm1.shape,m2.shape\n\n(torch.Size([5, 784]), torch.Size([784, 10]))\n\n\n\nar,ac = m1.shape # n_rows * n_cols\nbr,bc = m2.shape\n(ar,ac),(br,bc)\n\n((5, 784), (784, 10))\n\n\n\nt1 = torch.zeros(ar, bc)\nt1.shape\n\ntorch.Size([5, 10])\n\n\n\nfor i in range(ar):         # 5\n    for j in range(bc):     # 10\n        for k in range(ac): # 784\n            t1[i,j] += m1[i,k] * m2[k,j]\n\n\nt1.shape\n\ntorch.Size([5, 10])\n\n\n\nimport numpy as np\nnp.set_printoptions(precision=2, linewidth=140)\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\n\n\nt1\n\ntensor([[-10.94,  -0.68,  -7.00,  -4.01,  -2.09,  -3.36,   3.91,  -3.44, -11.47,  -2.12],\n        [ 14.54,   6.00,   2.89,  -4.08,   6.59, -14.74,  -9.28,   2.16, -15.28,  -2.68],\n        [  2.22,  -3.22,  -4.80,  -6.05,  14.17,  -8.98,  -4.79,  -5.44, -20.68,  13.57],\n        [ -6.71,   8.90,  -7.46,  -7.90,   2.70,  -4.73, -11.03, -12.98,  -6.44,   3.64],\n        [ -2.44,  -6.40,  -2.40,  -9.04,  11.18,  -5.77,  -8.92,  -3.79,  -8.98,   5.28]])\n\n\n\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc):\n            for k in range(ac): c[i,j] += a[i,k] * b[k,j]\n    return c\n\n\n\n\nCPU times: user 603 ms, sys: 0 ns, total: 603 ms\nWall time: 603 ms\n\n\n\nar*bc*ac\n\n39200\n\n\n\n\n2d Python kernel\n\nfrom types import SimpleNamespace as ns\n\n\ndef blk_kernel2d(f, blocks, threads, *args):\n    for i0 in range(blocks.y):\n        for i1 in range(blocks.x):\n            for j0 in range(threads.y):\n                for j1 in range(threads.x): f(ns(x=i1,y=i0), ns(x=j1,y=j0), threads, *args)\n\n\ndef matmul_bk(blockidx, threadidx, blockdim, m, n, out, h, w, k):\n    r = blockidx.y*blockdim.y + threadidx.y\n    c = blockidx.x*blockdim.x + threadidx.x\n    \n    if (r&gt;=h or c&gt;=w): return\n    o = 0.\n    for i in range(k): o += m[r*k+i] * n[i*w+c]\n    out[r*w+c] = o\n\n\ndef matmul_2d(m, n):\n    h,k  = m.shape\n    k2,w = n.shape\n    assert k==k2, \"Size mismatch!\"\n    output = torch.zeros(h, w, dtype=m.dtype)\n    tpb = ns(x=16,y=16)\n    blocks = ns(x=math.ceil(w/tpb.x), y=math.ceil(h/tpb.y))\n    blk_kernel2d(matmul_bk, blocks, tpb,\n                 m.flatten(), n.flatten(), output.flatten(), h, w, k)\n    return output\n\n\nres = matmul_2d(m1, m2)\ntorch.isclose(t1, res).all()\n\ntensor(True)\n\n\n\n\nBroadcasting\n\ndef matmul(a,b):\n    (ar,ac),(br,bc) = a.shape,b.shape\n    c = torch.zeros(ar, bc)\n    for i in range(ar): c[i] = (a[i,:,None] * b).sum(dim=0)\n    return c\n\n\ntorch.isclose(t1,matmul(m1, m2)).all()\n\ntensor(True)\n\n\n\n\n\nCPU times: user 1.84 ms, sys: 286 µs, total: 2.13 ms\nWall time: 1.79 ms\n\n\n\nm1 = x_train\ntr = matmul(m1, m2)\ntr.shape\n\ntorch.Size([50000, 10])\n\n\n\n\n\nCPU times: user 2.33 s, sys: 11.2 ms, total: 2.34 s\nWall time: 1.32 s\n\n\n\nar,ac = m1.shape\nbr,bc = m2.shape\nar*bc*ac\n\n392000000\n\n\n\n\nCUDA matmul\n\ncuda_src = cuda_begin + r'''\n__global__ void matmul_k(float* m, float* n, float* out, int h, int w, int k) {\n    int r = blockIdx.y*blockDim.y + threadIdx.y;\n    int c = blockIdx.x*blockDim.x + threadIdx.x;\n\n    if (r&gt;=h || c&gt;=w) return;\n    float o = 0;\n    for (int i = 0; i&lt;k; ++i) o += m[r*k+i] * n[i*w+c];\n    out[r*w+c] = o;\n}\n\ntorch::Tensor matmul(torch::Tensor m, torch::Tensor n) {\n    CHECK_INPUT(m); CHECK_INPUT(n);\n    int h = m.size(0);\n    int w = n.size(1);\n    int k = m.size(1);\n    TORCH_CHECK(k==n.size(0), \"Size mismatch!\");\n    auto output = torch::zeros({h, w}, m.options());\n\n    dim3 tpb(16,16);\n    dim3 blocks(cdiv(w, tpb.x), cdiv(h, tpb.y));\n    matmul_k&lt;&lt;&lt;blocks, tpb&gt;&gt;&gt;(\n        m.data_ptr&lt;float&gt;(), n.data_ptr&lt;float&gt;(), output.data_ptr&lt;float&gt;(), h, w, k);\n    C10_CUDA_KERNEL_LAUNCH_CHECK();\n    return output;\n}\n'''\n\n\ncpp_src = \"torch::Tensor matmul(torch::Tensor m, torch::Tensor n);\"\n\n\nmodule = load_cuda(cuda_src, cpp_src, ['matmul'])\n\n\n---------------------------------------------------------------------------\nOSError                                   Traceback (most recent call last)\nCell In[63], line 1\n----&gt; 1 module = load_cuda(cuda_src, cpp_src, ['matmul'])\n\nCell In[29], line 2, in load_cuda(cuda_src, cpp_src, funcs, opt, verbose)\n      1 def load_cuda(cuda_src, cpp_src, funcs, opt=False, verbose=False):\n----&gt; 2     return load_inline(cuda_sources=[cuda_src], cpp_sources=[cpp_src], functions=funcs,\n      3                        extra_cuda_cflags=[\"-O2\"] if opt else [], verbose=verbose, name=\"inline_ext\")\n\nFile ~/mambaforge/envs/cfast/lib/python3.11/site-packages/torch/utils/cpp_extension.py:1434, in load_inline(name, cpp_sources, cuda_sources, functions, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_python_module, with_pytorch_error_handling, keep_intermediates)\n   1430         cuda_source_file.write('\\n'.join(cuda_sources))\n   1432     sources.append(cuda_source_path)\n-&gt; 1434 return _jit_compile(\n   1435     name,\n   1436     sources,\n   1437     extra_cflags,\n   1438     extra_cuda_cflags,\n   1439     extra_ldflags,\n   1440     extra_include_paths,\n   1441     build_directory,\n   1442     verbose,\n   1443     with_cuda,\n   1444     is_python_module,\n   1445     is_standalone=False,\n   1446     keep_intermediates=keep_intermediates)\n\nFile ~/mambaforge/envs/cfast/lib/python3.11/site-packages/torch/utils/cpp_extension.py:1509, in _jit_compile(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_python_module, is_standalone, keep_intermediates)\n   1505                 hipified_sources.add(hipify_result[s_abs][\"hipified_path\"] if s_abs in hipify_result else s_abs)\n   1507             sources = list(hipified_sources)\n-&gt; 1509         _write_ninja_file_and_build_library(\n   1510             name=name,\n   1511             sources=sources,\n   1512             extra_cflags=extra_cflags or [],\n   1513             extra_cuda_cflags=extra_cuda_cflags or [],\n   1514             extra_ldflags=extra_ldflags or [],\n   1515             extra_include_paths=extra_include_paths or [],\n   1516             build_directory=build_directory,\n   1517             verbose=verbose,\n   1518             with_cuda=with_cuda,\n   1519             is_standalone=is_standalone)\n   1520 finally:\n   1521     baton.release()\n\nFile ~/mambaforge/envs/cfast/lib/python3.11/site-packages/torch/utils/cpp_extension.py:1601, in _write_ninja_file_and_build_library(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_standalone)\n   1599 if with_cuda is None:\n   1600     with_cuda = any(map(_is_cuda_file, sources))\n-&gt; 1601 extra_ldflags = _prepare_ldflags(\n   1602     extra_ldflags or [],\n   1603     with_cuda,\n   1604     verbose,\n   1605     is_standalone)\n   1606 build_file_path = os.path.join(build_directory, 'build.ninja')\n   1607 if verbose:\n\nFile ~/mambaforge/envs/cfast/lib/python3.11/site-packages/torch/utils/cpp_extension.py:1699, in _prepare_ldflags(extra_ldflags, with_cuda, verbose, is_standalone)\n   1697         extra_ldflags.append(f'/LIBPATH:{os.path.join(CUDNN_HOME, \"lib\", \"x64\")}')\n   1698 elif not IS_HIP_EXTENSION:\n-&gt; 1699     extra_ldflags.append(f'-L{_join_cuda_home(\"lib64\")}')\n   1700     extra_ldflags.append('-lcudart')\n   1701     if CUDNN_HOME is not None:\n\nFile ~/mambaforge/envs/cfast/lib/python3.11/site-packages/torch/utils/cpp_extension.py:2223, in _join_cuda_home(*paths)\n   2216 r'''\n   2217 Joins paths with CUDA_HOME, or raises an error if it CUDA_HOME is not set.\n   2218 \n   2219 This is basically a lazy way of raising an error for missing $CUDA_HOME\n   2220 only once we need to get any CUDA-specific path.\n   2221 '''\n   2222 if CUDA_HOME is None:\n-&gt; 2223     raise EnvironmentError('CUDA_HOME environment variable is not set. '\n   2224                            'Please set it to your CUDA install root.')\n   2225 return os.path.join(CUDA_HOME, *paths)\n\nOSError: CUDA_HOME environment variable is not set. Please set it to your CUDA install root.\n\n\n\n\nm1c,m2c = m1.contiguous().cuda(), m2.contiguous().cuda()\n\n\ntorch.isclose(tr,module.matmul(m1c, m2c).cpu(), atol=1e-5).all()\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[66], line 1\n----&gt; 1 torch.isclose(tr,module.matmul(m1c, m2c).cpu(), atol=1e-5).all()\n\nNameError: name 'module' is not defined\n\n\n\n\nres=module.matmul(m1c, m2c).cpu()\nres.shape\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nFile &lt;timed exec&gt;:1\n\nNameError: name 'module' is not defined\n\n\n\n\n\nPytorch\n\ntorch.isclose(tr,(m1c@m2c).cpu(), atol=1e-5).all()\n\ntensor(True)\n\n\n\n\n\nThe slowest run took 11.09 times longer than the fastest. This could mean that an intermediate result is being cached.\n5.98 ms ± 7.14 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\n\nRGB-&gt;Grey CUDA 3d\n\ncuda_src = cuda_begin + r'''\n__global__ void rgb_to_grayscale_kernel(unsigned char* x, unsigned char* out, int w, int h) {\n    int c = blockIdx.x*blockDim.x + threadIdx.x;\n    int r = blockIdx.y*blockDim.y + threadIdx.y;\n\n    if (c&lt;w && r&lt;h) {\n        int i = r*w + c;\n        int n = h*w;\n        out[i] = 0.2989*x[i] + 0.5870*x[i+n] + 0.1140*x[i+2*n];\n    }\n}\n\ntorch::Tensor rgb_to_grayscale(torch::Tensor input) {\n    CHECK_INPUT(input);\n    int h = input.size(1);\n    int w = input.size(2);\n    torch::Tensor output = torch::empty({h,w}, input.options());\n    dim3 tpb(16,16);\n    dim3 blocks(cdiv(w, tpb.x), cdiv(h, tpb.y));\n    rgb_to_grayscale_kernel&lt;&lt;&lt;blocks, tpb&gt;&gt;&gt;(\n        input.data_ptr&lt;unsigned char&gt;(), output.data_ptr&lt;unsigned char&gt;(), w, h);\n    C10_CUDA_KERNEL_LAUNCH_CHECK();\n    return output;\n}'''\n\n\nmodule = load_cuda(cuda_src, cpp_src, ['rgb_to_grayscale'])\n\n\n---------------------------------------------------------------------------\nOSError                                   Traceback (most recent call last)\nCell In[71], line 1\n----&gt; 1 module = load_cuda(cuda_src, cpp_src, ['rgb_to_grayscale'])\n\nCell In[29], line 2, in load_cuda(cuda_src, cpp_src, funcs, opt, verbose)\n      1 def load_cuda(cuda_src, cpp_src, funcs, opt=False, verbose=False):\n----&gt; 2     return load_inline(cuda_sources=[cuda_src], cpp_sources=[cpp_src], functions=funcs,\n      3                        extra_cuda_cflags=[\"-O2\"] if opt else [], verbose=verbose, name=\"inline_ext\")\n\nFile ~/mambaforge/envs/cfast/lib/python3.11/site-packages/torch/utils/cpp_extension.py:1434, in load_inline(name, cpp_sources, cuda_sources, functions, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_python_module, with_pytorch_error_handling, keep_intermediates)\n   1430         cuda_source_file.write('\\n'.join(cuda_sources))\n   1432     sources.append(cuda_source_path)\n-&gt; 1434 return _jit_compile(\n   1435     name,\n   1436     sources,\n   1437     extra_cflags,\n   1438     extra_cuda_cflags,\n   1439     extra_ldflags,\n   1440     extra_include_paths,\n   1441     build_directory,\n   1442     verbose,\n   1443     with_cuda,\n   1444     is_python_module,\n   1445     is_standalone=False,\n   1446     keep_intermediates=keep_intermediates)\n\nFile ~/mambaforge/envs/cfast/lib/python3.11/site-packages/torch/utils/cpp_extension.py:1509, in _jit_compile(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_python_module, is_standalone, keep_intermediates)\n   1505                 hipified_sources.add(hipify_result[s_abs][\"hipified_path\"] if s_abs in hipify_result else s_abs)\n   1507             sources = list(hipified_sources)\n-&gt; 1509         _write_ninja_file_and_build_library(\n   1510             name=name,\n   1511             sources=sources,\n   1512             extra_cflags=extra_cflags or [],\n   1513             extra_cuda_cflags=extra_cuda_cflags or [],\n   1514             extra_ldflags=extra_ldflags or [],\n   1515             extra_include_paths=extra_include_paths or [],\n   1516             build_directory=build_directory,\n   1517             verbose=verbose,\n   1518             with_cuda=with_cuda,\n   1519             is_standalone=is_standalone)\n   1520 finally:\n   1521     baton.release()\n\nFile ~/mambaforge/envs/cfast/lib/python3.11/site-packages/torch/utils/cpp_extension.py:1601, in _write_ninja_file_and_build_library(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_standalone)\n   1599 if with_cuda is None:\n   1600     with_cuda = any(map(_is_cuda_file, sources))\n-&gt; 1601 extra_ldflags = _prepare_ldflags(\n   1602     extra_ldflags or [],\n   1603     with_cuda,\n   1604     verbose,\n   1605     is_standalone)\n   1606 build_file_path = os.path.join(build_directory, 'build.ninja')\n   1607 if verbose:\n\nFile ~/mambaforge/envs/cfast/lib/python3.11/site-packages/torch/utils/cpp_extension.py:1699, in _prepare_ldflags(extra_ldflags, with_cuda, verbose, is_standalone)\n   1697         extra_ldflags.append(f'/LIBPATH:{os.path.join(CUDNN_HOME, \"lib\", \"x64\")}')\n   1698 elif not IS_HIP_EXTENSION:\n-&gt; 1699     extra_ldflags.append(f'-L{_join_cuda_home(\"lib64\")}')\n   1700     extra_ldflags.append('-lcudart')\n   1701     if CUDNN_HOME is not None:\n\nFile ~/mambaforge/envs/cfast/lib/python3.11/site-packages/torch/utils/cpp_extension.py:2223, in _join_cuda_home(*paths)\n   2216 r'''\n   2217 Joins paths with CUDA_HOME, or raises an error if it CUDA_HOME is not set.\n   2218 \n   2219 This is basically a lazy way of raising an error for missing $CUDA_HOME\n   2220 only once we need to get any CUDA-specific path.\n   2221 '''\n   2222 if CUDA_HOME is None:\n-&gt; 2223     raise EnvironmentError('CUDA_HOME environment variable is not set. '\n   2224                            'Please set it to your CUDA install root.')\n   2225 return os.path.join(CUDA_HOME, *paths)\n\nOSError: CUDA_HOME environment variable is not set. Please set it to your CUDA install root.\n\n\n\n\nres = module.rgb_to_grayscale(imgc).cpu()\nshow_img(res, cmap='gray')\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[72], line 1\n----&gt; 1 res = module.rgb_to_grayscale(imgc).cpu()\n      2 show_img(res, cmap='gray')\n\nNameError: name 'module' is not defined",
    "crumbs": [
      "Blog",
      "CUDA"
    ]
  },
  {
    "objectID": "pca.html",
    "href": "pca.html",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "from sklearn.datasets import load_digits\nimport pandas as pd\n\ndataset = load_digits()\ndataset.keys()\n\ndict_keys(['data', 'target', 'frame', 'feature_names', 'target_names', 'images', 'DESCR'])\n\n\n\ndataset.data.shape\n\n(1797, 64)\n\n\n\ndataset.data[0]\n\narray([ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.,  0.,  0., 13., 15., 10.,\n       15.,  5.,  0.,  0.,  3., 15.,  2.,  0., 11.,  8.,  0.,  0.,  4.,\n       12.,  0.,  0.,  8.,  8.,  0.,  0.,  5.,  8.,  0.,  0.,  9.,  8.,\n        0.,  0.,  4., 11.,  0.,  1., 12.,  7.,  0.,  0.,  2., 14.,  5.,\n       10., 12.,  0.,  0.,  0.,  0.,  6., 13., 10.,  0.,  0.,  0.])\n\n\n\ndataset.data[0].reshape(8,8)\n\narray([[ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.],\n       [ 0.,  0., 13., 15., 10., 15.,  5.,  0.],\n       [ 0.,  3., 15.,  2.,  0., 11.,  8.,  0.],\n       [ 0.,  4., 12.,  0.,  0.,  8.,  8.,  0.],\n       [ 0.,  5.,  8.,  0.,  0.,  9.,  8.,  0.],\n       [ 0.,  4., 11.,  0.,  1., 12.,  7.,  0.],\n       [ 0.,  2., 14.,  5., 10., 12.,  0.,  0.],\n       [ 0.,  0.,  6., 13., 10.,  0.,  0.,  0.]])\n\n\n\nfrom matplotlib import pyplot as plt\n\nplt.gray()\nplt.matshow(dataset.data[0].reshape(8,8))\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\nplt.matshow(dataset.data[9].reshape(8,8))\n\n\n\n\n\n\n\n\n\ndataset.target[:5]\n\narray([0, 1, 2, 3, 4])\n\n\n\ndf = pd.DataFrame(dataset.data, columns=dataset.feature_names)\ndf.head()\n\n\n\n\n\n\n\n\npixel_0_0\npixel_0_1\npixel_0_2\npixel_0_3\npixel_0_4\npixel_0_5\npixel_0_6\npixel_0_7\npixel_1_0\npixel_1_1\n...\npixel_6_6\npixel_6_7\npixel_7_0\npixel_7_1\npixel_7_2\npixel_7_3\npixel_7_4\npixel_7_5\npixel_7_6\npixel_7_7\n\n\n\n\n0\n0.0\n0.0\n5.0\n13.0\n9.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n6.0\n13.0\n10.0\n0.0\n0.0\n0.0\n\n\n1\n0.0\n0.0\n0.0\n12.0\n13.0\n5.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n11.0\n16.0\n10.0\n0.0\n0.0\n\n\n2\n0.0\n0.0\n0.0\n4.0\n15.0\n12.0\n0.0\n0.0\n0.0\n0.0\n...\n5.0\n0.0\n0.0\n0.0\n0.0\n3.0\n11.0\n16.0\n9.0\n0.0\n\n\n3\n0.0\n0.0\n7.0\n15.0\n13.0\n1.0\n0.0\n0.0\n0.0\n8.0\n...\n9.0\n0.0\n0.0\n0.0\n7.0\n13.0\n13.0\n9.0\n0.0\n0.0\n\n\n4\n0.0\n0.0\n0.0\n1.0\n11.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n2.0\n16.0\n4.0\n0.0\n0.0\n\n\n\n\n5 rows × 64 columns\n\n\n\n\ndataset.target\n\narray([0, 1, 2, ..., 8, 9, 8])\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\npixel_0_0\npixel_0_1\npixel_0_2\npixel_0_3\npixel_0_4\npixel_0_5\npixel_0_6\npixel_0_7\npixel_1_0\npixel_1_1\n...\npixel_6_6\npixel_6_7\npixel_7_0\npixel_7_1\npixel_7_2\npixel_7_3\npixel_7_4\npixel_7_5\npixel_7_6\npixel_7_7\n\n\n\n\ncount\n1797.0\n1797.000000\n1797.000000\n1797.000000\n1797.000000\n1797.000000\n1797.000000\n1797.000000\n1797.000000\n1797.000000\n...\n1797.000000\n1797.000000\n1797.000000\n1797.000000\n1797.000000\n1797.000000\n1797.000000\n1797.000000\n1797.000000\n1797.000000\n\n\nmean\n0.0\n0.303840\n5.204786\n11.835838\n11.848080\n5.781859\n1.362270\n0.129661\n0.005565\n1.993879\n...\n3.725097\n0.206455\n0.000556\n0.279354\n5.557596\n12.089037\n11.809126\n6.764051\n2.067891\n0.364496\n\n\nstd\n0.0\n0.907192\n4.754826\n4.248842\n4.287388\n5.666418\n3.325775\n1.037383\n0.094222\n3.196160\n...\n4.919406\n0.984401\n0.023590\n0.934302\n5.103019\n4.374694\n4.933947\n5.900623\n4.090548\n1.860122\n\n\nmin\n0.0\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.0\n0.000000\n1.000000\n10.000000\n10.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n11.000000\n10.000000\n0.000000\n0.000000\n0.000000\n\n\n50%\n0.0\n0.000000\n4.000000\n13.000000\n13.000000\n4.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n0.000000\n0.000000\n0.000000\n4.000000\n13.000000\n14.000000\n6.000000\n0.000000\n0.000000\n\n\n75%\n0.0\n0.000000\n9.000000\n15.000000\n15.000000\n11.000000\n0.000000\n0.000000\n0.000000\n3.000000\n...\n7.000000\n0.000000\n0.000000\n0.000000\n10.000000\n16.000000\n16.000000\n12.000000\n2.000000\n0.000000\n\n\nmax\n0.0\n8.000000\n16.000000\n16.000000\n16.000000\n16.000000\n16.000000\n15.000000\n2.000000\n16.000000\n...\n16.000000\n13.000000\n1.000000\n9.000000\n16.000000\n16.000000\n16.000000\n16.000000\n16.000000\n16.000000\n\n\n\n\n8 rows × 64 columns\n\n\n\n\nX = df\ny = dataset.target\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nX_scaled\n\narray([[ 0.        , -0.33501649, -0.04308102, ..., -1.14664746,\n        -0.5056698 , -0.19600752],\n       [ 0.        , -0.33501649, -1.09493684, ...,  0.54856067,\n        -0.5056698 , -0.19600752],\n       [ 0.        , -0.33501649, -1.09493684, ...,  1.56568555,\n         1.6951369 , -0.19600752],\n       ...,\n       [ 0.        , -0.33501649, -0.88456568, ..., -0.12952258,\n        -0.5056698 , -0.19600752],\n       [ 0.        , -0.33501649, -0.67419451, ...,  0.8876023 ,\n        -0.5056698 , -0.19600752],\n       [ 0.        , -0.33501649,  1.00877481, ...,  0.8876023 ,\n        -0.26113572, -0.19600752]])\n\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=30)\n\n\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\nmodel.score(X_test, y_test)\n\n0.9722222222222222\n\n\n\nUse PCA to reduce dimensions\n\n\nX\n\n\n\n\n\n\n\n\npixel_0_0\npixel_0_1\npixel_0_2\npixel_0_3\npixel_0_4\npixel_0_5\npixel_0_6\npixel_0_7\npixel_1_0\npixel_1_1\n...\npixel_6_6\npixel_6_7\npixel_7_0\npixel_7_1\npixel_7_2\npixel_7_3\npixel_7_4\npixel_7_5\npixel_7_6\npixel_7_7\n\n\n\n\n0\n0.0\n0.0\n5.0\n13.0\n9.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n6.0\n13.0\n10.0\n0.0\n0.0\n0.0\n\n\n1\n0.0\n0.0\n0.0\n12.0\n13.0\n5.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n11.0\n16.0\n10.0\n0.0\n0.0\n\n\n2\n0.0\n0.0\n0.0\n4.0\n15.0\n12.0\n0.0\n0.0\n0.0\n0.0\n...\n5.0\n0.0\n0.0\n0.0\n0.0\n3.0\n11.0\n16.0\n9.0\n0.0\n\n\n3\n0.0\n0.0\n7.0\n15.0\n13.0\n1.0\n0.0\n0.0\n0.0\n8.0\n...\n9.0\n0.0\n0.0\n0.0\n7.0\n13.0\n13.0\n9.0\n0.0\n0.0\n\n\n4\n0.0\n0.0\n0.0\n1.0\n11.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n2.0\n16.0\n4.0\n0.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1792\n0.0\n0.0\n4.0\n10.0\n13.0\n6.0\n0.0\n0.0\n0.0\n1.0\n...\n4.0\n0.0\n0.0\n0.0\n2.0\n14.0\n15.0\n9.0\n0.0\n0.0\n\n\n1793\n0.0\n0.0\n6.0\n16.0\n13.0\n11.0\n1.0\n0.0\n0.0\n0.0\n...\n1.0\n0.0\n0.0\n0.0\n6.0\n16.0\n14.0\n6.0\n0.0\n0.0\n\n\n1794\n0.0\n0.0\n1.0\n11.0\n15.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n2.0\n9.0\n13.0\n6.0\n0.0\n0.0\n\n\n1795\n0.0\n0.0\n2.0\n10.0\n7.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n2.0\n0.0\n0.0\n0.0\n5.0\n12.0\n16.0\n12.0\n0.0\n0.0\n\n\n1796\n0.0\n0.0\n10.0\n14.0\n8.0\n1.0\n0.0\n0.0\n0.0\n2.0\n...\n8.0\n0.0\n0.0\n1.0\n8.0\n12.0\n14.0\n12.0\n1.0\n0.0\n\n\n\n\n1797 rows × 64 columns\n\n\n\n\nUse components such that 95% of variance is retained\n\n\nfrom sklearn.decomposition import PCA\n\npca = PCA(0.95)\nX_pca = pca.fit_transform(X)\nX_pca.shape\n\n(1797, 29)\n\n\n\npca.explained_variance_ratio_\n\narray([0.14890594, 0.13618771, 0.11794594, 0.08409979, 0.05782415,\n       0.0491691 , 0.04315987, 0.03661373, 0.03353248, 0.03078806,\n       0.02372341, 0.02272697, 0.01821863, 0.01773855, 0.01467101,\n       0.01409716, 0.01318589, 0.01248138, 0.01017718, 0.00905617,\n       0.00889538, 0.00797123, 0.00767493, 0.00722904, 0.00695889,\n       0.00596081, 0.00575615, 0.00515158, 0.0048954 ])\n\n\n\npca.n_components_\n\n29\n\n\nPCA created 29 components out of 64 original columns\n\nX_pca\n\narray([[ -1.25946645,  21.27488348,  -9.46305462, ...,   3.67072108,\n         -0.9436689 ,  -1.13250195],\n       [  7.9576113 , -20.76869896,   4.43950604, ...,   2.18261819,\n         -0.51022719,   2.31354911],\n       [  6.99192297,  -9.95598641,   2.95855808, ...,   4.22882114,\n          2.1576573 ,   0.8379578 ],\n       ...,\n       [ 10.8012837 ,  -6.96025223,   5.59955453, ...,  -3.56866194,\n          1.82444444,   3.53885886],\n       [ -4.87210009,  12.42395362, -10.17086635, ...,   3.25330054,\n          0.95484174,  -0.93895602],\n       [ -0.34438963,   6.36554919,  10.77370849, ...,  -3.01636722,\n          1.29752723,   2.58810313]])\n\n\n\nX_train_pca, X_test_pca, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=30)\n\n\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(X_train_pca, y_train)\nmodel.score(X_test_pca, y_test)\n\n0.9694444444444444\n\n\nLet’s now select only two components\n\npca = PCA(n_components=16)\nX_pca = pca.fit_transform(X)\nX_pca.shape\n\n(1797, 16)\n\n\n\nX_pca\n\narray([[ -1.2594598 ,  21.27487967,  -9.46303989, ...,  -3.31223438,\n          6.02583852,   2.68741732],\n       [  7.95761297, -20.7686941 ,   4.43950445, ...,  -6.50507014,\n         -2.27310577,  -2.14622234],\n       [  6.99193119,  -9.95600512,   2.95858793, ...,   6.83484372,\n         -1.65925951,  -4.25884236],\n       ...,\n       [ 10.80128998,  -6.96027365,   5.5995876 , ...,   1.24767177,\n          2.62633682,  -5.79460334],\n       [ -4.87210168,  12.42396132, -10.17087749, ...,  -1.21624842,\n         10.77743555,   2.38010911],\n       [ -0.34438951,   6.36554076,  10.77371718, ...,   7.0540168 ,\n          0.72142473,   0.27006973]])\n\n\n\npca.explained_variance_ratio_\n\narray([0.14890594, 0.13618771, 0.11794594, 0.08409979, 0.05782415,\n       0.0491691 , 0.04315987, 0.03661373, 0.03353248, 0.03078806,\n       0.02372336, 0.02272696, 0.01821856, 0.01773823, 0.01466903,\n       0.01409675])\n\n\nYou can see that both combined retains 0.14+0.13=0.27 or 27% of important feature information\n\nX_train_pca, X_test_pca, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=30)\n\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(X_train_pca, y_train)\nmodel.score(X_test_pca, y_test)\n\n0.9472222222222222\n\n\nWe get less accuancy (~60%) as using only 2 components did not retain much of the feature information. However in real life you will find many cases where using 2 or few PCA components can still give you a pretty good accuracy\n\n\n\n Back to top",
    "crumbs": [
      "Blog",
      "Principal Component Analysis"
    ]
  },
  {
    "objectID": "k-nn.html",
    "href": "k-nn.html",
    "title": "K-NN",
    "section": "",
    "text": "def sayhello(name): return f'Hello {name}'",
    "crumbs": [
      "Blog",
      "K-NN"
    ]
  },
  {
    "objectID": "k-nn.html#advantages",
    "href": "k-nn.html#advantages",
    "title": "K-NN",
    "section": "Advantages",
    "text": "Advantages\n\nEasy to implement: Given the algorithm’s simplicity and accuracy, it is one of the first classifiers that a new data scientist will learn.\nAdapts easily: As new training samples are added, the algorithm adjusts to account for any new data since all training data is stored into memory.\nFew hyperparameters: KNN only requires a k value and a distance metric, which is low when compared to other machine learning algorithms.",
    "crumbs": [
      "Blog",
      "K-NN"
    ]
  },
  {
    "objectID": "k-nn.html#disadvantages",
    "href": "k-nn.html#disadvantages",
    "title": "K-NN",
    "section": "Disadvantages",
    "text": "Disadvantages\n\nDoes not scale well: Since KNN is a lazy algorithm, it takes up more memory and data storage compared to other classifiers. This can be costly from both a time and money perspective. More memory and storage will drive up business expenses and more data can take longer to compute. While different data structures, such as Ball-Tree, have been created to address the computational inefficiencies, a different classifier may be ideal depending on the business problem.\nCurse of dimensionality: The KNN algorithm tends to fall victim to the curse of dimensionality, which means that it doesn’t perform well with high-dimensional data inputs. This is sometimes also referred to as the peaking phenomenon (PDF, 340 MB) (link resides outside of ibm.com), where after the algorithm attains the optimal number of features, additional features increases the amount of classification errors, especially when the sample size is smaller.\nProne to overfitting: Due to the “curse of dimensionality”, KNN is also more prone to overfitting. While feature selection and dimensionality reduction techniques are leveraged to prevent this from occurring, the value of k can also impact the model’s behavior. Lower values of k can overfit the data, whereas higher values of k tend to “smooth out” the prediction values since it is averaging the values over a greater area, or neighborhood. However, if the value of k is too high, then it can underfit the data.",
    "crumbs": [
      "Blog",
      "K-NN"
    ]
  },
  {
    "objectID": "k-nn.html#distance-metrics",
    "href": "k-nn.html#distance-metrics",
    "title": "K-NN",
    "section": "Distance Metrics",
    "text": "Distance Metrics\n\nEuclidean distance (p=2): This is the most commonly used distance measure\nManhattan distance (p=1): This is also another popular distance metric, which measures the absolute value between two points.\nMinkowski distance: This distance measure is the generalized form of Euclidean and Manhattan distance metrics.\nHamming distance: This technique is used typically used with Boolean or string vectors, identifying the points where the vectors do not match.",
    "crumbs": [
      "Blog",
      "K-NN"
    ]
  },
  {
    "objectID": "k-nn.html#compute-knn-defining-k",
    "href": "k-nn.html#compute-knn-defining-k",
    "title": "K-NN",
    "section": "Compute KNN: defining k",
    "text": "Compute KNN: defining k\nThe k value in the k-NN algorithm defines how many neighbors will be checked to determine the classification of a specific query point. For example, if k=1, the instance will be assigned to the same class as its single nearest neighbor. Defining k can be a balancing act as different values can lead to overfitting or underfitting. Lower values of k can have high variance, but low bias, and larger values of k may lead to high bias and lower variance. The choice of k will largely depend on the input data as data with more outliers or noise will likely perform better with higher values of k. Overall, it is recommended to have an odd number for k to avoid ties in classification, and cross-validation tactics can help you choose the optimal k for your dataset.",
    "crumbs": [
      "Blog",
      "K-NN"
    ]
  },
  {
    "objectID": "k-nn.html#data-collection",
    "href": "k-nn.html#data-collection",
    "title": "K-NN",
    "section": "Data collection",
    "text": "Data collection\n\n!pip list | grep pandas\n!pip list | grep scikit-learn\n\npandas                        2.0.3\nscikit-learn                  1.3.0\n\n\n\nimport pandas as pd\nfrom sklearn.datasets import load_iris\n\n\niris = load_iris()\n\n\niris.feature_names\n\n['sepal length (cm)',\n 'sepal width (cm)',\n 'petal length (cm)',\n 'petal width (cm)']\n\n\n\niris.target_names\n\narray(['setosa', 'versicolor', 'virginica'], dtype='&lt;U10')\n\n\n\niris.target\n\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n\n\n\ndf = pd.DataFrame(iris.data,columns=iris.feature_names)\ndf.head()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n4.9\n3.0\n1.4\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\n\n\n3\n4.6\n3.1\n1.5\n0.2\n\n\n4\n5.0\n3.6\n1.4\n0.2\n\n\n\n\n\n\n\n\ndf['target'] = iris.target\ndf.head()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n0\n\n\n1\n4.9\n3.0\n1.4\n0.2\n0\n\n\n2\n4.7\n3.2\n1.3\n0.2\n0\n\n\n3\n4.6\n3.1\n1.5\n0.2\n0\n\n\n4\n5.0\n3.6\n1.4\n0.2\n0\n\n\n\n\n\n\n\n\ndf[df.target==1].head()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\n50\n7.0\n3.2\n4.7\n1.4\n1\n\n\n51\n6.4\n3.2\n4.5\n1.5\n1\n\n\n52\n6.9\n3.1\n4.9\n1.5\n1\n\n\n53\n5.5\n2.3\n4.0\n1.3\n1\n\n\n54\n6.5\n2.8\n4.6\n1.5\n1\n\n\n\n\n\n\n\n\ndf[df.target==2].head()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\n100\n6.3\n3.3\n6.0\n2.5\n2\n\n\n101\n5.8\n2.7\n5.1\n1.9\n2\n\n\n102\n7.1\n3.0\n5.9\n2.1\n2\n\n\n103\n6.3\n2.9\n5.6\n1.8\n2\n\n\n104\n6.5\n3.0\n5.8\n2.2\n2\n\n\n\n\n\n\n\n\ndf['flower_name'] =df.target.apply(lambda x: iris.target_names[x])\ndf.head()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\nflower_name\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n0\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\n0\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\n0\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\n0\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\n0\nsetosa\n\n\n\n\n\n\n\n\ndf0 = df[:50]\ndf1 = df[50:100]\ndf2 = df[100:]",
    "crumbs": [
      "Blog",
      "K-NN"
    ]
  },
  {
    "objectID": "k-nn.html#data-visuals",
    "href": "k-nn.html#data-visuals",
    "title": "K-NN",
    "section": "Data Visuals",
    "text": "Data Visuals\n\nimport matplotlib.pyplot as plt\n\n\nplt.xlabel('Sepal Length')\nplt.ylabel('Sepal Width')\nplt.scatter(df0['sepal length (cm)'],\n            df0['sepal width (cm)'],\n            color=\"green\",marker='+')\n\nplt.scatter(df1['sepal length (cm)'],\n            df1['sepal width (cm)'],\n            color=\"blue\",marker='.')\n\nplt.scatter(df2['sepal length (cm)'],\n            df2['sepal width (cm)'],\n            color=\"red\",marker='*')\n\n\n\n\n\n\n\n\n\nplt.xlabel('Petal Length')\nplt.ylabel('Petal Width')\nplt.scatter(df0['petal length (cm)'],\n            df0['petal width (cm)'],\n            color=\"green\",marker='+')\n\nplt.scatter(df1['petal length (cm)'],\n            df1['petal width (cm)'],\n            color=\"blue\",marker='.')\n\nplt.scatter(df2['sepal length (cm)'],\n            df2['sepal width (cm)'],\n            color=\"red\",marker='*')",
    "crumbs": [
      "Blog",
      "K-NN"
    ]
  },
  {
    "objectID": "k-nn.html#model-design",
    "href": "k-nn.html#model-design",
    "title": "K-NN",
    "section": "Model Design",
    "text": "Model Design\n\nfrom sklearn.model_selection import train_test_split\n\n\nX = df.drop(['target','flower_name'], axis='columns')\ny = df.target\n\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,\n                                                    test_size=0.2,\n                                                    random_state=1)\n\n\nfrom sklearn.preprocessing import StandardScaler\n\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n\nlen(X_train)\n\n120\n\n\n\nlen(X_test)\n\n30",
    "crumbs": [
      "Blog",
      "K-NN"
    ]
  },
  {
    "objectID": "k-nn.html#first-model-k-nn-10",
    "href": "k-nn.html#first-model-k-nn-10",
    "title": "K-NN",
    "section": "First Model: K-nn 10",
    "text": "First Model: K-nn 10\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\nn_neighbors=10\nknn = KNeighborsClassifier(n_neighbors)\nknn.fit(X_train, y_train)\n\nKNeighborsClassifier(n_neighbors=10)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifierKNeighborsClassifier(n_neighbors=10)\n\n\n\nknn.score(X_test, y_test)\n\n1.0\n\n\n\nknn.predict([[4.8,3.0,1.5,0.3]])\n\narray([2])\n\n\n\nfrom sklearn.metrics import confusion_matrix\n\n\ny_pred = knn.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\ncm\n\narray([[11,  0,  0],\n       [ 0, 13,  0],\n       [ 0,  0,  6]])\n\n\n\n!pip list | grep seaborn || pip install seaborn\n\nseaborn                       0.12.2\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sn\n\n\nplt.figure(figsize=(7,5))\nsn.heatmap(cm, annot=True)\nplt.xlabel('Predicted')\nplt.ylabel('Truth')\n\nText(58.222222222222214, 0.5, 'Truth')\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import classification_report\n\n\nprint(classification_report(y_test, y_pred))\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        11\n           1       1.00      1.00      1.00        13\n           2       1.00      1.00      1.00         6\n\n    accuracy                           1.00        30\n   macro avg       1.00      1.00      1.00        30\nweighted avg       1.00      1.00      1.00        30\n\n\n\n\nfrom matplotlib.colors import ListedColormap\nimport numpy as np\n\n\n# Plot the decision regions\ncmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\ncmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n\n# Define the resolution of the grid\nh = 0.02\nx_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\ny_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\nx1_min, x1_max = X_train[:, 2].min() - 1, X_train[:, 2].max() + 1\ny1_min, y1_max = X_train[:, 3].min() - 1, X_train[:, 3].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nxx1, yy1 = np.meshgrid(np.arange(x1_min, x1_max, h), np.arange(y1_min, y1_max, h))\n\n\n# Predict the class labels for each point in the grid\nZ = knn.predict(np.c_[xx.ravel(), yy.ravel(),xx.ravel(), yy.ravel() ])\nZ = Z.reshape(xx.shape)\n\nprint(f'{len(xx)} {len(yy)} {len(xx1)} {len(yy1)} ')\n\n387 387 256 256 \n\n\n\n# Plot the decision regions\nplt.figure()\nplt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n# Plot the training points\nplt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cmap_bold,\n            edgecolor='k', s=20)\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.title(\"k-NN classifier with k={}\".format(10))\n\n\nplt.show()\n\n\n\n\n\n\n\n\n\nlen(xx1.ravel())\n\n68608\n\n\n\n# Plot the decision regions\nplt.figure()\nplt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n# Plot the training points\nplt.scatter(X_train[:, 2], X_train[:, 3], c=y_train, cmap=cmap_bold,\n            edgecolor='k', s=20)\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.title(\"k-NN classifier with k={}\".format(10))\n\nText(0.5, 1.0, 'k-NN classifier with k=10')",
    "crumbs": [
      "Blog",
      "K-NN"
    ]
  },
  {
    "objectID": "k-nn.html#knn-model-with-gridsearchcv-finding-optimum-k",
    "href": "k-nn.html#knn-model-with-gridsearchcv-finding-optimum-k",
    "title": "K-NN",
    "section": "Knn model with GridSearchCV: finding optimum K",
    "text": "Knn model with GridSearchCV: finding optimum K\n\nfrom sklearn.model_selection import GridSearchCV\n\n\ncreate a dictionary of all values we want to test for n_neighbors\n\n\nknn2 = KNeighborsClassifier()\nparam_grid = {'n_neighbors': np.arange(1, 25)}\n\n\nuse gridsearch to test all values for n_neighbors\n\n\nknn_gscv = GridSearchCV(knn2, param_grid, cv=5)#fit model to data\nknn_gscv.fit(X, y)\n\nGridSearchCV(cv=5, estimator=KNeighborsClassifier(),\n             param_grid={'n_neighbors': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24])})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5, estimator=KNeighborsClassifier(),\n             param_grid={'n_neighbors': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24])})estimator: KNeighborsClassifierKNeighborsClassifier()KNeighborsClassifierKNeighborsClassifier()\n\n\n\n#check top performing n_neighbors value\nknn_gscv.best_params_\n\n{'n_neighbors': 6}",
    "crumbs": [
      "Blog",
      "K-NN"
    ]
  },
  {
    "objectID": "hyperparameteroptimization.html",
    "href": "hyperparameteroptimization.html",
    "title": "Hyper Parameter Optimization",
    "section": "",
    "text": "For iris flower dataset in sklearn library, we are going to find out best model and best hyper parameters using GridSearchCV\nLoad iris flower dataset\nfrom sklearn import svm, datasets\niris = datasets.load_iris()\nimport pandas as pd\ndf = pd.DataFrame(iris.data,columns=iris.feature_names)\ndf['flower'] = iris.target\ndf['flower'] = df['flower'].apply(lambda x: iris.target_names[x])\ndf[47:150]\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\nflower\n\n\n\n\n47\n4.6\n3.2\n1.4\n0.2\nsetosa\n\n\n48\n5.3\n3.7\n1.5\n0.2\nsetosa\n\n\n49\n5.0\n3.3\n1.4\n0.2\nsetosa\n\n\n50\n7.0\n3.2\n4.7\n1.4\nversicolor\n\n\n51\n6.4\n3.2\n4.5\n1.5\nversicolor\n\n\n...\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n146\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n147\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n148\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n149\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n\n103 rows × 5 columns",
    "crumbs": [
      "Blog",
      "Hyper Parameter Optimization"
    ]
  },
  {
    "objectID": "hyperparameteroptimization.html#approach-1-use-train_test_split-and-manually-tune-parameters-by-trial-and-error",
    "href": "hyperparameteroptimization.html#approach-1-use-train_test_split-and-manually-tune-parameters-by-trial-and-error",
    "title": "Hyper Parameter Optimization",
    "section": "Approach 1: Use train_test_split and manually tune parameters by trial and error",
    "text": "Approach 1: Use train_test_split and manually tune parameters by trial and error\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3)\n\n\nmodel = svm.SVC(kernel='rbf',C=30,gamma='auto')\nmodel.fit(X_train,y_train)\nmodel.score(X_test, y_test)\n\n0.9555555555555556",
    "crumbs": [
      "Blog",
      "Hyper Parameter Optimization"
    ]
  },
  {
    "objectID": "hyperparameteroptimization.html#approach-2-use-k-fold-cross-validation",
    "href": "hyperparameteroptimization.html#approach-2-use-k-fold-cross-validation",
    "title": "Hyper Parameter Optimization",
    "section": "Approach 2: Use K Fold Cross validation",
    "text": "Approach 2: Use K Fold Cross validation\nManually try suppling models with different parameters to cross_val_score function with 5 fold cross validation\n\ncross_val_score(svm.SVC(kernel='linear',C=10,gamma='auto'),iris.data, iris.target, cv=5)\n\n\n\n\narray([1.        , 1.        , 0.9       , 0.96666667, 1.        ])\n\n\n\ncross_val_score(svm.SVC(kernel='rbf',C=10,gamma='auto'),iris.data, iris.target, cv=5)\n\n\n\n\narray([0.96666667, 1.        , 0.96666667, 0.96666667, 1.        ])\n\n\n\ncross_val_score(svm.SVC(kernel='rbf',C=20,gamma='auto'),iris.data, iris.target, cv=5)\n\n\n\n\narray([0.96666667, 1.        , 0.9       , 0.96666667, 1.        ])\n\n\nAbove approach is tiresome and very manual. We can use for loop as an alternative\n\nkernels = ['rbf', 'linear']\nC = [1,10,20]\navg_scores = {}\nfor kval in kernels:\n    for cval in C:\n        cv_scores = cross_val_score(svm.SVC(kernel=kval,C=cval,gamma='auto'),iris.data, iris.target, cv=5)\n        avg_scores[kval + '_' + str(cval)] = np.average(cv_scores)\n\navg_scores\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{'rbf_1': 0.9800000000000001,\n 'rbf_10': 0.9800000000000001,\n 'rbf_20': 0.9666666666666668,\n 'linear_1': 0.9800000000000001,\n 'linear_10': 0.9733333333333334,\n 'linear_20': 0.9666666666666666}\n\n\nFrom above results we can say that rbf with C=1 or 10 or linear with C=1 will give best performance",
    "crumbs": [
      "Blog",
      "Hyper Parameter Optimization"
    ]
  },
  {
    "objectID": "hyperparameteroptimization.html#approach-3-use-gridsearchcv",
    "href": "hyperparameteroptimization.html#approach-3-use-gridsearchcv",
    "title": "Hyper Parameter Optimization",
    "section": "Approach 3: Use GridSearchCV",
    "text": "Approach 3: Use GridSearchCV\nGridSearchCV does exactly same thing as for loop above but in a single line of code\n\nfrom sklearn.model_selection import GridSearchCV\nclf = GridSearchCV(svm.SVC(gamma='auto'), {\n    'C': [1,10,20],\n    'kernel': ['rbf','linear']\n}, cv=5, return_train_score=False)\nclf.fit(iris.data, iris.target)\nclf.cv_results_\n\n{'mean_fit_time': array([0.00118256, 0.00104566, 0.0007266 , 0.00084271, 0.00088511,\n        0.00065002]),\n 'std_fit_time': array([6.73577797e-04, 4.38131552e-04, 1.68876617e-04, 3.96551972e-04,\n        4.28567104e-04, 8.82425265e-05]),\n 'mean_score_time': array([0.00081396, 0.00048532, 0.00048227, 0.00054522, 0.00063434,\n        0.00044188]),\n 'std_score_time': array([4.28254909e-04, 1.10279724e-04, 1.84006103e-04, 1.59358051e-04,\n        3.89407295e-04, 6.20551122e-05]),\n 'param_C': masked_array(data=[1, 1, 10, 10, 20, 20],\n              mask=[False, False, False, False, False, False],\n        fill_value='?',\n             dtype=object),\n 'param_kernel': masked_array(data=['rbf', 'linear', 'rbf', 'linear', 'rbf', 'linear'],\n              mask=[False, False, False, False, False, False],\n        fill_value='?',\n             dtype=object),\n 'params': [{'C': 1, 'kernel': 'rbf'},\n  {'C': 1, 'kernel': 'linear'},\n  {'C': 10, 'kernel': 'rbf'},\n  {'C': 10, 'kernel': 'linear'},\n  {'C': 20, 'kernel': 'rbf'},\n  {'C': 20, 'kernel': 'linear'}],\n 'split0_test_score': array([0.96666667, 0.96666667, 0.96666667, 1.        , 0.96666667,\n        1.        ]),\n 'split1_test_score': array([1., 1., 1., 1., 1., 1.]),\n 'split2_test_score': array([0.96666667, 0.96666667, 0.96666667, 0.9       , 0.9       ,\n        0.9       ]),\n 'split3_test_score': array([0.96666667, 0.96666667, 0.96666667, 0.96666667, 0.96666667,\n        0.93333333]),\n 'split4_test_score': array([1., 1., 1., 1., 1., 1.]),\n 'mean_test_score': array([0.98      , 0.98      , 0.98      , 0.97333333, 0.96666667,\n        0.96666667]),\n 'std_test_score': array([0.01632993, 0.01632993, 0.01632993, 0.03887301, 0.03651484,\n        0.0421637 ]),\n 'rank_test_score': array([1, 1, 1, 4, 5, 6], dtype=int32)}\n\n\n\ndf = pd.DataFrame(clf.cv_results_)\ndf\n\n\n\n\n\n\n\n\nmean_fit_time\nstd_fit_time\nmean_score_time\nstd_score_time\nparam_C\nparam_kernel\nparams\nsplit0_test_score\nsplit1_test_score\nsplit2_test_score\nsplit3_test_score\nsplit4_test_score\nmean_test_score\nstd_test_score\nrank_test_score\n\n\n\n\n0\n0.001183\n0.000674\n0.000814\n0.000428\n1\nrbf\n{'C': 1, 'kernel': 'rbf'}\n0.966667\n1.0\n0.966667\n0.966667\n1.0\n0.980000\n0.016330\n1\n\n\n1\n0.001046\n0.000438\n0.000485\n0.000110\n1\nlinear\n{'C': 1, 'kernel': 'linear'}\n0.966667\n1.0\n0.966667\n0.966667\n1.0\n0.980000\n0.016330\n1\n\n\n2\n0.000727\n0.000169\n0.000482\n0.000184\n10\nrbf\n{'C': 10, 'kernel': 'rbf'}\n0.966667\n1.0\n0.966667\n0.966667\n1.0\n0.980000\n0.016330\n1\n\n\n3\n0.000843\n0.000397\n0.000545\n0.000159\n10\nlinear\n{'C': 10, 'kernel': 'linear'}\n1.000000\n1.0\n0.900000\n0.966667\n1.0\n0.973333\n0.038873\n4\n\n\n4\n0.000885\n0.000429\n0.000634\n0.000389\n20\nrbf\n{'C': 20, 'kernel': 'rbf'}\n0.966667\n1.0\n0.900000\n0.966667\n1.0\n0.966667\n0.036515\n5\n\n\n5\n0.000650\n0.000088\n0.000442\n0.000062\n20\nlinear\n{'C': 20, 'kernel': 'linear'}\n1.000000\n1.0\n0.900000\n0.933333\n1.0\n0.966667\n0.042164\n6\n\n\n\n\n\n\n\n\ndf[['param_C','param_kernel','mean_test_score']]\n\n\n\n\n\n\n\n\nparam_C\nparam_kernel\nmean_test_score\n\n\n\n\n0\n1\nrbf\n0.980000\n\n\n1\n1\nlinear\n0.980000\n\n\n2\n10\nrbf\n0.980000\n\n\n3\n10\nlinear\n0.973333\n\n\n4\n20\nrbf\n0.966667\n\n\n5\n20\nlinear\n0.966667\n\n\n\n\n\n\n\n\nclf.best_params_\n\n{'C': 1, 'kernel': 'rbf'}\n\n\n\nclf.best_score_\n\n0.9800000000000001\n\n\n\ndir(clf)\n\n['__abstractmethods__',\n '__annotations__',\n '__class__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getstate__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__setstate__',\n '__sizeof__',\n '__sklearn_clone__',\n '__str__',\n '__subclasshook__',\n '__weakref__',\n '_abc_impl',\n '_build_request_for_signature',\n '_check_feature_names',\n '_check_n_features',\n '_check_refit_for_multimetric',\n '_estimator_type',\n '_format_results',\n '_get_default_requests',\n '_get_metadata_request',\n '_get_param_names',\n '_get_tags',\n '_more_tags',\n '_parameter_constraints',\n '_repr_html_',\n '_repr_html_inner',\n '_repr_mimebundle_',\n '_required_parameters',\n '_run_search',\n '_select_best_index',\n '_validate_data',\n '_validate_params',\n 'best_estimator_',\n 'best_index_',\n 'best_params_',\n 'best_score_',\n 'classes_',\n 'cv',\n 'cv_results_',\n 'decision_function',\n 'error_score',\n 'estimator',\n 'fit',\n 'get_metadata_routing',\n 'get_params',\n 'inverse_transform',\n 'multimetric_',\n 'n_features_in_',\n 'n_jobs',\n 'n_splits_',\n 'param_grid',\n 'pre_dispatch',\n 'predict',\n 'predict_log_proba',\n 'predict_proba',\n 'refit',\n 'refit_time_',\n 'return_train_score',\n 'score',\n 'score_samples',\n 'scorer_',\n 'scoring',\n 'set_fit_request',\n 'set_params',\n 'transform',\n 'verbose']\n\n\nUse RandomizedSearchCV to reduce number of iterations and with random combination of parameters. This is useful when you have too many parameters to try and your training time is longer. It helps reduce the cost of computation\n\nfrom sklearn.model_selection import RandomizedSearchCV\nrs = RandomizedSearchCV(svm.SVC(gamma='auto'), {\n        'C': [1,10,20],\n        'kernel': ['rbf','linear']\n    }, \n    cv=5, \n    return_train_score=False, \n    n_iter=2\n)\nrs.fit(iris.data, iris.target)\npd.DataFrame(rs.cv_results_)[['param_C','param_kernel','mean_test_score']]\n\n\n\n\n\n\n\n\nparam_C\nparam_kernel\nmean_test_score\n\n\n\n\n0\n20\nrbf\n0.966667\n\n\n1\n1\nrbf\n0.980000\n\n\n\n\n\n\n\nHow about different models with different hyperparameters?\n\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier\n\n\nmodel_params = {\n    'svm': {\n        'model': svm.SVC(gamma='auto'),\n        'params' : {\n            'C': list(range(1, 21)),\n            'kernel': ['rbf','linear']\n        }  \n    },\n    'random_forest': {\n        'model': RandomForestClassifier(),\n        'params' : {\n            'n_estimators': list(range(1, 11))\n        }\n    },\n    'logistic_regression' : {\n        'model': LogisticRegression(solver='liblinear',multi_class='auto'),\n        'params': {\n            'C': list(range(1, 11))\n        }\n    },\n    'naive_bayes_gaussian': {\n        'model': GaussianNB(),\n        'params': {}\n    },\n    'naive_bayes_multinomial': {\n        'model': MultinomialNB(),\n        'params': {}\n    },\n    'decision_tree': {\n        'model': DecisionTreeClassifier(),\n        'params': {\n            'criterion': ['gini','entropy'],\n            \n        }\n    }     \n}\n\n\nscores = []\n\nfor model_name, mp in model_params.items():\n    clf =  GridSearchCV(mp['model'], mp['params'], cv=5, return_train_score=False)\n    clf.fit(iris.data, iris.target)\n    scores.append({\n        'model': model_name,\n        'best_score': clf.best_score_,\n        'best_params': clf.best_params_\n    })\n    \ndf = pd.DataFrame(scores,columns=['model','best_score','best_params'])\ndf\n\n\n\n\n\n\n\n\nmodel\nbest_score\nbest_params\n\n\n\n\n0\nsvm\n0.986667\n{'C': 4, 'kernel': 'rbf'}\n\n\n1\nrandom_forest\n0.966667\n{'n_estimators': 4}\n\n\n2\nlogistic_regression\n0.966667\n{'C': 4}\n\n\n3\nnaive_bayes_gaussian\n0.953333\n{}\n\n\n4\nnaive_bayes_multinomial\n0.953333\n{}\n\n\n5\ndecision_tree\n0.966667\n{'criterion': 'gini'}\n\n\n\n\n\n\n\nBased on above, I can conclude that SVM with C=1 and kernel=‘rbf’ is the best model for solving my problem of iris flower classification",
    "crumbs": [
      "Blog",
      "Hyper Parameter Optimization"
    ]
  },
  {
    "objectID": "ensemble_tut.html",
    "href": "ensemble_tut.html",
    "title": "Ensemble Tutorial",
    "section": "",
    "text": "dataset credits: https://www.kaggle.com/fedesoriano/heart-failure-prediction\nimport pandas as pd\n\ndf = pd.read_csv(\"Data/heart.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nAge\nSex\nChestPainType\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nHeartDisease\n\n\n\n\n0\n40\nM\nATA\n140\n289\n0\nNormal\n172\nN\n0.0\nUp\n0\n\n\n1\n49\nF\nNAP\n160\n180\n0\nNormal\n156\nN\n1.0\nFlat\n1\n\n\n2\n37\nM\nATA\n130\n283\n0\nST\n98\nN\n0.0\nUp\n0\n\n\n3\n48\nF\nASY\n138\n214\n0\nNormal\n108\nY\n1.5\nFlat\n1\n\n\n4\n54\nM\nNAP\n150\n195\n0\nNormal\n122\nN\n0.0\nUp\n0\ndf.shape\n\n(918, 12)\ndf.describe()\n\n\n\n\n\n\n\n\nAge\nRestingBP\nCholesterol\nFastingBS\nMaxHR\nOldpeak\nHeartDisease\n\n\n\n\ncount\n918.000000\n918.000000\n918.000000\n918.000000\n918.000000\n918.000000\n918.000000\n\n\nmean\n53.510893\n132.396514\n198.799564\n0.233115\n136.809368\n0.887364\n0.553377\n\n\nstd\n9.432617\n18.514154\n109.384145\n0.423046\n25.460334\n1.066570\n0.497414\n\n\nmin\n28.000000\n0.000000\n0.000000\n0.000000\n60.000000\n-2.600000\n0.000000\n\n\n25%\n47.000000\n120.000000\n173.250000\n0.000000\n120.000000\n0.000000\n0.000000\n\n\n50%\n54.000000\n130.000000\n223.000000\n0.000000\n138.000000\n0.600000\n1.000000\n\n\n75%\n60.000000\n140.000000\n267.000000\n0.000000\n156.000000\n1.500000\n1.000000\n\n\nmax\n77.000000\n200.000000\n603.000000\n1.000000\n202.000000\n6.200000\n1.000000\ndf[df.Cholesterol&gt;(df.Cholesterol.mean()+3*df.Cholesterol.std())]\n\n\n\n\n\n\n\n\nAge\nSex\nChestPainType\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nHeartDisease\n\n\n\n\n76\n32\nM\nASY\n118\n529\n0\nNormal\n130\nN\n0.0\nFlat\n1\n\n\n149\n54\nM\nASY\n130\n603\n1\nNormal\n125\nY\n1.0\nFlat\n1\n\n\n616\n67\nF\nNAP\n115\n564\n0\nLVH\n160\nN\n1.6\nFlat\n0\ndf.shape\n\n(918, 12)\ndf1 = df[df.Cholesterol&lt;=(df.Cholesterol.mean()+3*df.Cholesterol.std())]\ndf1.shape\n\n(915, 12)\ndf[df.MaxHR&gt;(df.MaxHR.mean()+3*df.MaxHR.std())]\n\n\n\n\n\n\n\n\nAge\nSex\nChestPainType\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nHeartDisease\ndf[df.FastingBS&gt;(df.FastingBS.mean()+3*df.FastingBS.std())]\n\n\n\n\n\n\n\n\nAge\nSex\nChestPainType\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nHeartDisease\ndf[df.Oldpeak&gt;(df.Oldpeak.mean()+3*df.Oldpeak.std())]\n\n\n\n\n\n\n\n\nAge\nSex\nChestPainType\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nHeartDisease\n\n\n\n\n166\n50\nM\nASY\n140\n231\n0\nST\n140\nY\n5.0\nFlat\n1\n\n\n702\n59\nM\nTA\n178\n270\n0\nLVH\n145\nN\n4.2\nDown\n0\n\n\n771\n55\nM\nASY\n140\n217\n0\nNormal\n111\nY\n5.6\nDown\n1\n\n\n791\n51\nM\nASY\n140\n298\n0\nNormal\n122\nY\n4.2\nFlat\n1\n\n\n850\n62\nF\nASY\n160\n164\n0\nLVH\n145\nN\n6.2\nDown\n1\n\n\n900\n58\nM\nASY\n114\n318\n0\nST\n140\nN\n4.4\nDown\n1\ndf2 = df1[df1.Oldpeak&lt;=(df1.Oldpeak.mean()+3*df1.Oldpeak.std())]\ndf2.shape\n\n(909, 12)\ndf[df.RestingBP&gt;(df.RestingBP.mean()+3*df.RestingBP.std())]\n\n\n\n\n\n\n\n\nAge\nSex\nChestPainType\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nHeartDisease\n\n\n\n\n109\n39\nM\nATA\n190\n241\n0\nNormal\n106\nN\n0.0\nUp\n0\n\n\n241\n54\nM\nASY\n200\n198\n0\nNormal\n142\nY\n2.0\nFlat\n1\n\n\n365\n64\nF\nASY\n200\n0\n0\nNormal\n140\nY\n1.0\nFlat\n1\n\n\n399\n61\nM\nNAP\n200\n0\n1\nST\n70\nN\n0.0\nFlat\n1\n\n\n592\n61\nM\nASY\n190\n287\n1\nLVH\n150\nY\n2.0\nDown\n1\n\n\n732\n56\nF\nASY\n200\n288\n1\nLVH\n133\nY\n4.0\nDown\n1\n\n\n759\n54\nM\nATA\n192\n283\n0\nLVH\n195\nN\n0.0\nUp\n1\ndf3 = df2[df2.RestingBP&lt;=(df2.RestingBP.mean()+3*df2.RestingBP.std())]\ndf3.shape\n\n(902, 12)\ndf.ChestPainType.unique()\n\narray(['ATA', 'NAP', 'ASY', 'TA'], dtype=object)\ndf.RestingECG.unique()\n\narray(['Normal', 'ST', 'LVH'], dtype=object)\ndf.ExerciseAngina.unique()\n\narray(['N', 'Y'], dtype=object)\ndf.ST_Slope.unique()\n\narray(['Up', 'Flat', 'Down'], dtype=object)\ndf4 = df3.copy()\ndf4.ExerciseAngina.replace(\n    {\n        'N': 0,\n        'Y': 1\n    },\n    inplace=True)\n\ndf4.ST_Slope.replace(\n    {\n        'Down': 1,\n        'Flat': 2,\n        'Up': 3\n    },\n    inplace=True\n)\n\ndf4.RestingECG.replace(\n    {\n        'Normal': 1,\n        'ST': 2,\n        'LVH': 3\n    },\n    inplace=True)\n\ndf4.head()\n\n\n\n\n\n\n\n\nAge\nSex\nChestPainType\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nHeartDisease\n\n\n\n\n0\n40\nM\nATA\n140\n289\n0\n1\n172\n0\n0.0\n3\n0\n\n\n1\n49\nF\nNAP\n160\n180\n0\n1\n156\n0\n1.0\n2\n1\n\n\n2\n37\nM\nATA\n130\n283\n0\n2\n98\n0\n0.0\n3\n0\n\n\n3\n48\nF\nASY\n138\n214\n0\n1\n108\n1\n1.5\n2\n1\n\n\n4\n54\nM\nNAP\n150\n195\n0\n1\n122\n0\n0.0\n3\n0\ndf5 = pd.get_dummies(df4, drop_first=True)\ndf5.head()\n\n\n\n\n\n\n\n\nAge\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nHeartDisease\nSex_M\nChestPainType_ATA\nChestPainType_NAP\nChestPainType_TA\n\n\n\n\n0\n40\n140\n289\n0\n1\n172\n0\n0.0\n3\n0\n1\n1\n0\n0\n\n\n1\n49\n160\n180\n0\n1\n156\n0\n1.0\n2\n1\n0\n0\n1\n0\n\n\n2\n37\n130\n283\n0\n2\n98\n0\n0.0\n3\n0\n1\n1\n0\n0\n\n\n3\n48\n138\n214\n0\n1\n108\n1\n1.5\n2\n1\n0\n0\n0\n0\n\n\n4\n54\n150\n195\n0\n1\n122\n0\n0.0\n3\n0\n1\n0\n1\n0\nX = df5.drop(\"HeartDisease\",axis='columns')\ny = df5.HeartDisease\n\nX.head()\n\n\n\n\n\n\n\n\nAge\nRestingBP\nCholesterol\nFastingBS\nRestingECG\nMaxHR\nExerciseAngina\nOldpeak\nST_Slope\nSex_M\nChestPainType_ATA\nChestPainType_NAP\nChestPainType_TA\n\n\n\n\n0\n40\n140\n289\n0\n1\n172\n0\n0.0\n3\n1\n1\n0\n0\n\n\n1\n49\n160\n180\n0\n1\n156\n0\n1.0\n2\n0\n0\n1\n0\n\n\n2\n37\n130\n283\n0\n2\n98\n0\n0.0\n3\n1\n1\n0\n0\n\n\n3\n48\n138\n214\n0\n1\n108\n1\n1.5\n2\n0\n0\n0\n0\n\n\n4\n54\n150\n195\n0\n1\n122\n0\n0.0\n3\n1\n0\n1\n0\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nX_scaled\n\narray([[-1.42896269,  0.46089071,  0.85238015, ...,  2.06757196,\n        -0.53547478, -0.22914788],\n       [-0.47545956,  1.5925728 , -0.16132855, ..., -0.4836591 ,\n         1.86750159, -0.22914788],\n       [-1.74679706, -0.10495034,  0.79657967, ...,  2.06757196,\n        -0.53547478, -0.22914788],\n       ...,\n       [ 0.37209878, -0.10495034, -0.61703246, ..., -0.4836591 ,\n        -0.53547478, -0.22914788],\n       [ 0.37209878, -0.10495034,  0.35947592, ...,  2.06757196,\n        -0.53547478, -0.22914788],\n       [-1.64085227,  0.3477225 , -0.20782894, ..., -0.4836591 ,\n         1.86750159, -0.22914788]])\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=20)\nX_train.shape\n\n(721, 13)\nX_test.shape\n\n(181, 13)\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(SVC(), X, y, cv=5)\nscores.mean()\n\n0.6906445672191528\nUse bagging now with svm\nfrom sklearn.ensemble import BaggingClassifier\n\nbag_model = BaggingClassifier(estimator=SVC(), n_estimators=100, max_samples=0.8, random_state=0)\nscores = cross_val_score(bag_model, X, y, cv=5)\nscores.mean()\n\n0.6839656230816453\nAs you can see above, using bagging in case of SVM doesn’t make much difference in terms of model accuracy. Bagging is effective when we have high variance and instable model such as decision tree. Let’s explore how bagging changes the performance for a decision tree classifier.\nfrom sklearn.tree import DecisionTreeClassifier\n\nscores = cross_val_score(DecisionTreeClassifier(random_state=0), X, y, cv=5)\nscores.mean()\n\n0.7193984039287907\nUse bagging now with decision tree\nbag_model = BaggingClassifier(\n    estimator=DecisionTreeClassifier(random_state=0), \n    n_estimators=100, \n    max_samples=0.9, \n    oob_score=True,\n    random_state=0\n)\n\nscores = cross_val_score(bag_model, X, y, cv=5)\nscores.mean()\n\n0.8037016574585636\nYou can see that with bagging the score improved from 71.93% to 80.37%\nfrom sklearn.ensemble import RandomForestClassifier\n\nscores = cross_val_score(RandomForestClassifier(), X, y, cv=5)\nscores.mean()\n\n0.826998158379374",
    "crumbs": [
      "Blog",
      "Ensemble Tutorial"
    ]
  },
  {
    "objectID": "ensemble_tut.html#boosting",
    "href": "ensemble_tut.html#boosting",
    "title": "Ensemble Tutorial",
    "section": "Boosting",
    "text": "Boosting\n\nfrom sklearn.ensemble import AdaBoostClassifier\n\n\nclf = AdaBoostClassifier(\n    n_estimators=100,\n    random_state=0,\n    algorithm='SAMME')\n\n\nclf.fit(X_train, y_train)\n\nAdaBoostClassifier(algorithm='SAMME', n_estimators=100, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.AdaBoostClassifierAdaBoostClassifier(algorithm='SAMME', n_estimators=100, random_state=0)\n\n\n\nclf.score(X_test, y_test)\n\n0.8397790055248618\n\n\nRandom forest gave even a better performance with 81.7% as score. Underneath it used bagging where it sampled not only data rows but also the columns (or features)",
    "crumbs": [
      "Blog",
      "Ensemble Tutorial"
    ]
  },
  {
    "objectID": "loss_or_cost_funtions.html",
    "href": "loss_or_cost_funtions.html",
    "title": "Loss or Cost funtions",
    "section": "",
    "text": "The key takeaway is that the loss function is a measurable way to gauge the performance and accuracy of a machine learning model. In this case, the loss function acts as a guide for the learning process within a model or machine learning algorithm.\nThe role of the loss function is crucial in the training of machine learning models and includes the following:\n\nPerformance measurement: Loss functions offer a clear metric to evaluate a model’s performance by quantifying the difference between predictions and actual results.\nDirection for improvement: Loss functions guide model improvement by directing the algorithm to adjust parameters(weights) iteratively to reduce loss and improve predictions.\nBalancing bias and variance: Effective loss functions help balance model bias (oversimplification) and variance (overfitting), essential for the model’s generalization to new data.\nInfluencing model behavior: Certain loss functions can affect the model’s behavior, such as being more robust against data outliers or prioritizing specific types of errors.\n\nLet’s explore the roles of particular loss functions in later sections and build a detailed intuition and understanding of the loss function.",
    "crumbs": [
      "Blog",
      "Loss or Cost funtions"
    ]
  },
  {
    "objectID": "loss_or_cost_funtions.html#loss-functions-in-brief",
    "href": "loss_or_cost_funtions.html#loss-functions-in-brief",
    "title": "Loss or Cost funtions",
    "section": "",
    "text": "The key takeaway is that the loss function is a measurable way to gauge the performance and accuracy of a machine learning model. In this case, the loss function acts as a guide for the learning process within a model or machine learning algorithm.\nThe role of the loss function is crucial in the training of machine learning models and includes the following:\n\nPerformance measurement: Loss functions offer a clear metric to evaluate a model’s performance by quantifying the difference between predictions and actual results.\nDirection for improvement: Loss functions guide model improvement by directing the algorithm to adjust parameters(weights) iteratively to reduce loss and improve predictions.\nBalancing bias and variance: Effective loss functions help balance model bias (oversimplification) and variance (overfitting), essential for the model’s generalization to new data.\nInfluencing model behavior: Certain loss functions can affect the model’s behavior, such as being more robust against data outliers or prioritizing specific types of errors.\n\nLet’s explore the roles of particular loss functions in later sections and build a detailed intuition and understanding of the loss function.",
    "crumbs": [
      "Blog",
      "Loss or Cost funtions"
    ]
  },
  {
    "objectID": "loss_or_cost_funtions.html#applicability-to-classification",
    "href": "loss_or_cost_funtions.html#applicability-to-classification",
    "title": "Loss or Cost funtions",
    "section": "Applicability to Classification",
    "text": "Applicability to Classification\n\nBinary Cross-Entropy Loss / Log Loss\nTo understand Binary Cross-Entropy Loss, sometimes called Log Loss, it is helpful to discuss the components of the terms. - Loss: This is a mathematical quantification of the margin/difference between the prediction of a machine learning algorithm and the actual target value. - Entropy: A simple definition of entropy is that it is a calculation of the degree of randomness or disorder within a system - Cross Entropy: This is a term commonly utilised in information theory, and it measures the differences between two probability distributions that can be used to identify an observation. - Binary: This is an expression of numerical digits using either of two states, 0 or 1. This is extended to the definition of Binary Classification where we ditingus=ish two classes(A and B) using binary representation, where class A is assigned the numerical representation of 0 and class B is assigned 1.\n\\(L(y , f(x)) = -[y * log(f(x)) + (1 - y) * log(1 - f(x))]\\)\nWhere:\nL represents the Binary Cross-Entropy Loss function\ny is the true binary label (0 or 1)\nf(x) is the predicted probability of the positive class (between 0 and 1)\n\n\nCategorical Cross-Entropy Loss\n\n\nHinge Loss\n\nHinge Loss is a loss function utilized within machine learning to train classifiers that optimize to increase the margin between data points and the decision boundary. Hence, it is mainly used for maximum margin classifications. To ensure the maximum margin between the data points and boundaries, hinge loss penalizes predictions from the machine learning model that are wrongly classified, which are predictions that fall on the wrong side of the margin boundary and also predictions that are correctly classified but are within close proximity to the decision boundary.\n\n\\(L(y - f(x)) = \\max(0, 1 - y * f(x))\\) Where:\nL represents the Hinge Loss\ny is the true label or target value (-1 or 1)\nf(x) is the predicted value or decision function output\n\n\nLog Loss",
    "crumbs": [
      "Blog",
      "Loss or Cost funtions"
    ]
  },
  {
    "objectID": "loss_or_cost_funtions.html#applicability-to-regression",
    "href": "loss_or_cost_funtions.html#applicability-to-regression",
    "title": "Loss or Cost funtions",
    "section": "Applicability to Regression",
    "text": "Applicability to Regression\n\nMean Square Error (MSE) / L2 Loss\n\nMSE is a standard loss function utilized in most regression tasks since it directs the model to optimize to minimize the squared differences between the predicted and target values.\n\n\\(MSE  = \\dfrac{1}{n} * \\sum(y_i -\\bar{y})^2\\)\nWhere:\nn is the number of samples in the dataset\nyᵢ is the predicted value for the i-th sample\nȳ is the target value for the i-th sample\n\n\nMean Absolute Error (MAE) / L1 Loss\n\nA scenario where MAE is an applicable loss function is one where we don’t want to penalize outliers considerably or at all, for example, predicting delivery times for a food delivery company.\n\n\\(MAE  = \\dfrac{1}{n} * \\sum(y_i -\\bar{y})\\)\nWhere:\nn is the number of samples in the dataset\nyᵢ is the predicted value for the i-th sample\nȳ is the target value for the i-th sample\n\n\nHuber Loss / Smooth Mean Absolute Error\n\nThe Huber Loss function effectively combines two components for handling errors differently, with the transition point between these components determined by the threshold δ:\nQuadratic Component for Small Errors: For errors smaller than δ, it uses the quadratic component (1/2) * (f(x) - y)^2\nLinear Component for Large Errors: For errors larger than δ, it applies the linear component δ * |f(x) - y| - (1/2) * δ^2\n\n\\(L(\\delta, y , f(x)) = \\dfrac{1}{2} * (f(x) - y)^2 \\quad if \\quad |f(x) - y| &lt;= \\delta\\)\n\\(\\quad \\quad \\quad \\quad = \\delta * |f(x) - y| - \\dfrac{1}{2} * \\delta^2 \\quad if \\quad |f(x) - y| &gt; \\delta\\)\nWhere:\nL represents the Huber Loss function\nδ is the delta parameter, which determines the threshold for switching between the quadratic and linear components of the loss function\ny is the true value or target value\nf(x) is the predicted value",
    "crumbs": [
      "Blog",
      "Loss or Cost funtions"
    ]
  },
  {
    "objectID": "cnns.html",
    "href": "cnns.html",
    "title": "Convolution Neural Networks",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Blog",
      "Convolution Neural Networks"
    ]
  },
  {
    "objectID": "polynomialregression.html",
    "href": "polynomialregression.html",
    "title": "Polynomial Regression",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nX = np.random.rand(100,1)\ny = 4 + 5 * X + 1 * np.random.randn(100, 1)\nplt.scatter(X, y)\nplt.show()\nreg = LinearRegression()\nreg.fit(X, y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\nX_vals = np.linspace(0, 1, 100).reshape(-1,1)\ny_vals = reg.predict(X_vals)\nplt.scatter(X, y)\nplt.plot(X_vals, y_vals, color ='g')\nplt.show()",
    "crumbs": [
      "Blog",
      "Polynomial Regression"
    ]
  },
  {
    "objectID": "polynomialregression.html#second-order",
    "href": "polynomialregression.html#second-order",
    "title": "Polynomial Regression",
    "section": "Second Order",
    "text": "Second Order\n\nX = 4 * np.random.rand(50,1) -2\ny = 4 + 2 * X + 5 * X ** 2 + 2 * np.random.randn(50, 1)\n\n\npoly_features = PolynomialFeatures(degree = 2, include_bias = False)\nX_poly = poly_features.fit_transform(X)\n\n\nreg1 = LinearRegression()\nreg1.fit(X_poly, y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nX_vals = np.linspace(-2, 2, 50).reshape(-1,1)\nX_vals_poly = poly_features.transform(X_vals)\ny_vals = reg1.predict(X_vals_poly)\nplt.scatter(X, y)\nplt.plot(X_vals, y_vals, color ='g')\nplt.show()\n\n\n\n\n\n\n\n\n\nX_vals_poly[:,1]\n\narray([4.00000000e+00, 3.68013328e+00, 3.37359434e+00, 3.08038317e+00,\n       2.80049979e+00, 2.53394419e+00, 2.28071637e+00, 2.04081633e+00,\n       1.81424406e+00, 1.60099958e+00, 1.40108288e+00, 1.21449396e+00,\n       1.04123282e+00, 8.81299459e-01, 7.34693878e-01, 6.01416077e-01,\n       4.81466056e-01, 3.74843815e-01, 2.81549354e-01, 2.01582674e-01,\n       1.34943773e-01, 8.16326531e-02, 4.16493128e-02, 1.49937526e-02,\n       1.66597251e-03, 1.66597251e-03, 1.49937526e-02, 4.16493128e-02,\n       8.16326531e-02, 1.34943773e-01, 2.01582674e-01, 2.81549354e-01,\n       3.74843815e-01, 4.81466056e-01, 6.01416077e-01, 7.34693878e-01,\n       8.81299459e-01, 1.04123282e+00, 1.21449396e+00, 1.40108288e+00,\n       1.60099958e+00, 1.81424406e+00, 2.04081633e+00, 2.28071637e+00,\n       2.53394419e+00, 2.80049979e+00, 3.08038317e+00, 3.37359434e+00,\n       3.68013328e+00, 4.00000000e+00])\n\n\n\nplt.scatter(X_vals, X_vals_poly[:,1])",
    "crumbs": [
      "Blog",
      "Polynomial Regression"
    ]
  },
  {
    "objectID": "polynomialregression.html#higher-order",
    "href": "polynomialregression.html#higher-order",
    "title": "Polynomial Regression",
    "section": "Higher Order",
    "text": "Higher Order\n\nX = 4 * np.random.rand(50,1) -2\ny = 4 + 2 * X + 5 * X ** 2 + 12 * X ** 3 + 2 * X ** 4 + + 2 * np.random.randn(50, 1)\n\npoly_features = PolynomialFeatures(degree = 4, include_bias = False)\nX_poly = poly_features.fit_transform(X)\n\nreg2 = LinearRegression()\nreg2.fit(X_poly, y)\n\nX_vals = np.linspace(-2, 2, 50).reshape(-1,1)\nX_vals_poly = poly_features.transform(X_vals)\ny_vals = reg2.predict(X_vals_poly)\nplt.scatter(X, y)\nplt.plot(X_vals, y_vals, color ='g')\nplt.show()",
    "crumbs": [
      "Blog",
      "Polynomial Regression"
    ]
  }
]